[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to COMM 605!",
    "section": "",
    "text": "Source: xkcd.com\n\n\n\n\n\n⏰ Time\nMonday 5:00 – 7:50 PM\n\n\n📍 Location\nLiberal Arts Hall (LBR)- Room 3201\n\n\n📭 Email\nryanwang@mail.rit.edu\n\n\n🏢 Office\nRoom 3041 Eastman Building OR Zoom (By appointment)\n\n\n\n\nCourse overview\nThis is the tutorial website for COMM 605 Social media analytics and research at Rochester Institute of Technology. All the codes in the tutorials are created by the instructor and open access resources, including but not limited to:\n\nWickham, H., Çetinkaya-Rundel, M., & Grolemund, G. (2023). R for data science (2nd version).\nLiang, H. COMM3650: Social Media Analytics for Communication Professionals.\n\nThis course focuses on social media research with various methodological approaches to study public data, users and messages. Students will be introduced to a variety of techniques and concepts used to obtain, monitor and evaluate social media content with a focus on how the analytics could inform communication strategies. During the course, students will also learn how to design and evaluate social media-based research studies.\n\n\nObjectives\nStudents who successfully complete assigned coursework should develop the following skills:\n\nexplain the theoretical and methodological perspectives that guide social media research;\napply computational tools and practice basic data science skills to answer empirical research questions and conduct social science research;\nidentify major milestones in social media platforms and research;\ncritique on how algorithms and (social media)data may affect civil societies in both positive and adverse ways.\n\n\n\nSuggested readings\n\nRequired: Salganik, M. J. (2017). Bit by bit: Social research in the digital age.\nSuggested: Grimmer, J., Roberts, M. E., & Stewart, B. M. (2022). Text as data: A new framework for machine learning and the social sciences.\nSuggested: Borgatti, S. P., Everett, M. G., & Johnson, J. C. (2018). Analyzing social networks."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "week3.html",
    "href": "week3.html",
    "title": "Week 3 Lab",
    "section": "",
    "text": "There are mainly four panels once you open the IDE (you can modify the theme and the layout under File - Preference - Appearance/Pane layout).\n\nConsole\nEnvironment/History/…\nFiles/Plots/Viewer/…\nSources"
  },
  {
    "objectID": "materials.html",
    "href": "materials.html",
    "title": "Week 3 Lab",
    "section": "",
    "text": "This is the first lab.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.3     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors"
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Course schedule",
    "section": "",
    "text": "Weeks\nTopics\n\n\n\n\nWeek 1: Aug 28\nClass overview and set-up\n\n\nWeek 2: Sep 4\nLabor Day - No Classes 😃\n\n\nWeek 3: Sep 11\nOverview of social media research\nLab: Get familiar with R/R Studio and data wrangling\n\n\nWeek 4: Sep 18\nWhat (social media) data is out there?\nLab: Data collection – API\n\n\nWeek 5: Sep 25\nHow to utilize social media data?\nLab: Data collection – Web scraping\nFinal project idea due Oct 1\n\n\nWeek 6: Oct 2\nThe pitfall of social media data\nLab: Data visualization\n\n\nWeek 7: Oct 9\nFall break - No Classes 😃\nPractice 1 due on Oct 15\n\n\nWeek 8: Oct 16\nCase study: Text-as-data I\nLab: Basic text analysis\n\n\nWeek 9: Oct 23\nCase study: Text-as-data II\nLab: Topic modeling\n\n\nWeek 10: Oct 30\nCase study: Text-as-data III\nLab: Sentiment analysis\nPractice 2 due on Nov 5\n\n\nWeek 11: Nov 6\nCase study: Network I\nLab: Network analysis\n\n\nWeek 12: Nov 13\nCase study: Network II\nLab: Network analysis & visualization\n\n\nWeek 13: Nov 20\nCourse wrap-up\nPractice 3 due on Nov 26\n\n\nWeek 14: Nov 27\nIndividual meetings & consultant\n\n\nWeek 15: Dec 4\nProject presentation\n\n\nWeek 16: Dec 11\nFinal paper due"
  },
  {
    "objectID": "week3.html#reproducible-research",
    "href": "week3.html#reproducible-research",
    "title": "Week 3 Lab",
    "section": "Reproducible research",
    "text": "Reproducible research\nReproducible: Can someone else reproduce your entire analysis?\n\nAvailable data\nAvailable codes (including the random seed for machine learning)\n\nWe will be mainly using two types of file formats (other related formats such as Rproj and Rdata:\n\nR script (a text file contains the same commands that your would enter on the command line of R)\nRMarkdown\n\nText, code, and results (from your analysis)\nFormatted output: html, pdf (which requires tex, a typesetting system), etc.\nResource: cheatsheet, The Definitive Guide\n\n\nR markdown is a simple and easy to use plain text language used to combine your R code, results from your data analysis (including plots and tables) and written commentary into a single nicely formatted and reproducible document (like a report, publication, thesis chapter or a web page like this one).1\n\n\nOther examples of markup languages include (compared with Word):\n\nHTML (HyperText Markup Language): website\nLaTex: Overleaf\nMarkdown (a “lightweight” markup language)"
  },
  {
    "objectID": "week3.html#r-packages",
    "href": "week3.html#r-packages",
    "title": "Week 3 Lab",
    "section": "R packages",
    "text": "R packages\nUntil Sep 11th, 2023, there are 19861 available packages on CRAN (The Comprehensive R Archive Network) package repository.\n\n# Install packages\n## install.packages(\"xxx\") - if on CRAN\n## devtools::install_github(\"houyunhuang/ggcor\") - if only on github\n\n\n# Loading packages\nlibrary(dplyr)\n\n\n# An elegant way to install and load packages by using a loop\n if (!require(dplyr)) {\n    install.packages(\"dplyr\")\n    library(dplyr)\n}\n\n# With multiple packages\nsomepackages &lt;- c(\"dplyr\", \"plyr\", \"magrittr\")\n\nfor (pkg in somepackages) {\n  if (!require(pkg, character.only = TRUE)) {\n    install.packages(pkg)\n    library(pkg,character.only = TRUE)\n  }\n}\n\n\n# Unload packages\ndetach(\"package:dplyr\", unload = TRUE)\n\n# Remove packages\n## remove.packages(\"dplyr\")\n\n\nThe chunk options in R code\nGlobal options: knitr::opts_chunk$set(echo = FALSE)\n\nIn this example, the knitr::opts_chunk$set(echo = FALSE) line in the setup chunk tells R Markdown to hide the R code within code chunks for the entire document, except when overridden within individual code chunks using {r} options. This is often used to create clean and readable reports or documents where you want to present the results of your R code without cluttering the document with the code itself.\n\nOther (individual) chunk options:\n\ninclude = FALSE prevents code and results from appearing in the finished file. R Markdown still runs the code in the chunk, and the results can be used by other chunks.\necho = FALSE prevents code, but not the results from appearing in the finished file. This is a useful way to embed figures.\nmessage = FALSE prevents messages that are generated by code from appearing in the finished file.\nwarning = FALSE prevents warnings that are generated by code from appearing in the finished.\nfig.cap = \"...\" adds a caption to graphical results.\n\n\n\nSome useful packages\n\nData loading\n\nxlsx: read and write Micorsoft Excel files from R\nhaven: enable R to read and write data from SAS, SPSS, and Stata.\n\nData wrangling\n\ntidyverse: a collection of R packages designed for data science that share an underlying design philosophy, grammar, and data structures, for data import, tidying, and visualization listed here.\ndplyr: essential shortcuts for subsetting, summarizing, rearranging, and joining together data sets.\ntidyr: tools for changing the layout of your data sets. Use the gather and spread functions to convert your data into the tidy format, the layout R likes best.\nstringr: easy to learn tools for regular expressions and character strings.\nlubridate: tools that make working with dates and times easier.\nsna / network: a range of tools for social network analysis.\n\nData visualization\n\nggplot2: R’s famous package for making beautiful graphics. ggplot2 lets you use the grammar of graphics to build layered, customizable plots.\nother extension of ggplot2\nigraph: a package for network analysis and visualization\n\nData modeling/analysis\n\ntidymodels: a collection of packages for modeling and machine learning using tidyverse principles\ncaret: tools for classification and regression training\ncar: a hands-on companion to applied regression, especially the anova and vif function.\nlme4: linear and Non-linear mixed effects models\nquanteda: an R package for managing and analyzing text.\nstatnet: a suite of open source R-based software packages for (statistical) network analysis"
  },
  {
    "objectID": "week2.html",
    "href": "week2.html",
    "title": "Week 2 Lab",
    "section": "",
    "text": "There are mainly four panels once you open the IDE (you can modify the theme and the layout under File - Preference - Appearance/Pane layout).\n\nConsole\nEnvironment/History/…\nFiles/Plots/Viewer/…\nSources"
  },
  {
    "objectID": "week2.html#reproducible-research",
    "href": "week2.html#reproducible-research",
    "title": "Week 2 Lab",
    "section": "Reproducible research",
    "text": "Reproducible research\nReproducible: Can someone else reproduce your entire analysis?\n\nAvailable data\nAvailable codes (including the random seed for machine learning)\n\nWe will be mainly using two types of file formats (other related formats such as Rproj and Rdata:\n\nR script (a text file contains the same commands that your would enter on the command line of R)\nRMarkdown\n\nText, code, and results (from your analysis)\nFormatted output: html, pdf (which requires tex, a typesetting system), etc.\nResource: cheatsheet, The Definitive Guide\n\n\nR markdown is a simple and easy to use plain text language used to combine your R code, results from your data analysis (including plots and tables) and written commentary into a single nicely formatted and reproducible document (like a report, publication, thesis chapter or a web page like this one).1\n\n\nOther examples of markup languages include (compared with Word):\n\nHTML (HyperText Markup Language): website\nLaTex: Overleaf\nMarkdown (a “lightweight” markup language)"
  },
  {
    "objectID": "week2.html#r-packages",
    "href": "week2.html#r-packages",
    "title": "Week 2 Lab",
    "section": "R packages",
    "text": "R packages\nUntil Sep 11th, 2023, there are 19861 available packages on CRAN (The Comprehensive R Archive Network) package repository.\n\n# Install packages\n## install.packages(\"xxx\") - if on CRAN\n## devtools::install_github(\"houyunhuang/ggcor\") - if only on github\n\n\n# Loading packages\nlibrary(dplyr)\n\n\n# An elegant way to install and load packages by using a loop\n if (!require(dplyr)) {\n    install.packages(\"dplyr\")\n    library(dplyr)\n}\n\n# With multiple packages\nsomepackages <- c(\"dplyr\", \"plyr\", \"magrittr\")\n\nfor (pkg in somepackages) {\n  if (!require(pkg, character.only = TRUE)) {\n    install.packages(pkg)\n    library(pkg,character.only = TRUE)\n  }\n}\n\n\n# Unload packages\ndetach(\"package:dplyr\", unload = TRUE)\n\n# Remove packages\n## remove.packages(\"dplyr\")\n\n\nThe chunk options in R code\nGlobal options: knitr::opts_chunk$set(echo = FALSE)\n\nIn this example, the knitr::opts_chunk$set(echo = FALSE) line in the setup chunk tells R Markdown to hide the R code within code chunks for the entire document, except when overridden within individual code chunks using {r} options. This is often used to create clean and readable reports or documents where you want to present the results of your R code without cluttering the document with the code itself.\n\nOther (individual) chunk options:\n\ninclude = FALSE prevents code and results from appearing in the finished file. R Markdown still runs the code in the chunk, and the results can be used by other chunks.\necho = FALSE prevents code, but not the results from appearing in the finished file. This is a useful way to embed figures.\nmessage = FALSE prevents messages that are generated by code from appearing in the finished file.\nwarning = FALSE prevents warnings that are generated by code from appearing in the finished.\nfig.cap = \"...\" adds a caption to graphical results.\n\n\n\nSome useful packages\n\nData loading\n\nxlsx: read and write Micorsoft Excel files from R\nhaven: enable R to read and write data from SAS, SPSS, and Stata.\n\nData wrangling\n\ntidyverse: a collection of R packages designed for data science that share an underlying design philosophy, grammar, and data structures, for data import, tidying, and visualization listed here.\ndplyr: essential shortcuts for subsetting, summarizing, rearranging, and joining together data sets.\ntidyr: tools for changing the layout of your data sets. Use the gather and spread functions to convert your data into the tidy format, the layout R likes best.\nstringr: easy to learn tools for regular expressions and character strings.\nlubridate: tools that make working with dates and times easier.\nsna / network: a range of tools for social network analysis.\n\nData visualization\n\nggplot2: R’s famous package for making beautiful graphics. ggplot2 lets you use the grammar of graphics to build layered, customizable plots.\nother extension of ggplot2\nigraph: a package for network analysis and visualization\n\nData modeling/analysis\n\ntidymodels: a collection of packages for modeling and machine learning using tidyverse principles\ncaret: tools for classification and regression training\ncar: a hands-on companion to applied regression, especially the anova and vif function.\nlme4: linear and Non-linear mixed effects models\nquanteda: an R package for managing and analyzing text.\nstatnet: a suite of open source R-based software packages for (statistical) network analysis"
  },
  {
    "objectID": "week2.html#r-basic-operators",
    "href": "week2.html#r-basic-operators",
    "title": "Week 2 Lab",
    "section": "R basic operators",
    "text": "R basic operators\n\nArithmetic operators\n\n1 + 19 # addition\n19 - 1 # subtraction\n5 * 4 # multiplication\n10 / 2 # division\n11 %/% 2 # integer division\n41 %% 21 # modulus\n20 ^ 2 # exponents\n20 ** 2\n\n\ndata <- data.frame(x1 = 1:3,  \n                      x2 = 2:4,\n                      x3 = 2)\ndata \n\n\ndata^2\n\n\n\nThe <- operator\nAssignment is a binary operator: the left side is a symbol/variable/object, the right is a value/expression being assigned to the left.\n\nx <- 1\nx <- c(1, 2, 3, 4, 5)\nx <- data.frame(x1 = 1:3,  \n                x2 = 2:4,\n                x3 = 2)\n\n\n\nThe [] operator\nIndexing is a binary operator (two operands: the object being indexed (e.g., a vector, list, or data frame) and the index or indices used to select specific elements from that object. )\n\nx <- c(5, 4, 3, 2, 1)\nx[1] # Extracts the first element\n\n\nx <- data.frame(x1 = 1:3,  \n                x2 = 2:4,\n                x3 = 2)\nx[3] \nx[,3]\nx[3,]\nx[3,2]\nx[\"x3\"]\n\n\n\nThe $ operator\nThe $ operator is used to extract or subset a specific part of a data object in R.\n\nExtract the values in a data frame columns\n\n\ndata <- data.frame(x1 = 1:5,  # Create example data\n                   x2 = letters[1:5],\n                   x3 = 9)\ndata  \n\n\ndata$x2\n\n\nReturn specific list elements\n\n\nmy_list <- list(A = 1:5,  # Create example list\n                B = letters[1:5],\n                C = 9)\nmy_list # Print example list\n\n\nmy_list$B # Extract element of list\n\n\n\nThe () operator\nA function call is also a binary operator as the left side is a symbol pointing to the function argument and the right side are the arguments\n\nmax(1,2)\nx <- max(1,2)\n\n\n\nThe ? operator\n\n?: Search R documentation for a specific term.\n??: Search R help files for a word or phrase.\n\n\n\nThe %>% or |> opertor\n%>% is a longstanding feature of the magrittr package for R. It takes the output of one function and passes it into another function as an argument. This allows us to link a sequence of analysis steps (think about a a conveyor belt in a factory)\n\nReadability and clarity\nEast of modification\nAvoid intermediate variables\n\n\nlibrary(tidyverse)\n?group_by\n?mtcars\nmtcars\n\nx <- filter(mtcars, cyl == 6)\ny <- select(x, c(\"mpg\", \"hp\"))\n\nmtcars %>%\n  filter(cyl == 6) %>%\n  select(mpg, hp)\n\n\nresult <- mtcars %>%\n  group_by(cyl) %>%\n  summarise(meanMPG = mean(mpg))\n\n\n\nThe %in% opertor\n%in% is a matching feature to check if the values of the first argument are present in the second argument and returns a logical vector indicating if there is a match or not for its left operand. Here, the first and second arguments can be a value, vector, list, or sequence.\n\n# Check value in a Vector\n67 %in% c(2,5,8,23,67,34)\n45 %in% c(2,5,8,23,67,34)\n\n# Check values from one vector present in another vector\nvec1 <- c(2,5,8,23,67,34)\nvec2 <- c(1,2,8,34) \nvec2 %in% vec1\n\n# Check values in a dataframe\ndf=data.frame(\n  emp_id=c(1,2,3,5),\n  name=c(\"John\",\"Rose\",\"Williams\", \"Ray\"),\n  dept_id=c(10,20,10,50)\n)\n\ndf$dept_state <- if_else(df$dept_id %in% c(10,50),'NY','CA')\ndf\n\n\ndf2 <- df[df$name %in% c('Ray','Rose'), ]\ndf2"
  },
  {
    "objectID": "week2.html#data-type-in-r",
    "href": "week2.html#data-type-in-r",
    "title": "Week 2 Lab",
    "section": "Data type in R",
    "text": "Data type in R\n\n# numeric (double if with more than two decimal numbers)\nx <- 10.5\nclass(x)\n\n# integer\nx <- 1000L\nclass(x)\n\n\n# complex\nx <- 9i + 3\nclass(x)\n\n# character/string\nx <- \"R is exciting\"\nclass(x)\n\n# logical/boolean\nx <- TRUE\nclass(x)\n\n# date\nx = \"01-06-2021\"\nx = as.Date(x, \"%d-%m-%Y\")\nclass(x)\n\n# Factors\n## Factors are the data objects which are used to categorize the data and store it as levels. \n## They can store both strings and integers. They are useful in the columns which have a limited number of unique values. Like Male/Female and True/False, etc. They are useful in data analysis for statistical modeling.\nx <- c(\"East\",\"South\",\"East\",\"North\",\"North\",\"East\",\"West\",\"West\",\"West\",\"South\",\"North\")\nx_factor <- factor(x) ### as.factor\nx_factor2 <- factor(x, levels = c(\"East\", \"West\", \"South\", \"North\"))\nsummary(x_factor)\nsummary(x_factor2)\n\n# Missing values\nx <- c(1, 2, NA, 4)\nis.na(x)\nwhich(is.na(x))\nx_omit <- na.omit(x)\n\n\nNotes on NA\n\nA missing value in a factor variable is displayed as <NA> rather than just NA.\nR has a special value NaN for “not a number.” 0/0 is an example of a calculation that will produce a NaN. NaNs print as NaN, but generally act like NAs.\nAnother special case is Inf, such as log(0)\n\n\n\nExercise\n\nCreate a new R script\nInstall quanteda package (for textual analysis later in the semester) and load it\nCreate a variable called first_num and assign it the value of 605\nCreate a variable called first_char and assign it the value of my first character\nCreate a vector called gender, including: “male”, “female”, “other”, “female”, “male”, “female”, “female”, “other”, “male”. Make gender as a factor vector, following the order of “female”, “other”, and “male”."
  },
  {
    "objectID": "week2.html#exercise",
    "href": "week2.html#exercise",
    "title": "Week 2 Lab",
    "section": "Exercise",
    "text": "Exercise\n\nCreate a new R script\nInstall quanteda package (for textual analysis later in the semester) and load it\nCreate a variable called first_num and assign it the value of 605\nCreate a variable called first_char and assign it the value of my first character\nCreate a vector called gender, including: “male”, “female”, “other”, “female”, “male”, “female”, “female”, “other”, “male”. Make gender as a factor vector, following the order of “female”, “other”, and “male”."
  },
  {
    "objectID": "week3.html#understanding-data-frame",
    "href": "week3.html#understanding-data-frame",
    "title": "Week 3 Lab",
    "section": "Understanding data frame",
    "text": "Understanding data frame\n\nCreate a data frame\n\nheight <- c(180, 155, 160, 167, 181)\nweight <- c(65, 50, 52, 58, 70)\nnames <- c(\"Joanna\", \"Charlotte\", \"Helen\", \"Karen\", \"Amy\")\n\ndata <- data.frame(height = height, weight = weight, names = names) #stringsAsFactors = TRUE\ndata\ndim(data)\nstr(data)\n\n\n\nPositional index\n\ndata[1,3] # the first value (1st row ) of the names variable (3th column)\ndata$names[1]\ndata[1:2, 2:3] #  the first 2 rows and the last 2 columns\ndata[1:2, ]\ndata[, 2:3]\n\n\n\nOrdering data frames\n\nheight_order <- data[order(data$height), ]\nrownames(height_order) <- 1:nrow(height_order)\nheight_order\n\nheight_order <- data[order(data$height, decreasing = T),]\n\n\n\nAdding/Removing columns and rows\n\ndata2 <- data.frame(state = c(\"NY\", \"PA\", \"MD\", \"VA\", \"MA\"))\ndata_newcolumn <- cbind(data, data2)\n\ndata_removecolumn <- data_newcolumn[, c(1:2, 4)]\ndata_newcolumn$state <- NULL\n\ndata3 <- data.frame(height = c(120, 150, 132, 122),\n                    weight = c(44, 56, 49, 45),\n                    names = c(\"Ryan\", \"Chris\", \"Ben\", \"John\"))\ndata_newrow <- rbind(data, data3)\ndata_removerow <- data_newrow[c(1,6:9),]\n\n\n\nMerging data frames\nHere are two fictitious datasets of a clinical trial. One table contains demographic information of the patients and the other one adverse events recorded throughout the course of the trial.\n\ndemographics <- data.frame(\n  id = c(\"P1\", \"P2\", \"P3\"),\n  age = c(40, 54, 47),\n  state = c(\"NY\", \"MA\", \"PA\"),\n  stringsAsFactors = FALSE\n)\n\nadverse_events <- data.frame(\n  id = c(\"P1\", \"P1\", \"P3\", \"P4\"),\n  term = c(\"Headache\", \"Neutropenia\", \"Constipation\", \"Tachycardia\"),\n  onset_date = c(\"2020-12-03\", \"2021-01-03\", \"2020-11-29\", \"2021-01-27\"),\n  stringsAsFactors = FALSE\n)\n\n\nmerge(demographics, adverse_events, by = \"id\")\nmerge(demographics, adverse_events, by = \"id\", all.x = T)\nmerge(demographics, adverse_events, by = \"id\", all.y = T)\nmerge(demographics, adverse_events, by = \"id\", all = T)\n\n\nadverse_events2 <- adverse_events\ncolnames(adverse_events2)[1] <- \"pat_id\"\nmerge(demographics, adverse_events2, by.x = \"id\", by.y = \"pat_id\", all = T)\n\n\n\nExercise\nPlease merge the following two datasets emp_df (employee information)and dept_df (department information) using two ID columns dept_id and dept_branch_id.\n\nemp_df=data.frame(\n  emp_id=c(1,2,3,4,5,6),\n  name=c(\"Chris\",\"Rose\",\"Williams\",\"Jones\",\"Jayden\",\"Xavior\"),\n  superior_emp_id=c(1,1,1,2,2,2),\n  dept_id=c(10,20,10,10,40,30),\n  dept_branch_id= c(101,102,101,101,104,103)\n)\n\ndept_df=data.frame(\n  dept_id=c(10,20,30,40),\n  dept_name=c(\"Finance\",\"Marketing\",\"Sales\",\"IT\"),\n  dept_branch_id= c(101,102,103,104)\n)\n\n\n\nShow the code\nmerge(emp_df, dept_df, by = c(\"dept_id\", \"dept_branch_id\"), all.x = T)"
  },
  {
    "objectID": "week3.html#data-importing-and-exporting",
    "href": "week3.html#data-importing-and-exporting",
    "title": "Week 3 Lab",
    "section": "Data importing (and exporting)",
    "text": "Data importing (and exporting)\nCommon data formats:\n\nCSV (comma-seperated values) / TSV (tab-seperated values)\nxlsx\ntxt\nother softwares/packages: .sav(SPSS), .dta(STATA)\n\nCommon data types in R:\n\ndata frame\ntibble (tbl_df): it does much less than a data frame (a neater data frame), as it never changes the type of the inputs (e.g. it keeps list columns unchanged, and never converts strings to factors), it never changes the names of variables, it only recycles inputs of length 1, and it never creates row.names().\n\n\ndata <- data.frame(a = 1:26, b = letters[1:26], c = Sys.Date() - 1:26)\ndata\nas_tibble(data)\n\n\nstudents <- read.csv(\"https://pos.it/r4ds-students-csv\") # from URL\nstudents <- read.csv(\"data/students.csv\") # from local\nstudents\nstr(students)\nsummary(students)\n\n\nstudents <- read.csv(\"data/students.csv\", na.strings=c(\"N/A\", \"\"))\n# students <- read_csv(\"data/students.csv\", na = c(\"N/A\", \"\"))\n\n\n❓ Question:: What is are the differences between read.csv and read_csv?\n\n\nBasic data cleaning\n\nstr(students)\nstudents %>%\n  rename(student_id = Student.ID,\n         full_name = Full.Name,\n         fav_food = favourite.food)\n\nrename(students,\n       student_id = Student.ID,\n       full_name = Full.Name,\n       fav_food = favourite.food)\n\nstudents_rename <- students %>%\n                   rename(student_id = Student.ID,\n                          full_name = Full.Name,\n                          fav_food = favourite.food)\n\n\n# a faster way\nstudents_rename <- clean_names(students)\nstudents_rename <- mutate(students_rename, meal_plan = factor(meal_plan))\nstr(students_rename)\n\n\nstudents_clean <- students_rename %>%\n                  mutate(age = if_else(age == \"five\", \"5\", age))\n\n#students_rename2 <- students_rename\n#students_rename2$age <- ifelse(students_rename2$age == \"five\", 5, students_rename2$age)\n#students_rename2$age[students_rename2$age == \"five\"] <- 5\n\nNote: if_else() has three arguments. The first argument test should be a logical vector. The result will contain the value of the second argument, yes, when test is TRUE, and the value of the third argument, no, when it is FALSE. Here we’re saying if age is the character string “five”, make it “5”, and if not leave it as age.\n\nstudents_clean_final <- students_clean %>%\n                        mutate(group = case_when(\n                          age <= 5 ~ \"young\",\n                          age > 5 ~ \"old\",\n                          .default = \"other\"\n                        ))\n\n\n❓ Question: how to use pipe %>% to save some time here?\n\n\nShow the code\nstudents_clean_final <- students %>%\n                        clean_names() %>%\n                        mutate(meal_plan = factor(meal_plan),\n                               age = parse_number(if_else(age == \"five\", \"5\", age)),\n                               group = case_when(\n                                       age <= 5 ~ \"young\",\n                                       age > 5 ~ \"old\",\n                                       .default = \"other\"))\n\n\n\nwrite.csv(students_clean_final, \"data/students_final.csv\", row.names = F)"
  },
  {
    "objectID": "week3.html#data-tidying",
    "href": "week3.html#data-tidying",
    "title": "Week 3 Lab",
    "section": "Data tidying",
    "text": "Data tidying\nIn real life, the social media data you collected is “messy” and “dirty”.\n\nData Scientists spend up to 80% of the time on data cleaning and 20 percent of their time on actual data analysis. 1\n\nThe process of “tidying” data would thus create what’s known as tidy data, as populated by Hadley Wickham (one of the authors of R for Data Science).\n\nTidy datasets provide a standardized way to link the structure of a dataset (its physical layout) with its semantics (its meaning). 2\n\nEach dataset shows the same values of four variables: country, year, population, and number of documented cases of TB (tuberculosis), but each dataset organizes the values in a different way.\n\n❓ Question: are they the same datasets? Which one is easier to work with and why?\n\ntable1\n\n# A tibble: 6 × 4\n  country      year  cases population\n  <chr>       <dbl>  <dbl>      <dbl>\n1 Afghanistan  1999    745   19987071\n2 Afghanistan  2000   2666   20595360\n3 Brazil       1999  37737  172006362\n4 Brazil       2000  80488  174504898\n5 China        1999 212258 1272915272\n6 China        2000 213766 1280428583\n\ntable2\n\n# A tibble: 12 × 4\n   country      year type            count\n   <chr>       <dbl> <chr>           <dbl>\n 1 Afghanistan  1999 cases             745\n 2 Afghanistan  1999 population   19987071\n 3 Afghanistan  2000 cases            2666\n 4 Afghanistan  2000 population   20595360\n 5 Brazil       1999 cases           37737\n 6 Brazil       1999 population  172006362\n 7 Brazil       2000 cases           80488\n 8 Brazil       2000 population  174504898\n 9 China        1999 cases          212258\n10 China        1999 population 1272915272\n11 China        2000 cases          213766\n12 China        2000 population 1280428583\n\ntable3\n\n# A tibble: 6 × 3\n  country      year rate             \n  <chr>       <dbl> <chr>            \n1 Afghanistan  1999 745/19987071     \n2 Afghanistan  2000 2666/20595360    \n3 Brazil       1999 37737/172006362  \n4 Brazil       2000 80488/174504898  \n5 China        1999 212258/1272915272\n6 China        2000 213766/1280428583\n\n\nA tidy data set is:\n\nEach variable is a column; each column is a variable.\nEach observation is a row; each row is an observation.\nEach value is a cell; each cell is a single value.\n\n\n\n\nSource: r4ds\n\n\nLet’s work with the table1 here.\n\n# Compute rate per 10,000\ntb_table <- table1 %>%\n  mutate(rate = cases / population * 10000)\n\n\ntb_year <- table1 %>% \n  group_by(year) %>% \n  summarize(total_cases = sum(cases))\n\n\n\nExercise\nUsing table1 to calculate the TB cases per year in each country. (Hint: use mean())\n\n\nShow the code\ntable1 %>% \n  group_by(country) %>% \n  summarize(mean_cases = mean(cases))"
  },
  {
    "objectID": "week3.html#data-transformation",
    "href": "week3.html#data-transformation",
    "title": "Week 3 Lab",
    "section": "Data transformation",
    "text": "Data transformation\n\nColumns and rows\nflights is a dataset on flights that departed from New York City in 2013.\n\n#install.packages(\"nycflights13\")\nlibrary(nycflights13)\nflights\nstr(flights)\n\n\n# Subseting certain columns\nflights_sub <- flights %>%\n  select(c(month, day, flight, carrier, origin, dest, distance, air_time))\n\n# Creating new columns that are derived from the existing columns\nflights_sub <- flights_sub %>% \n  mutate(speed = distance / air_time * 60)\n\n\n# Filtering certain rows\nflights_IAH <- flights %>%\n  filter(dest == \"IAH\")\n\nflights_summer <- flights %>%\n   filter(month == 6 | month == 7 | month == 8) #OR\n\nflights_summer <- flights %>%\n   filter(month %in% c(6,7,8))\n\nflights_jan1 <- flights %>% \n  filter(month == 1 & day == 1) #AND\n\n\nflights %>% \n  arrange(year, month, day, dep_time)\n\nflights %>% \n  arrange(desc(dep_delay))\n\n\nflights %>% \n  distinct(origin, dest, .keep_all = TRUE)\n\n\nflights %>% \n  group_by(month) %>% \n  summarize(\n    mean_delay = mean(dep_delay, na.rm = TRUE), \n    n = n()\n  )\n\ndaily_flights <- flights %>%\n  group_by(year, month, day) %>%\n  summarize(\n    mean_delay = mean(dep_delay, na.rm = TRUE), \n    n = n())\n\n\n\nExercise\n\nUsing the same flights dataset to creat a new dataframe flights_houston, and:\n\n\nOnly include flights heading to Houston (IAH and HOU)\nCalcuate the speed (\\(speed = distance / air\\_time * 60\\))\nOnly keep the columns of “year”, “month”, “day”,“dep_time”, “carrier”, “flight”, and “speed”\nArrange the data based on the speed with a desceding order.\n\n\n\nShow the code\nflights_houston <- flights %>% \n  filter(dest == \"IAH\" | dest == \"HOU\") %>% \n  mutate(speed = distance / air_time * 60) %>% \n  select(year:day, dep_time, carrier, flight, speed) %>% \n  arrange(desc(speed))\n\n\n\nUsing the same flights dataset to find out which carrier heading to which airport has the worst average delays?\n\n\n\nShow the code\ndelay_flights <- flights %>%\n  group_by(carrier, dest) %>%\n  summarize(\n    mean_delay = mean(dep_delay, na.rm = TRUE), \n    n = n())\n\n\n\n\nLengthening and widening data\nIn reality, you need long-format data much more commonly than wide-format data (such as visualizing in ggplot2 and modeling).\n\nWide format data: it has a column for each variable and a long format data. The billboard dataset records the billboard rank of songs in the year 2000:\n\n\nbillboard\n\n\nLong format data: it has a column for possible variable types and a column for the values of those variables. cms_patient_experience, is a dataset from the Centers of Medicare and Medicaid services that collects data about patient experiences:\n\n\ncms_patient_experience\n\ntidyr provides two functions for pivoting data:\n\npivot_longer(): it takes wide-format data and turn it into long-format data (melt in reshape2). \npivot_wider(): it takes long-format data and turn it into wide-format data (cast in reshape2).\n\n\nbillboard %>% \n  pivot_longer(\n    cols = starts_with(\"wk\"), \n    names_to = \"week\", \n    values_to = \"rank\"\n  )\n\n\nbilloard_longer <- billboard %>% \n                    pivot_longer(\n                      cols = starts_with(\"wk\"), \n                      names_to = \"week\", \n                      values_to = \"rank\",\n                      values_drop_na = TRUE) %>% \n                    mutate(week = parse_number(week))\n\nBut in reality, we might need to deal with multiple variables at the same time… Now, let’s take a look at the who2 dataset, the source of table1 that you saw above.\n\nwho2\n\nThis dataset is collected by the World Health Organisation, records information about tuberculosis diagnoses. There are two columns that are already variables and are easy to interpret: country and year. They are followed by 56 columns like “sp_m_014”, “ep_m_4554”, and “rel_m_3544”. Each column name is made up of three pieces separated by “_“. The first piece,”sp/rel/ep”, describes the method used for the diagnosis, the second piece, “m/f” is the gender (coded as a binary variable in this dataset), and the third piece, “014/1524/2534/3544/4554/5564/65” is the age range (“014” represents “0-14”, for example).\n\nwho2_long <- who2 %>% \n              pivot_longer(\n                cols = !(country:year),\n                names_to = c(\"diagnosis\", \"gender\", \"age\"), \n                names_sep = \"_\",\n                values_to = \"count\"\n              )\n\n\n\nExercise\nThe following (wide) dataset shows the number of points scored by various NBA basketball players in different years. Please tranform it to a long format.\n\nnba <- data.frame(player=c('Joel Embiid', 'Luka Doncic', 'Jayson Tatum', 'Trae Young'),\n                 year1=c(28.5, 27.7, 26.4,25.3),\n                 year2=c(30.6, 28.4, 26.9, 28.4),\n                 year3=c(33.1, 32.4, 30.1, 26.2))\n\n\n\nShow the code\nnba_long <- nba %>%\n  pivot_longer(cols=c('year1', 'year2', 'year3'),\n               names_to='year',\n               values_to='points')\nnba_long"
  },
  {
    "objectID": "week5.html",
    "href": "week5.html",
    "title": "Week 5 Lab",
    "section": "",
    "text": "You will need to have a Google account/gmail (e.g., RIT email).\nLog in Google’s developers console to create a new project: https://console.developers.google.com/\n\n\n\nClick “Enabled APIs & services”, search for “YouTube Data API V3” and enable it.\n\n  \n\nCreate your API key, choose “Public data”, then restrict the API with only the YouTube Data API V3.   \nCreate the credentials for your API keys. Configure your OAuth consent screen, choose internal user type (if you choose external user, extra steps for authorization might be needed), put in your email contact,then you should be able to see the client ID and secret. Copy and save it, and don’t share with others.\n\n   \nLastly you might need to add a redirect url into your client ID. Under Cresidentials, click on the Client ID you created, then put “http://localhost:1410/” into “Authorized redirect URIs”, and save it.\nAdditionally, if you are making it for external usage, you might need to do an extra step for verification. Under the tab of “OAuth consent screen”, click “Publish APP”.\n\n\n\n\n\n\n# install.packages(\"tuber\")\nlibrary(tuber)\nlibrary(tidyverse)\n\n\n\n\n\nclient_id &lt;- \"YOUR CLIENT ID\"\nclient_secret &lt;- \"YOUR CLIENT SECRET\"\nyt_oauth(app_id = client_id,\n         app_secret = client_secret,\n         token = \" \")\nyt_authorized()\n\nIf you sign in with your gmail account, click allow the authorization, then you should be able to see the following page, which mean you have successfully complete the authorization process and you can now close the page. \n\n\n\n\n\n\nWarning\n\n\n\nYou might run into a error message - “HTTP failure: 400” and “Error: redirect_uri_mismatch”. A quick workaround would be running install.packages(\"httpuv\") in your console, which should fix the issue.\n\n\n\n\n\nBefore collecting the comments, you will need the ID of the video. The ID can be identifid from the url. For instance, we are using a video titled “Exposing the Russian Military Unit Behind a Massacre in Bucha | Visual Investigations” from the New York Times channel as an example. You can find the ID for this specific video is IrGZ66uKcl0 (after v=) in the URL. \n\ncomment &lt;- get_all_comments(video_id = \"IrGZ66uKcl0\")\n\n\n\n\n\n\n\nWarning\n\n\n\nYou might run into a error message - “HTTP failure: 401” once in a while, which is highly likely due to the short expiration time for the access token. It seems that there are no systematic ways to fix it for now. But you can remove the .httr-oauth file in your working directory and re-authorize it manually again (so I would suggest you to set up the working directory before you start any project)\n\n\nYou can take a quick glimpse at the comments you collected. You probabbly notice that the numbers of observations are different than what you saw on the webpage. Some of them could be filtered by anti-spam bots or some other reasons related to the API. Most of the variables are pretty much self-explanatory based on their names. To be noted, parentId specifies the ID of the comment for which replies should be retrieved. In other words, if it shows NA, it is not a reply. Currently, YouTube only supports replies only for top-level comments. For more details, please refer to YouTube data API document.\n\n\n\n\n\n\nNote\n\n\n\nTypically, every API will have rate limits, which can restrict the number of times a user or client can access the server within a specified period of time. By default, YouTube data API v3 token grant rate limit is 10,000 grants per day (not necessary equal to the number of comments). You can either apply to raise the daily token limits; or switch to another API when it reaches the rate limits. \n\n\n\nstr(comment)\n\nAdditionally, you can also get the statistics of the whole videos including the count of view, like, favorite, and comment. To be noted, it will be stored in a list rather than a dataframe. But we can transform it into a dataframe which might be easier to merge or analyze later.\n\nstat &lt;- get_stats(video_id = \"IrGZ66uKcl0\")\nstat_df &lt;- as.data.frame(stat)\n\n\n\n\nFirst you need to identify the channel ID. You can go to the about page, click the share icon and copy channel ID. For instance, the channel ID for NYT’s channel is UCvsAa96EzubF7zNHJEzvG2g.\n\nnytstat &lt;- get_channel_stats(\"UCqnbDFdCpuN8CMEg0VuEBqA\")\nnytvideos = yt_search(term=\"\", type=\"video\", channel_id = \"UCqnbDFdCpuN8CMEg0VuEBqA\")\n\nIn the previous chunk, we would like to collect each video ID in the channel. Then we will extract the IDs and iterate the same function get_comment_threads for each ID.\n\nnytcomments &lt;- lapply(as.character(nytvideos$video_id), function(x){get_comment_threads(c(video_id = x), max_results = 20)})\n\n\nnytcomments_df &lt;- nytcomments %&gt;% map_df(as_tibble)\nnytcomments_df$videoId &lt;- as.factor(nytcomments_df$videoId)\nstr(nytcomments_df$videoId )\n\nSimilarly, you can also get the stats from every video.\n\nnyt_videostat &lt;- lapply(as.character(nytvideos$video_id), function(x){get_stats(c(video_id = x))})\nnyt_videostat_df &lt;- nyt_videostat %&gt;% map_df(as_tibble)\nnyt_videostat_df &lt;- nyt_videostat_df %&gt;% distinct(id, keep_all = T)\n\nAnd you can merge the video stat with the comments (for later analysis if needed)\n\nnyt_all &lt;- merge(nytcomments_df, nyt_videostat_df, by.x = \"videoId\", by.y = \"id\", all.x = T)\n\n\n\n\nSometime, you might not need all the videos in a channel. Then you can collect the videos for a specific list. For instance, we are using Russia-Ukraine War playlist under the New York Times YouTube channel as an example. And you can also identify the playlist ID in the URL, and the playlist ID here is PL4CGYNsoW2iC67ssBNT-h7nTLbckKZByR.\n\n\nukraine_list &lt;- get_playlist_items(\n                c(playlist_id = \"PL4CGYNsoW2iC67ssBNT-h7nTLbckKZByR\"),\n                max_results = 200)\n\n\nukraine_comments &lt;- lapply(as.character(ukraine_list$contentDetails.videoId), function(x){ get_comment_threads(c(video_id = x), max_results = 100)})\nukraine_comments_df &lt;- ukraine_comments %&gt;%\n                       map_df(as_tibble)\n\n\nukraine_videostat &lt;- lapply(as.character(ukraine_stat$contentDetails.videoId), function(x){get_stats(c(video_id = x))})\nukraine_videostat_df &lt;- ukraine_videostat %&gt;% map_df(as_tibble)"
  },
  {
    "objectID": "week5.html#get-your-youtube-client-id-and-sercret",
    "href": "week5.html#get-your-youtube-client-id-and-sercret",
    "title": "Week 5 Lab",
    "section": "",
    "text": "You will need to have a Google account/gmail (e.g., RIT email).\nLog in Google’s developers console to create a new project: https://console.developers.google.com/\n\n\n\nClick “Enabled APIs & services”, search for “YouTube Data API V3” and enable it.\n\n  \n\nCreate your API key, choose “Public data”, then restrict the API with only the YouTube Data API V3.   \nCreate the credentials for your API keys. Configure your OAuth consent screen, choose internal user type (if you choose external user, extra steps for authorization might be needed), put in your email contact,then you should be able to see the client ID and secret. Copy and save it, and don’t share with others.\n\n   \nLastly you might need to add a redirect url into your client ID. Under Cresidentials, click on the Client ID you created, then put “http://localhost:1410/” into “Authorized redirect URIs”, and save it.\nAdditionally, if you are making it for external usage, you might need to do an extra step for verification. Under the tab of “OAuth consent screen”, click “Publish APP”."
  },
  {
    "objectID": "week5.html#download-youtube-data-through-tuber-package.",
    "href": "week5.html#download-youtube-data-through-tuber-package.",
    "title": "Week 5 Lab",
    "section": "",
    "text": "# install.packages(\"tuber\")\nlibrary(tuber)\nlibrary(tidyverse)\n\n\n\n\n\nclient_id &lt;- \"YOUR CLIENT ID\"\nclient_secret &lt;- \"YOUR CLIENT SECRET\"\nyt_oauth(app_id = client_id,\n         app_secret = client_secret,\n         token = \" \")\nyt_authorized()\n\nIf you sign in with your gmail account, click allow the authorization, then you should be able to see the following page, which mean you have successfully complete the authorization process and you can now close the page. \n\n\n\n\n\n\nWarning\n\n\n\nYou might run into a error message - “HTTP failure: 400” and “Error: redirect_uri_mismatch”. A quick workaround would be running install.packages(\"httpuv\") in your console, which should fix the issue.\n\n\n\n\n\nBefore collecting the comments, you will need the ID of the video. The ID can be identifid from the url. For instance, we are using a video titled “Exposing the Russian Military Unit Behind a Massacre in Bucha | Visual Investigations” from the New York Times channel as an example. You can find the ID for this specific video is IrGZ66uKcl0 (after v=) in the URL. \n\ncomment &lt;- get_all_comments(video_id = \"IrGZ66uKcl0\")\n\n\n\n\n\n\n\nWarning\n\n\n\nYou might run into a error message - “HTTP failure: 401” once in a while, which is highly likely due to the short expiration time for the access token. It seems that there are no systematic ways to fix it for now. But you can remove the .httr-oauth file in your working directory and re-authorize it manually again (so I would suggest you to set up the working directory before you start any project)\n\n\nYou can take a quick glimpse at the comments you collected. You probabbly notice that the numbers of observations are different than what you saw on the webpage. Some of them could be filtered by anti-spam bots or some other reasons related to the API. Most of the variables are pretty much self-explanatory based on their names. To be noted, parentId specifies the ID of the comment for which replies should be retrieved. In other words, if it shows NA, it is not a reply. Currently, YouTube only supports replies only for top-level comments. For more details, please refer to YouTube data API document.\n\n\n\n\n\n\nNote\n\n\n\nTypically, every API will have rate limits, which can restrict the number of times a user or client can access the server within a specified period of time. By default, YouTube data API v3 token grant rate limit is 10,000 grants per day (not necessary equal to the number of comments). You can either apply to raise the daily token limits; or switch to another API when it reaches the rate limits. \n\n\n\nstr(comment)\n\nAdditionally, you can also get the statistics of the whole videos including the count of view, like, favorite, and comment. To be noted, it will be stored in a list rather than a dataframe. But we can transform it into a dataframe which might be easier to merge or analyze later.\n\nstat &lt;- get_stats(video_id = \"IrGZ66uKcl0\")\nstat_df &lt;- as.data.frame(stat)\n\n\n\n\nFirst you need to identify the channel ID. You can go to the about page, click the share icon and copy channel ID. For instance, the channel ID for NYT’s channel is UCvsAa96EzubF7zNHJEzvG2g.\n\nnytstat &lt;- get_channel_stats(\"UCqnbDFdCpuN8CMEg0VuEBqA\")\nnytvideos = yt_search(term=\"\", type=\"video\", channel_id = \"UCqnbDFdCpuN8CMEg0VuEBqA\")\n\nIn the previous chunk, we would like to collect each video ID in the channel. Then we will extract the IDs and iterate the same function get_comment_threads for each ID.\n\nnytcomments &lt;- lapply(as.character(nytvideos$video_id), function(x){get_comment_threads(c(video_id = x), max_results = 20)})\n\n\nnytcomments_df &lt;- nytcomments %&gt;% map_df(as_tibble)\nnytcomments_df$videoId &lt;- as.factor(nytcomments_df$videoId)\nstr(nytcomments_df$videoId )\n\nSimilarly, you can also get the stats from every video.\n\nnyt_videostat &lt;- lapply(as.character(nytvideos$video_id), function(x){get_stats(c(video_id = x))})\nnyt_videostat_df &lt;- nyt_videostat %&gt;% map_df(as_tibble)\nnyt_videostat_df &lt;- nyt_videostat_df %&gt;% distinct(id, keep_all = T)\n\nAnd you can merge the video stat with the comments (for later analysis if needed)\n\nnyt_all &lt;- merge(nytcomments_df, nyt_videostat_df, by.x = \"videoId\", by.y = \"id\", all.x = T)\n\n\n\n\nSometime, you might not need all the videos in a channel. Then you can collect the videos for a specific list. For instance, we are using Russia-Ukraine War playlist under the New York Times YouTube channel as an example. And you can also identify the playlist ID in the URL, and the playlist ID here is PL4CGYNsoW2iC67ssBNT-h7nTLbckKZByR.\n\n\nukraine_list &lt;- get_playlist_items(\n                c(playlist_id = \"PL4CGYNsoW2iC67ssBNT-h7nTLbckKZByR\"),\n                max_results = 200)\n\n\nukraine_comments &lt;- lapply(as.character(ukraine_list$contentDetails.videoId), function(x){ get_comment_threads(c(video_id = x), max_results = 100)})\nukraine_comments_df &lt;- ukraine_comments %&gt;%\n                       map_df(as_tibble)\n\n\nukraine_videostat &lt;- lapply(as.character(ukraine_stat$contentDetails.videoId), function(x){get_stats(c(video_id = x))})\nukraine_videostat_df &lt;- ukraine_videostat %&gt;% map_df(as_tibble)"
  },
  {
    "objectID": "week3.html#rstudio-orientation",
    "href": "week3.html#rstudio-orientation",
    "title": "Week 3 Lab",
    "section": "",
    "text": "There are mainly four panels once you open the IDE (you can modify the theme and the layout under File - Preference - Appearance/Pane layout).\n\nConsole\nEnvironment/History/…\nFiles/Plots/Viewer/…\nSources"
  },
  {
    "objectID": "week3.html#r-basic-operators",
    "href": "week3.html#r-basic-operators",
    "title": "Week 3 Lab",
    "section": "R basic operators",
    "text": "R basic operators\n\nArithmetic operators\n\n1 + 19 # addition\n19 - 1 # subtraction\n5 * 4 # multiplication\n10 / 2 # division\n11 %/% 2 # integer division\n41 %% 21 # modulus\n20 ^ 2 # exponents\n20 ** 2\n\n\ndata &lt;- data.frame(x1 = 1:3,  \n                      x2 = 2:4,\n                      x3 = 2)\ndata \n\n\ndata^2\n\n\n\nThe &lt;- operator\nAssignment is a binary operator: the left side is a symbol/variable/object, the right is a value/expression being assigned to the left.\n\nx &lt;- 1\nx &lt;- c(1, 2, 3, 4, 5)\nx &lt;- data.frame(x1 = 1:3,  \n                x2 = 2:4,\n                x3 = 2)\n\n\n\nThe [] operator\nIndexing is a binary operator (two operands: the object being indexed (e.g., a vector, list, or data frame) and the index or indices used to select specific elements from that object. )\n\nx &lt;- c(5, 4, 3, 2, 1)\nx[1] # Extracts the first element\n\n\nx &lt;- data.frame(x1 = 1:3,  \n                x2 = 2:4,\n                x3 = 2)\nx[3] \nx[,3]\nx[3,]\nx[3,2]\nx[\"x3\"]\n\n\n\nThe $ operator\nThe $ operator is used to extract or subset a specific part of a data object in R.\n\nExtract the values in a data frame columns\n\n\ndata &lt;- data.frame(x1 = 1:5,  # Create example data\n                   x2 = letters[1:5],\n                   x3 = 9)\ndata  \n\n\ndata$x2\n\n\nReturn specific list elements\n\n\nmy_list &lt;- list(A = 1:5,  # Create example list\n                B = letters[1:5],\n                C = 9)\nmy_list # Print example list\n\n\nmy_list$B # Extract element of list\n\n\n\nThe () operator\nA function call is also a binary operator as the left side is a symbol pointing to the function argument and the right side are the arguments\n\nmax(1,2)\nx &lt;- max(1,2)\n\n\n\nThe ? operator\n\n?: Search R documentation for a specific term.\n??: Search R help files for a word or phrase.\n\n\n\nThe %&gt;% or |&gt; opertor\n%&gt;% is a longstanding feature of the magrittr package for R. It takes the output of one function and passes it into another function as an argument. This allows us to link a sequence of analysis steps (think about a a conveyor belt in a factory)\n\nReadability and clarity\nEast of modification\nAvoid intermediate variables\n\n\nlibrary(tidyverse)\n?group_by\n?mtcars\nmtcars\n\nx &lt;- filter(mtcars, cyl == 6)\ny &lt;- select(x, c(\"mpg\", \"hp\"))\n\nmtcars %&gt;%\n  filter(cyl == 6) %&gt;%\n  select(mpg, hp)\n\n\nresult &lt;- mtcars %&gt;%\n  group_by(cyl) %&gt;%\n  summarise(meanMPG = mean(mpg))\n\n\n\nThe %in% opertor\n%in% is a matching feature to check if the values of the first argument are present in the second argument and returns a logical vector indicating if there is a match or not for its left operand. Here, the first and second arguments can be a value, vector, list, or sequence.\n\n# Check value in a Vector\n67 %in% c(2,5,8,23,67,34)\n45 %in% c(2,5,8,23,67,34)\n\n# Check values from one vector present in another vector\nvec1 &lt;- c(2,5,8,23,67,34)\nvec2 &lt;- c(1,2,8,34) \nvec2 %in% vec1\n\n# Check values in a dataframe\ndf=data.frame(\n  emp_id=c(1,2,3,5),\n  name=c(\"John\",\"Rose\",\"Williams\", \"Ray\"),\n  dept_id=c(10,20,10,50)\n)\n\ndf$dept_state &lt;- if_else(df$dept_id %in% c(10,50),'NY','CA')\ndf\n\n\ndf2 &lt;- df[df$name %in% c('Ray','Rose'), ]\ndf2"
  },
  {
    "objectID": "week3.html#data-type-in-r",
    "href": "week3.html#data-type-in-r",
    "title": "Week 3 Lab",
    "section": "Data type in R",
    "text": "Data type in R\n\n# numeric (double if with more than two decimal numbers)\nx &lt;- 10.5\nclass(x)\n\n# integer\nx &lt;- 1000L\nclass(x)\n\n\n# complex\nx &lt;- 9i + 3\nclass(x)\n\n# character/string\nx &lt;- \"R is exciting\"\nclass(x)\n\n# logical/boolean\nx &lt;- TRUE\nclass(x)\n\n# date\nx = \"01-06-2021\"\nx = as.Date(x, \"%d-%m-%Y\")\nclass(x)\n\n# Factors\n## Factors are the data objects which are used to categorize the data and store it as levels. \n## They can store both strings and integers. They are useful in the columns which have a limited number of unique values. Like Male/Female and True/False, etc. They are useful in data analysis for statistical modeling.\nx &lt;- c(\"East\",\"South\",\"East\",\"North\",\"North\",\"East\",\"West\",\"West\",\"West\",\"South\",\"North\")\nx_factor &lt;- factor(x) ### as.factor\nx_factor2 &lt;- factor(x, levels = c(\"East\", \"West\", \"South\", \"North\"))\nsummary(x_factor)\nsummary(x_factor2)\n\n# Missing values\nx &lt;- c(1, 2, NA, 4)\nis.na(x)\nwhich(is.na(x))\nx_omit &lt;- na.omit(x)\n\n\nNotes on NA\n\nA missing value in a factor variable is displayed as &lt;NA&gt; rather than just NA.\nR has a special value NaN for “not a number.” 0/0 is an example of a calculation that will produce a NaN. NaNs print as NaN, but generally act like NAs.\nAnother special case is Inf, such as log(0)\n\n\n\nExercise\n\nCreate a new R script\nInstall quanteda package (for textual analysis later in the semester) and load it\nCreate a variable called first_num and assign it the value of 605\nCreate a variable called first_char and assign it the value of my first character\nCreate a vector called gender, including: “male”, “female”, “other”, “female”, “male”, “female”, “female”, “other”, “male”. Make gender as a factor vector, following the order of “female”, “other”, and “male”."
  },
  {
    "objectID": "week3.html#footnotes",
    "href": "week3.html#footnotes",
    "title": "Week 3 Lab",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nDouglas, A. et al. (2023) An Introduction to R↩︎"
  },
  {
    "objectID": "week4.html",
    "href": "week4.html",
    "title": "Week 4 Lab",
    "section": "",
    "text": "Source: r4ds\nData wrangling/manipulation is the most important part when we deal with any types of data - before visualization and modeling. We will be mainly following R for Data Science, which includes importing data, transforming data, and “tidying” data.\nlibrary(tidyverse)\nlibrary(janitor)\ngetwd()\n#setwd(\"~/COMM 605/Tutorial/\") # no need to set up if you are opening a Rproject"
  },
  {
    "objectID": "week4.html#understanding-data-frame",
    "href": "week4.html#understanding-data-frame",
    "title": "Week 4 Lab",
    "section": "Understanding data frame",
    "text": "Understanding data frame\n\nCreate a data frame\n\nheight &lt;- c(180, 155, 160, 167, 181)\nweight &lt;- c(65, 50, 52, 58, 70)\nnames &lt;- c(\"Joanna\", \"Charlotte\", \"Helen\", \"Karen\", \"Amy\")\n\ndata &lt;- data.frame(height = height, weight = weight, names = names) #stringsAsFactors = TRUE\ndata\ndim(data)\nstr(data)\n\n\n\nPositional index\n\ndata[1,3] # the first value (1st row ) of the names variable (3th column)\ndata$names[1]\ndata[1:2, 2:3] #  the first 2 rows and the last 2 columns\ndata[1:2, ]\ndata[, 2:3]\n\n\n\nOrdering data frames\n\nheight_order &lt;- data[order(data$height), ]\nrownames(height_order) &lt;- 1:nrow(height_order)\nheight_order\n\nheight_order &lt;- data[order(data$height, decreasing = T),]\n\n\n\nAdding/Removing columns and rows\n\ndata2 &lt;- data.frame(state = c(\"NY\", \"PA\", \"MD\", \"VA\", \"MA\"))\ndata_newcolumn &lt;- cbind(data, data2)\n\ndata_removecolumn &lt;- data_newcolumn[, c(1:2, 4)]\ndata_newcolumn$state &lt;- NULL\n\ndata3 &lt;- data.frame(height = c(120, 150, 132, 122),\n                    weight = c(44, 56, 49, 45),\n                    names = c(\"Ryan\", \"Chris\", \"Ben\", \"John\"))\ndata_newrow &lt;- rbind(data, data3)\ndata_removerow &lt;- data_newrow[c(1,6:9),]\n\n\n\nMerging data frames\nHere are two fictitious datasets of a clinical trial. One table contains demographic information of the patients and the other one adverse events recorded throughout the course of the trial.\n\ndemographics &lt;- data.frame(\n  id = c(\"P1\", \"P2\", \"P3\"),\n  age = c(40, 54, 47),\n  state = c(\"NY\", \"MA\", \"PA\"),\n  stringsAsFactors = FALSE\n)\n\nadverse_events &lt;- data.frame(\n  id = c(\"P1\", \"P1\", \"P3\", \"P4\"),\n  term = c(\"Headache\", \"Neutropenia\", \"Constipation\", \"Tachycardia\"),\n  onset_date = c(\"2020-12-03\", \"2021-01-03\", \"2020-11-29\", \"2021-01-27\"),\n  stringsAsFactors = FALSE\n)\n\n\nmerge(demographics, adverse_events, by = \"id\")\nmerge(demographics, adverse_events, by = \"id\", all.x = T)\nmerge(demographics, adverse_events, by = \"id\", all.y = T)\nmerge(demographics, adverse_events, by = \"id\", all = T)\n\n\nadverse_events2 &lt;- adverse_events\ncolnames(adverse_events2)[1] &lt;- \"pat_id\"\nmerge(demographics, adverse_events2, by.x = \"id\", by.y = \"pat_id\", all = T)\n\n\n\nExercise\nPlease merge the following two datasets emp_df (employee information)and dept_df (department information) using two ID columns dept_id and dept_branch_id.\n\nemp_df=data.frame(\n  emp_id=c(1,2,3,4,5,6),\n  name=c(\"Chris\",\"Rose\",\"Williams\",\"Jones\",\"Jayden\",\"Xavior\"),\n  superior_emp_id=c(1,1,1,2,2,2),\n  dept_id=c(10,20,10,10,40,30),\n  dept_branch_id= c(101,102,101,101,104,103)\n)\n\ndept_df=data.frame(\n  dept_id=c(10,20,30,40),\n  dept_name=c(\"Finance\",\"Marketing\",\"Sales\",\"IT\"),\n  dept_branch_id= c(101,102,103,104)\n)\n\n\n\nShow the code\nmerge(emp_df, dept_df, by = c(\"dept_id\", \"dept_branch_id\"), all.x = T)"
  },
  {
    "objectID": "week4.html#data-importing-and-exporting",
    "href": "week4.html#data-importing-and-exporting",
    "title": "Week 4 Lab",
    "section": "Data importing (and exporting)",
    "text": "Data importing (and exporting)\nCommon data formats:\n\nCSV (comma-seperated values) / TSV (tab-seperated values)\nxlsx\ntxt\nother softwares/packages: .sav(SPSS), .dta(STATA)\n\nCommon data types in R:\n\ndata frame\ntibble (tbl_df): it does much less than a data frame (a neater data frame), as it never changes the type of the inputs (e.g. it keeps list columns unchanged, and never converts strings to factors), it never changes the names of variables, it only recycles inputs of length 1, and it never creates row.names().\n\n\ndata &lt;- data.frame(a = 1:26, b = letters[1:26], c = Sys.Date() - 1:26)\ndata\nas_tibble(data)\n\n\nstudents &lt;- read.csv(\"https://pos.it/r4ds-students-csv\") # from URL\nstudents &lt;- read.csv(\"data/students.csv\") # from local\nstudents\nstr(students)\nsummary(students)\n\n\nstudents &lt;- read.csv(\"data/students.csv\", na.strings=c(\"N/A\", \"\"))\n# students &lt;- read_csv(\"data/students.csv\", na = c(\"N/A\", \"\"))\n\n\n❓ Question:: What is are the differences between read.csv and read_csv?\n\n\nBasic data cleaning\n\nstr(students)\nstudents %&gt;%\n  rename(student_id = Student.ID,\n         full_name = Full.Name,\n         fav_food = favourite.food)\n\nrename(students,\n       student_id = Student.ID,\n       full_name = Full.Name,\n       fav_food = favourite.food)\n\nstudents_rename &lt;- students %&gt;%\n                   rename(student_id = Student.ID,\n                          full_name = Full.Name,\n                          fav_food = favourite.food)\n\n\n# a faster way\nstudents_rename &lt;- clean_names(students)\nstudents_rename &lt;- mutate(students_rename, meal_plan = factor(meal_plan))\nstr(students_rename)\n\n\nstudents_clean &lt;- students_rename %&gt;%\n                  mutate(age = if_else(age == \"five\", \"5\", age))\n\n#students_rename2 &lt;- students_rename\n#students_rename2$age &lt;- ifelse(students_rename2$age == \"five\", 5, students_rename2$age)\n#students_rename2$age[students_rename2$age == \"five\"] &lt;- 5\n\nNote: if_else() has three arguments. The first argument test should be a logical vector. The result will contain the value of the second argument, yes, when test is TRUE, and the value of the third argument, no, when it is FALSE. Here we’re saying if age is the character string “five”, make it “5”, and if not leave it as age.\n\nstudents_clean_final &lt;- students_clean %&gt;%\n                        mutate(group = case_when(\n                          age &lt;= 5 ~ \"young\",\n                          age &gt; 5 ~ \"old\",\n                          .default = \"other\"\n                        ))\n\n\n❓ Question: how to use pipe %&gt;% to save some time here?\n\n\nShow the code\nstudents_clean_final &lt;- students %&gt;%\n                        clean_names() %&gt;%\n                        mutate(meal_plan = factor(meal_plan),\n                               age = parse_number(if_else(age == \"five\", \"5\", age)),\n                               group = case_when(\n                                       age &lt;= 5 ~ \"young\",\n                                       age &gt; 5 ~ \"old\",\n                                       .default = \"other\"))\n\n\n\nwrite.csv(students_clean_final, \"data/students_final.csv\", row.names = F)"
  },
  {
    "objectID": "week4.html#data-tidying",
    "href": "week4.html#data-tidying",
    "title": "Week 4 Lab",
    "section": "Data tidying",
    "text": "Data tidying\nIn real life, the social media data you collected is “messy” and “dirty”.\n\nData Scientists spend up to 80% of the time on data cleaning and 20 percent of their time on actual data analysis. 1\n\nThe process of “tidying” data would thus create what’s known as tidy data, as populated by Hadley Wickham (one of the authors of R for Data Science).\n\nTidy datasets provide a standardized way to link the structure of a dataset (its physical layout) with its semantics (its meaning). 2\n\nEach dataset shows the same values of four variables: country, year, population, and number of documented cases of TB (tuberculosis), but each dataset organizes the values in a different way.\n\n❓ Question: are they the same datasets? Which one is easier to work with and why?\n\ntable1\n\n# A tibble: 6 × 4\n  country      year  cases population\n  &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;\n1 Afghanistan  1999    745   19987071\n2 Afghanistan  2000   2666   20595360\n3 Brazil       1999  37737  172006362\n4 Brazil       2000  80488  174504898\n5 China        1999 212258 1272915272\n6 China        2000 213766 1280428583\n\ntable2\n\n# A tibble: 12 × 4\n   country      year type            count\n   &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;           &lt;dbl&gt;\n 1 Afghanistan  1999 cases             745\n 2 Afghanistan  1999 population   19987071\n 3 Afghanistan  2000 cases            2666\n 4 Afghanistan  2000 population   20595360\n 5 Brazil       1999 cases           37737\n 6 Brazil       1999 population  172006362\n 7 Brazil       2000 cases           80488\n 8 Brazil       2000 population  174504898\n 9 China        1999 cases          212258\n10 China        1999 population 1272915272\n11 China        2000 cases          213766\n12 China        2000 population 1280428583\n\ntable3\n\n# A tibble: 6 × 3\n  country      year rate             \n  &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;            \n1 Afghanistan  1999 745/19987071     \n2 Afghanistan  2000 2666/20595360    \n3 Brazil       1999 37737/172006362  \n4 Brazil       2000 80488/174504898  \n5 China        1999 212258/1272915272\n6 China        2000 213766/1280428583\n\n\nA tidy data set is:\n\nEach variable is a column; each column is a variable.\nEach observation is a row; each row is an observation.\nEach value is a cell; each cell is a single value.\n\n\n\n\nSource: r4ds\n\n\nLet’s work with the table1 here.\n\n# Compute rate per 10,000\ntb_table &lt;- table1 %&gt;%\n  mutate(rate = cases / population * 10000)\n\n\ntb_year &lt;- table1 %&gt;% \n  group_by(year) %&gt;% \n  summarize(total_cases = sum(cases))\n\n\n\nExercise\nUsing table1 to calculate the TB cases per year in each country. (Hint: use mean())\n\n\nShow the code\ntable1 %&gt;% \n  group_by(country) %&gt;% \n  summarize(mean_cases = mean(cases))"
  },
  {
    "objectID": "week4.html#data-transformation",
    "href": "week4.html#data-transformation",
    "title": "Week 4 Lab",
    "section": "Data transformation",
    "text": "Data transformation\n\nColumns and rows\nflights is a dataset on flights that departed from New York City in 2013.\n\n#install.packages(\"nycflights13\")\nlibrary(nycflights13)\nflights\nstr(flights)\n\n\n# Subseting certain columns\nflights_sub &lt;- flights %&gt;%\n  select(c(month, day, flight, carrier, origin, dest, distance, air_time))\n\n# Creating new columns that are derived from the existing columns\nflights_sub &lt;- flights_sub %&gt;% \n  mutate(speed = distance / air_time * 60)\n\n\n# Filtering certain rows\nflights_IAH &lt;- flights %&gt;%\n  filter(dest == \"IAH\")\n\nflights_summer &lt;- flights %&gt;%\n   filter(month == 6 | month == 7 | month == 8) #OR\n\nflights_summer &lt;- flights %&gt;%\n   filter(month %in% c(6,7,8))\n\nflights_jan1 &lt;- flights %&gt;% \n  filter(month == 1 & day == 1) #AND\n\n\nflights %&gt;% \n  arrange(year, month, day, dep_time)\n\nflights %&gt;% \n  arrange(desc(dep_delay))\n\n\nflights %&gt;% \n  distinct(origin, dest, .keep_all = TRUE)\n\n\nflights %&gt;% \n  group_by(month) %&gt;% \n  summarize(\n    mean_delay = mean(dep_delay, na.rm = TRUE), \n    n = n()\n  )\n\ndaily_flights &lt;- flights %&gt;%\n  group_by(year, month, day) %&gt;%\n  summarize(\n    mean_delay = mean(dep_delay, na.rm = TRUE), \n    n = n())\n\n\n\nExercise\n\nUsing the same flights dataset to creat a new dataframe flights_houston, and:\n\n\nOnly include flights heading to Houston (IAH and HOU)\nCalcuate the speed (\\(speed = distance / air\\_time * 60\\))\nOnly keep the columns of “year”, “month”, “day”,“dep_time”, “carrier”, “flight”, and “speed”\nArrange the data based on the speed with a desceding order.\n\n\n\nShow the code\nflights_houston &lt;- flights %&gt;% \n  filter(dest == \"IAH\" | dest == \"HOU\") %&gt;% \n  mutate(speed = distance / air_time * 60) %&gt;% \n  select(year:day, dep_time, carrier, flight, speed) %&gt;% \n  arrange(desc(speed))\n\n\n\nUsing the same flights dataset to find out which carrier heading to which airport has the worst average delays?\n\n\n\nShow the code\ndelay_flights &lt;- flights %&gt;%\n  group_by(carrier, dest) %&gt;%\n  summarize(\n    mean_delay = mean(dep_delay, na.rm = TRUE), \n    n = n())\n\n\n\n\nLengthening and widening data\nIn reality, you need long-format data much more commonly than wide-format data (such as visualizing in ggplot2 and modeling).\n\nWide format data: it has a column for each variable and a long format data. The billboard dataset records the billboard rank of songs in the year 2000:\n\n\nbillboard\n\n\nLong format data: it has a column for possible variable types and a column for the values of those variables. cms_patient_experience, is a dataset from the Centers of Medicare and Medicaid services that collects data about patient experiences:\n\n\ncms_patient_experience\n\ntidyr provides two functions for pivoting data:\n\npivot_longer(): it takes wide-format data and turn it into long-format data (melt in reshape2). \npivot_wider(): it takes long-format data and turn it into wide-format data (cast in reshape2).\n\n\nbillboard %&gt;% \n  pivot_longer(\n    cols = starts_with(\"wk\"), \n    names_to = \"week\", \n    values_to = \"rank\"\n  )\n\n\nbilloard_longer &lt;- billboard %&gt;% \n                    pivot_longer(\n                      cols = starts_with(\"wk\"), \n                      names_to = \"week\", \n                      values_to = \"rank\",\n                      values_drop_na = TRUE) %&gt;% \n                    mutate(week = parse_number(week))\n\nBut in reality, we might need to deal with multiple variables at the same time… Now, let’s take a look at the who2 dataset, the source of table1 that you saw above.\n\nwho2\n\nThis dataset is collected by the World Health Organisation, records information about tuberculosis diagnoses. There are two columns that are already variables and are easy to interpret: country and year. They are followed by 56 columns like “sp_m_014”, “ep_m_4554”, and “rel_m_3544”. Each column name is made up of three pieces separated by “_”. The first piece,“sp/rel/ep”, describes the method used for the diagnosis, the second piece, “m/f” is the gender (coded as a binary variable in this dataset), and the third piece, “014/1524/2534/3544/4554/5564/65” is the age range (“014” represents “0-14”, for example).\n\nwho2_long &lt;- who2 %&gt;% \n              pivot_longer(\n                cols = !(country:year),\n                names_to = c(\"diagnosis\", \"gender\", \"age\"), \n                names_sep = \"_\",\n                values_to = \"count\"\n              )\n\n\n\nExercise\nThe following (wide) dataset shows the number of points scored by various NBA basketball players in different years. Please tranform it to a long format.\n\nnba &lt;- data.frame(player=c('Joel Embiid', 'Luka Doncic', 'Jayson Tatum', 'Trae Young'),\n                 year1=c(28.5, 27.7, 26.4,25.3),\n                 year2=c(30.6, 28.4, 26.9, 28.4),\n                 year3=c(33.1, 32.4, 30.1, 26.2))\n\n\n\nShow the code\nnba_long &lt;- nba %&gt;%\n  pivot_longer(cols=c('year1', 'year2', 'year3'),\n               names_to='year',\n               values_to='points')\nnba_long"
  },
  {
    "objectID": "week4.html#footnotes",
    "href": "week4.html#footnotes",
    "title": "Week 4 Lab",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nDasu, T. & Johnson, T. (2003). Exploratory Data Mining and Data Cleaning.↩︎\nWickham, H. (2014). Journal of Statistical Software, 59(10), 1–23. https://doi.org/10.18637/jss.v059.i10↩︎"
  },
  {
    "objectID": "week5.html#collect-reddit-data-through-redditextractor-package.",
    "href": "week5.html#collect-reddit-data-through-redditextractor-package.",
    "title": "Week 5 Lab",
    "section": "Collect Reddit data through RedditExtractoR package.",
    "text": "Collect Reddit data through RedditExtractoR package.\n\nLoading package\n\n# install.packages(\"RedditExtractoR\")\nlibrary(RedditExtractoR)\n\n\n\nObtaining the urls from r/Rochester subreddit\n\nrocthreads &lt;- find_thread_urls(subreddit = \"Rochester\", sort_by = \"top\", period = \"all\") # you can use keywords to limit the search as well\n\n\nroccomments &lt;- get_thread_content(rocthreads$url)\n\nYou can find there are two dataframes the large list, the threads is for the posts, and the comments is for the comments. Similarily, you can also merge the metadata of posts with the comments based on the common column url.\n\nreddit_posts &lt;- roccomments$threads\nreddit_comments &lt;- roccomments$comments\nreddit_all &lt;- merge(reddit_comments, reddit_posts, by = \"url\", all.x = T)"
  },
  {
    "objectID": "week6.html",
    "href": "week6.html",
    "title": "Week 6 Lab",
    "section": "",
    "text": "We will be mainly using ggplot2 package (and maybe other extended packages) for data visualization in this class. ggplot2 is a system for declaratively creating graphics, based on The Grammar of Graphics. This lab is just a quick glimpse on ggplot2. And if you would like to learn more, check out Hadley Wickham’s ggplot2: Elegant Graphics for Data Analysis. Here is a cheatsheet by posit."
  },
  {
    "objectID": "week6.html#a-breakdown-of-the-common-layers",
    "href": "week6.html#a-breakdown-of-the-common-layers",
    "title": "Week 6 Lab",
    "section": "A breakdown of the common layers",
    "text": "A breakdown of the common layers\n\nData:\n\nyour data, in tidy format, will provide ingredients for your plot\nuse dplyr techniques to prepare data for optimal plotting format\nusually, this means you should have one row for every observation that you want to plot\n\nAesthetics (aes), to make data visible\n\nx, y: variable along the x and y axis\ncolour: color of geoms according to data\nfill: the inside color of the geom\ngroup: what group a geom belongs to\nshape: the figure used to plot a point\nlinetype: the type of line used (solid, dashed, etc)\nsize: size scaling for an extra dimension\nalpha: the transparency of the geom\n\nGeometric objects (geoms - determines the type of plot)\n\ngeom_point(): scatterplot\ngeom_line(): lines connecting points by increasing value of x\ngeom_path(): lines connecting points in sequence of appearance\ngeom_boxplot(): box and whiskers plot for categorical variables\ngeom_bar(): bar charts for categorical x axis\ngeom_histogram(): histogram for continuous x axis\ngeom_violin(): distribution kernel of data dispersion\ngeom_smooth(): function line based on data\n\nFacets\n\nfacet_wrap() or facet_grid() for small multiples\n\nStatistics\n\nsimilar to geoms, but computed\nshow means, counts, and other statistical summaries of data\n\nCoordinates - fitting data onto a page\n\ncoord_cartesian to set limits\ncoord_polar for circular plots\ncoord_map for different map projections\n\nThemes\n\noverall visual defaults\nfonts, colors, shapes, outlines\n\n\nOur goal: putting all the layers together!"
  }
]