[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to COMM 605!",
    "section": "",
    "text": "Source: xkcd.com\n\n\n\n\n\n⏰ Time\nMonday 5:00 – 7:50 PM\n\n\n📍 Location\nLiberal Arts Hall (LBR)- Room 3201\n\n\n📭 Email\nryanwang@mail.rit.edu\n\n\n🏢 Office\nRoom 3041 Eastman Building OR Zoom (By appointment)\n\n\n\n\nCourse overview\nThis is the tutorial website for COMM 605 Social media analytics and research at Rochester Institute of Technology. All the codes in the tutorials are created by the instructor and open access resources, including but not limited to:\n\nWickham, H., Çetinkaya-Rundel, M., & Grolemund, G. (2023). R for data science (2nd version).\nLiang, H. COMM3650: Social Media Analytics for Communication Professionals.\nvan Atteveldt, W., Trilling, D. & Calderón, C. A. (2022). Computational Analysis of Communication\n\nThis course focuses on social media research with various methodological approaches to study public data, users and messages. Students will be introduced to a variety of techniques and concepts used to obtain, monitor and evaluate social media content with a focus on how the analytics could inform communication strategies. During the course, students will also learn how to design and evaluate social media-based research studies.\n\n\nObjectives\nStudents who successfully complete assigned coursework should develop the following skills:\n\nexplain the theoretical and methodological perspectives that guide social media research;\napply computational tools and practice basic data science skills to answer empirical research questions and conduct social science research;\nidentify major milestones in social media platforms and research;\ncritique on how algorithms and (social media)data may affect civil societies in both positive and adverse ways.\n\n\n\nSuggested readings\n\nRequired: Salganik, M. J. (2017). Bit by bit: Social research in the digital age.\nSuggested: Grimmer, J., Roberts, M. E., & Stewart, B. M. (2022). Text as data: A new framework for machine learning and the social sciences.\nSuggested: Borgatti, S. P., Everett, M. G., & Johnson, J. C. (2018). Analyzing social networks."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "week3.html",
    "href": "week3.html",
    "title": "Week 3 Lab",
    "section": "",
    "text": "There are mainly four panels once you open the IDE (you can modify the theme and the layout under File - Preference - Appearance/Pane layout).\n\nConsole\nEnvironment/History/…\nFiles/Plots/Viewer/…\nSources"
  },
  {
    "objectID": "materials.html",
    "href": "materials.html",
    "title": "Week 3 Lab",
    "section": "",
    "text": "This is the first lab.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.3     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors"
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Course schedule",
    "section": "",
    "text": "Weeks\nTopics\n\n\n\n\nWeek 1: Aug 28\nClass overview and set-up\n\n\nWeek 2: Sep 4\nLabor Day - No Classes 😃\n\n\nWeek 3: Sep 11\nOverview of social media research\nLab: Get familiar with R/R Studio and data wrangling\n\n\nWeek 4: Sep 18\nWhat (social media) data is out there?\nLab: Data collection – API\n\n\nWeek 5: Sep 25\nHow to utilize social media data?\nLab: Data collection – Web scraping\nFinal project idea due Oct 1\n\n\nWeek 6: Oct 2\nThe pitfall of social media data\nLab: Data visualization\n\n\nWeek 7: Oct 9\nFall break - No Classes 😃\nPractice 1 due on Oct 15\n\n\nWeek 8: Oct 16\nCase study: Text-as-data I\nLab: Basic text analysis\n\n\nWeek 9: Oct 23\nCase study: Text-as-data II\nLab: Topic modeling\n\n\nWeek 10: Oct 30\nCase study: Text-as-data III\nLab: Sentiment analysis\nPractice 2 due on Nov 5\n\n\nWeek 11: Nov 6\nCase study: Network I\nLab: Network analysis\n\n\nWeek 12: Nov 13\nCase study: Network II\nLab: Network analysis & visualization\n\n\nWeek 13: Nov 20\nCourse wrap-up\nPractice 3 due on Nov 26\n\n\nWeek 14: Nov 27\nIndividual meetings & consultant\n\n\nWeek 15: Dec 4\nProject presentation\n\n\nWeek 16: Dec 11\nFinal paper due"
  },
  {
    "objectID": "week3.html#reproducible-research",
    "href": "week3.html#reproducible-research",
    "title": "Week 3 Lab",
    "section": "Reproducible research",
    "text": "Reproducible research\nReproducible: Can someone else reproduce your entire analysis?\n\nAvailable data\nAvailable codes (including the random seed for machine learning)\n\nWe will be mainly using two types of file formats (other related formats such as Rproj and Rdata:\n\nR script (a text file contains the same commands that your would enter on the command line of R)\nRMarkdown\n\nText, code, and results (from your analysis)\nFormatted output: html, pdf (which requires tex, a typesetting system), etc.\nResource: cheatsheet, The Definitive Guide\n\n\nR markdown is a simple and easy to use plain text language used to combine your R code, results from your data analysis (including plots and tables) and written commentary into a single nicely formatted and reproducible document (like a report, publication, thesis chapter or a web page like this one).1\n\n\nOther examples of markup languages include (compared with Word):\n\nHTML (HyperText Markup Language): website\nLaTex: Overleaf\nMarkdown (a “lightweight” markup language)"
  },
  {
    "objectID": "week3.html#r-packages",
    "href": "week3.html#r-packages",
    "title": "Week 3 Lab",
    "section": "R packages",
    "text": "R packages\nUntil Sep 11th, 2023, there are 19861 available packages on CRAN (The Comprehensive R Archive Network) package repository.\n\n# Install packages\n## install.packages(\"xxx\") - if on CRAN\n## devtools::install_github(\"houyunhuang/ggcor\") - if only on github\n\n\n# Loading packages\nlibrary(dplyr)\n\n\n# An elegant way to install and load packages by using a loop\n if (!require(dplyr)) {\n    install.packages(\"dplyr\")\n    library(dplyr)\n}\n\n# With multiple packages\nsomepackages &lt;- c(\"dplyr\", \"plyr\", \"magrittr\")\n\nfor (pkg in somepackages) {\n  if (!require(pkg, character.only = TRUE)) {\n    install.packages(pkg)\n    library(pkg,character.only = TRUE)\n  }\n}\n\n\n# Unload packages\ndetach(\"package:dplyr\", unload = TRUE)\n\n# Remove packages\n## remove.packages(\"dplyr\")\n\n\nThe chunk options in R code\nGlobal options: knitr::opts_chunk$set(echo = FALSE)\n\nIn this example, the knitr::opts_chunk$set(echo = FALSE) line in the setup chunk tells R Markdown to hide the R code within code chunks for the entire document, except when overridden within individual code chunks using {r} options. This is often used to create clean and readable reports or documents where you want to present the results of your R code without cluttering the document with the code itself.\n\nOther (individual) chunk options:\n\ninclude = FALSE prevents code and results from appearing in the finished file. R Markdown still runs the code in the chunk, and the results can be used by other chunks.\necho = FALSE prevents code, but not the results from appearing in the finished file. This is a useful way to embed figures.\nmessage = FALSE prevents messages that are generated by code from appearing in the finished file.\nwarning = FALSE prevents warnings that are generated by code from appearing in the finished.\nfig.cap = \"...\" adds a caption to graphical results.\n\n\n\nSome useful packages\n\nData loading\n\nxlsx: read and write Micorsoft Excel files from R\nhaven: enable R to read and write data from SAS, SPSS, and Stata.\n\nData wrangling\n\ntidyverse: a collection of R packages designed for data science that share an underlying design philosophy, grammar, and data structures, for data import, tidying, and visualization listed here.\ndplyr: essential shortcuts for subsetting, summarizing, rearranging, and joining together data sets.\ntidyr: tools for changing the layout of your data sets. Use the gather and spread functions to convert your data into the tidy format, the layout R likes best.\nstringr: easy to learn tools for regular expressions and character strings.\nlubridate: tools that make working with dates and times easier.\nsna / network: a range of tools for social network analysis.\n\nData visualization\n\nggplot2: R’s famous package for making beautiful graphics. ggplot2 lets you use the grammar of graphics to build layered, customizable plots.\nother extension of ggplot2\nigraph: a package for network analysis and visualization\n\nData modeling/analysis\n\ntidymodels: a collection of packages for modeling and machine learning using tidyverse principles\ncaret: tools for classification and regression training\ncar: a hands-on companion to applied regression, especially the anova and vif function.\nlme4: linear and Non-linear mixed effects models\nquanteda: an R package for managing and analyzing text.\nstatnet: a suite of open source R-based software packages for (statistical) network analysis"
  },
  {
    "objectID": "week2.html",
    "href": "week2.html",
    "title": "Week 2 Lab",
    "section": "",
    "text": "There are mainly four panels once you open the IDE (you can modify the theme and the layout under File - Preference - Appearance/Pane layout).\n\nConsole\nEnvironment/History/…\nFiles/Plots/Viewer/…\nSources"
  },
  {
    "objectID": "week2.html#reproducible-research",
    "href": "week2.html#reproducible-research",
    "title": "Week 2 Lab",
    "section": "Reproducible research",
    "text": "Reproducible research\nReproducible: Can someone else reproduce your entire analysis?\n\nAvailable data\nAvailable codes (including the random seed for machine learning)\n\nWe will be mainly using two types of file formats (other related formats such as Rproj and Rdata:\n\nR script (a text file contains the same commands that your would enter on the command line of R)\nRMarkdown\n\nText, code, and results (from your analysis)\nFormatted output: html, pdf (which requires tex, a typesetting system), etc.\nResource: cheatsheet, The Definitive Guide\n\n\nR markdown is a simple and easy to use plain text language used to combine your R code, results from your data analysis (including plots and tables) and written commentary into a single nicely formatted and reproducible document (like a report, publication, thesis chapter or a web page like this one).1\n\n\nOther examples of markup languages include (compared with Word):\n\nHTML (HyperText Markup Language): website\nLaTex: Overleaf\nMarkdown (a “lightweight” markup language)"
  },
  {
    "objectID": "week2.html#r-packages",
    "href": "week2.html#r-packages",
    "title": "Week 2 Lab",
    "section": "R packages",
    "text": "R packages\nUntil Sep 11th, 2023, there are 19861 available packages on CRAN (The Comprehensive R Archive Network) package repository.\n\n# Install packages\n## install.packages(\"xxx\") - if on CRAN\n## devtools::install_github(\"houyunhuang/ggcor\") - if only on github\n\n\n# Loading packages\nlibrary(dplyr)\n\n\n# An elegant way to install and load packages by using a loop\n if (!require(dplyr)) {\n    install.packages(\"dplyr\")\n    library(dplyr)\n}\n\n# With multiple packages\nsomepackages <- c(\"dplyr\", \"plyr\", \"magrittr\")\n\nfor (pkg in somepackages) {\n  if (!require(pkg, character.only = TRUE)) {\n    install.packages(pkg)\n    library(pkg,character.only = TRUE)\n  }\n}\n\n\n# Unload packages\ndetach(\"package:dplyr\", unload = TRUE)\n\n# Remove packages\n## remove.packages(\"dplyr\")\n\n\nThe chunk options in R code\nGlobal options: knitr::opts_chunk$set(echo = FALSE)\n\nIn this example, the knitr::opts_chunk$set(echo = FALSE) line in the setup chunk tells R Markdown to hide the R code within code chunks for the entire document, except when overridden within individual code chunks using {r} options. This is often used to create clean and readable reports or documents where you want to present the results of your R code without cluttering the document with the code itself.\n\nOther (individual) chunk options:\n\ninclude = FALSE prevents code and results from appearing in the finished file. R Markdown still runs the code in the chunk, and the results can be used by other chunks.\necho = FALSE prevents code, but not the results from appearing in the finished file. This is a useful way to embed figures.\nmessage = FALSE prevents messages that are generated by code from appearing in the finished file.\nwarning = FALSE prevents warnings that are generated by code from appearing in the finished.\nfig.cap = \"...\" adds a caption to graphical results.\n\n\n\nSome useful packages\n\nData loading\n\nxlsx: read and write Micorsoft Excel files from R\nhaven: enable R to read and write data from SAS, SPSS, and Stata.\n\nData wrangling\n\ntidyverse: a collection of R packages designed for data science that share an underlying design philosophy, grammar, and data structures, for data import, tidying, and visualization listed here.\ndplyr: essential shortcuts for subsetting, summarizing, rearranging, and joining together data sets.\ntidyr: tools for changing the layout of your data sets. Use the gather and spread functions to convert your data into the tidy format, the layout R likes best.\nstringr: easy to learn tools for regular expressions and character strings.\nlubridate: tools that make working with dates and times easier.\nsna / network: a range of tools for social network analysis.\n\nData visualization\n\nggplot2: R’s famous package for making beautiful graphics. ggplot2 lets you use the grammar of graphics to build layered, customizable plots.\nother extension of ggplot2\nigraph: a package for network analysis and visualization\n\nData modeling/analysis\n\ntidymodels: a collection of packages for modeling and machine learning using tidyverse principles\ncaret: tools for classification and regression training\ncar: a hands-on companion to applied regression, especially the anova and vif function.\nlme4: linear and Non-linear mixed effects models\nquanteda: an R package for managing and analyzing text.\nstatnet: a suite of open source R-based software packages for (statistical) network analysis"
  },
  {
    "objectID": "week2.html#r-basic-operators",
    "href": "week2.html#r-basic-operators",
    "title": "Week 2 Lab",
    "section": "R basic operators",
    "text": "R basic operators\n\nArithmetic operators\n\n1 + 19 # addition\n19 - 1 # subtraction\n5 * 4 # multiplication\n10 / 2 # division\n11 %/% 2 # integer division\n41 %% 21 # modulus\n20 ^ 2 # exponents\n20 ** 2\n\n\ndata <- data.frame(x1 = 1:3,  \n                      x2 = 2:4,\n                      x3 = 2)\ndata \n\n\ndata^2\n\n\n\nThe <- operator\nAssignment is a binary operator: the left side is a symbol/variable/object, the right is a value/expression being assigned to the left.\n\nx <- 1\nx <- c(1, 2, 3, 4, 5)\nx <- data.frame(x1 = 1:3,  \n                x2 = 2:4,\n                x3 = 2)\n\n\n\nThe [] operator\nIndexing is a binary operator (two operands: the object being indexed (e.g., a vector, list, or data frame) and the index or indices used to select specific elements from that object. )\n\nx <- c(5, 4, 3, 2, 1)\nx[1] # Extracts the first element\n\n\nx <- data.frame(x1 = 1:3,  \n                x2 = 2:4,\n                x3 = 2)\nx[3] \nx[,3]\nx[3,]\nx[3,2]\nx[\"x3\"]\n\n\n\nThe $ operator\nThe $ operator is used to extract or subset a specific part of a data object in R.\n\nExtract the values in a data frame columns\n\n\ndata <- data.frame(x1 = 1:5,  # Create example data\n                   x2 = letters[1:5],\n                   x3 = 9)\ndata  \n\n\ndata$x2\n\n\nReturn specific list elements\n\n\nmy_list <- list(A = 1:5,  # Create example list\n                B = letters[1:5],\n                C = 9)\nmy_list # Print example list\n\n\nmy_list$B # Extract element of list\n\n\n\nThe () operator\nA function call is also a binary operator as the left side is a symbol pointing to the function argument and the right side are the arguments\n\nmax(1,2)\nx <- max(1,2)\n\n\n\nThe ? operator\n\n?: Search R documentation for a specific term.\n??: Search R help files for a word or phrase.\n\n\n\nThe %>% or |> opertor\n%>% is a longstanding feature of the magrittr package for R. It takes the output of one function and passes it into another function as an argument. This allows us to link a sequence of analysis steps (think about a a conveyor belt in a factory)\n\nReadability and clarity\nEast of modification\nAvoid intermediate variables\n\n\nlibrary(tidyverse)\n?group_by\n?mtcars\nmtcars\n\nx <- filter(mtcars, cyl == 6)\ny <- select(x, c(\"mpg\", \"hp\"))\n\nmtcars %>%\n  filter(cyl == 6) %>%\n  select(mpg, hp)\n\n\nresult <- mtcars %>%\n  group_by(cyl) %>%\n  summarise(meanMPG = mean(mpg))\n\n\n\nThe %in% opertor\n%in% is a matching feature to check if the values of the first argument are present in the second argument and returns a logical vector indicating if there is a match or not for its left operand. Here, the first and second arguments can be a value, vector, list, or sequence.\n\n# Check value in a Vector\n67 %in% c(2,5,8,23,67,34)\n45 %in% c(2,5,8,23,67,34)\n\n# Check values from one vector present in another vector\nvec1 <- c(2,5,8,23,67,34)\nvec2 <- c(1,2,8,34) \nvec2 %in% vec1\n\n# Check values in a dataframe\ndf=data.frame(\n  emp_id=c(1,2,3,5),\n  name=c(\"John\",\"Rose\",\"Williams\", \"Ray\"),\n  dept_id=c(10,20,10,50)\n)\n\ndf$dept_state <- if_else(df$dept_id %in% c(10,50),'NY','CA')\ndf\n\n\ndf2 <- df[df$name %in% c('Ray','Rose'), ]\ndf2"
  },
  {
    "objectID": "week2.html#data-type-in-r",
    "href": "week2.html#data-type-in-r",
    "title": "Week 2 Lab",
    "section": "Data type in R",
    "text": "Data type in R\n\n# numeric (double if with more than two decimal numbers)\nx <- 10.5\nclass(x)\n\n# integer\nx <- 1000L\nclass(x)\n\n\n# complex\nx <- 9i + 3\nclass(x)\n\n# character/string\nx <- \"R is exciting\"\nclass(x)\n\n# logical/boolean\nx <- TRUE\nclass(x)\n\n# date\nx = \"01-06-2021\"\nx = as.Date(x, \"%d-%m-%Y\")\nclass(x)\n\n# Factors\n## Factors are the data objects which are used to categorize the data and store it as levels. \n## They can store both strings and integers. They are useful in the columns which have a limited number of unique values. Like Male/Female and True/False, etc. They are useful in data analysis for statistical modeling.\nx <- c(\"East\",\"South\",\"East\",\"North\",\"North\",\"East\",\"West\",\"West\",\"West\",\"South\",\"North\")\nx_factor <- factor(x) ### as.factor\nx_factor2 <- factor(x, levels = c(\"East\", \"West\", \"South\", \"North\"))\nsummary(x_factor)\nsummary(x_factor2)\n\n# Missing values\nx <- c(1, 2, NA, 4)\nis.na(x)\nwhich(is.na(x))\nx_omit <- na.omit(x)\n\n\nNotes on NA\n\nA missing value in a factor variable is displayed as <NA> rather than just NA.\nR has a special value NaN for “not a number.” 0/0 is an example of a calculation that will produce a NaN. NaNs print as NaN, but generally act like NAs.\nAnother special case is Inf, such as log(0)\n\n\n\nExercise\n\nCreate a new R script\nInstall quanteda package (for textual analysis later in the semester) and load it\nCreate a variable called first_num and assign it the value of 605\nCreate a variable called first_char and assign it the value of my first character\nCreate a vector called gender, including: “male”, “female”, “other”, “female”, “male”, “female”, “female”, “other”, “male”. Make gender as a factor vector, following the order of “female”, “other”, and “male”."
  },
  {
    "objectID": "week2.html#exercise",
    "href": "week2.html#exercise",
    "title": "Week 2 Lab",
    "section": "Exercise",
    "text": "Exercise\n\nCreate a new R script\nInstall quanteda package (for textual analysis later in the semester) and load it\nCreate a variable called first_num and assign it the value of 605\nCreate a variable called first_char and assign it the value of my first character\nCreate a vector called gender, including: “male”, “female”, “other”, “female”, “male”, “female”, “female”, “other”, “male”. Make gender as a factor vector, following the order of “female”, “other”, and “male”."
  },
  {
    "objectID": "week3.html#understanding-data-frame",
    "href": "week3.html#understanding-data-frame",
    "title": "Week 3 Lab",
    "section": "Understanding data frame",
    "text": "Understanding data frame\n\nCreate a data frame\n\nheight <- c(180, 155, 160, 167, 181)\nweight <- c(65, 50, 52, 58, 70)\nnames <- c(\"Joanna\", \"Charlotte\", \"Helen\", \"Karen\", \"Amy\")\n\ndata <- data.frame(height = height, weight = weight, names = names) #stringsAsFactors = TRUE\ndata\ndim(data)\nstr(data)\n\n\n\nPositional index\n\ndata[1,3] # the first value (1st row ) of the names variable (3th column)\ndata$names[1]\ndata[1:2, 2:3] #  the first 2 rows and the last 2 columns\ndata[1:2, ]\ndata[, 2:3]\n\n\n\nOrdering data frames\n\nheight_order <- data[order(data$height), ]\nrownames(height_order) <- 1:nrow(height_order)\nheight_order\n\nheight_order <- data[order(data$height, decreasing = T),]\n\n\n\nAdding/Removing columns and rows\n\ndata2 <- data.frame(state = c(\"NY\", \"PA\", \"MD\", \"VA\", \"MA\"))\ndata_newcolumn <- cbind(data, data2)\n\ndata_removecolumn <- data_newcolumn[, c(1:2, 4)]\ndata_newcolumn$state <- NULL\n\ndata3 <- data.frame(height = c(120, 150, 132, 122),\n                    weight = c(44, 56, 49, 45),\n                    names = c(\"Ryan\", \"Chris\", \"Ben\", \"John\"))\ndata_newrow <- rbind(data, data3)\ndata_removerow <- data_newrow[c(1,6:9),]\n\n\n\nMerging data frames\nHere are two fictitious datasets of a clinical trial. One table contains demographic information of the patients and the other one adverse events recorded throughout the course of the trial.\n\ndemographics <- data.frame(\n  id = c(\"P1\", \"P2\", \"P3\"),\n  age = c(40, 54, 47),\n  state = c(\"NY\", \"MA\", \"PA\"),\n  stringsAsFactors = FALSE\n)\n\nadverse_events <- data.frame(\n  id = c(\"P1\", \"P1\", \"P3\", \"P4\"),\n  term = c(\"Headache\", \"Neutropenia\", \"Constipation\", \"Tachycardia\"),\n  onset_date = c(\"2020-12-03\", \"2021-01-03\", \"2020-11-29\", \"2021-01-27\"),\n  stringsAsFactors = FALSE\n)\n\n\nmerge(demographics, adverse_events, by = \"id\")\nmerge(demographics, adverse_events, by = \"id\", all.x = T)\nmerge(demographics, adverse_events, by = \"id\", all.y = T)\nmerge(demographics, adverse_events, by = \"id\", all = T)\n\n\nadverse_events2 <- adverse_events\ncolnames(adverse_events2)[1] <- \"pat_id\"\nmerge(demographics, adverse_events2, by.x = \"id\", by.y = \"pat_id\", all = T)\n\n\n\nExercise\nPlease merge the following two datasets emp_df (employee information)and dept_df (department information) using two ID columns dept_id and dept_branch_id.\n\nemp_df=data.frame(\n  emp_id=c(1,2,3,4,5,6),\n  name=c(\"Chris\",\"Rose\",\"Williams\",\"Jones\",\"Jayden\",\"Xavior\"),\n  superior_emp_id=c(1,1,1,2,2,2),\n  dept_id=c(10,20,10,10,40,30),\n  dept_branch_id= c(101,102,101,101,104,103)\n)\n\ndept_df=data.frame(\n  dept_id=c(10,20,30,40),\n  dept_name=c(\"Finance\",\"Marketing\",\"Sales\",\"IT\"),\n  dept_branch_id= c(101,102,103,104)\n)\n\n\n\nShow the code\nmerge(emp_df, dept_df, by = c(\"dept_id\", \"dept_branch_id\"), all.x = T)"
  },
  {
    "objectID": "week3.html#data-importing-and-exporting",
    "href": "week3.html#data-importing-and-exporting",
    "title": "Week 3 Lab",
    "section": "Data importing (and exporting)",
    "text": "Data importing (and exporting)\nCommon data formats:\n\nCSV (comma-seperated values) / TSV (tab-seperated values)\nxlsx\ntxt\nother softwares/packages: .sav(SPSS), .dta(STATA)\n\nCommon data types in R:\n\ndata frame\ntibble (tbl_df): it does much less than a data frame (a neater data frame), as it never changes the type of the inputs (e.g. it keeps list columns unchanged, and never converts strings to factors), it never changes the names of variables, it only recycles inputs of length 1, and it never creates row.names().\n\n\ndata <- data.frame(a = 1:26, b = letters[1:26], c = Sys.Date() - 1:26)\ndata\nas_tibble(data)\n\n\nstudents <- read.csv(\"https://pos.it/r4ds-students-csv\") # from URL\nstudents <- read.csv(\"data/students.csv\") # from local\nstudents\nstr(students)\nsummary(students)\n\n\nstudents <- read.csv(\"data/students.csv\", na.strings=c(\"N/A\", \"\"))\n# students <- read_csv(\"data/students.csv\", na = c(\"N/A\", \"\"))\n\n\n❓ Question:: What is are the differences between read.csv and read_csv?\n\n\nBasic data cleaning\n\nstr(students)\nstudents %>%\n  rename(student_id = Student.ID,\n         full_name = Full.Name,\n         fav_food = favourite.food)\n\nrename(students,\n       student_id = Student.ID,\n       full_name = Full.Name,\n       fav_food = favourite.food)\n\nstudents_rename <- students %>%\n                   rename(student_id = Student.ID,\n                          full_name = Full.Name,\n                          fav_food = favourite.food)\n\n\n# a faster way\nstudents_rename <- clean_names(students)\nstudents_rename <- mutate(students_rename, meal_plan = factor(meal_plan))\nstr(students_rename)\n\n\nstudents_clean <- students_rename %>%\n                  mutate(age = if_else(age == \"five\", \"5\", age))\n\n#students_rename2 <- students_rename\n#students_rename2$age <- ifelse(students_rename2$age == \"five\", 5, students_rename2$age)\n#students_rename2$age[students_rename2$age == \"five\"] <- 5\n\nNote: if_else() has three arguments. The first argument test should be a logical vector. The result will contain the value of the second argument, yes, when test is TRUE, and the value of the third argument, no, when it is FALSE. Here we’re saying if age is the character string “five”, make it “5”, and if not leave it as age.\n\nstudents_clean_final <- students_clean %>%\n                        mutate(group = case_when(\n                          age <= 5 ~ \"young\",\n                          age > 5 ~ \"old\",\n                          .default = \"other\"\n                        ))\n\n\n❓ Question: how to use pipe %>% to save some time here?\n\n\nShow the code\nstudents_clean_final <- students %>%\n                        clean_names() %>%\n                        mutate(meal_plan = factor(meal_plan),\n                               age = parse_number(if_else(age == \"five\", \"5\", age)),\n                               group = case_when(\n                                       age <= 5 ~ \"young\",\n                                       age > 5 ~ \"old\",\n                                       .default = \"other\"))\n\n\n\nwrite.csv(students_clean_final, \"data/students_final.csv\", row.names = F)"
  },
  {
    "objectID": "week3.html#data-tidying",
    "href": "week3.html#data-tidying",
    "title": "Week 3 Lab",
    "section": "Data tidying",
    "text": "Data tidying\nIn real life, the social media data you collected is “messy” and “dirty”.\n\nData Scientists spend up to 80% of the time on data cleaning and 20 percent of their time on actual data analysis. 1\n\nThe process of “tidying” data would thus create what’s known as tidy data, as populated by Hadley Wickham (one of the authors of R for Data Science).\n\nTidy datasets provide a standardized way to link the structure of a dataset (its physical layout) with its semantics (its meaning). 2\n\nEach dataset shows the same values of four variables: country, year, population, and number of documented cases of TB (tuberculosis), but each dataset organizes the values in a different way.\n\n❓ Question: are they the same datasets? Which one is easier to work with and why?\n\ntable1\n\n# A tibble: 6 × 4\n  country      year  cases population\n  <chr>       <dbl>  <dbl>      <dbl>\n1 Afghanistan  1999    745   19987071\n2 Afghanistan  2000   2666   20595360\n3 Brazil       1999  37737  172006362\n4 Brazil       2000  80488  174504898\n5 China        1999 212258 1272915272\n6 China        2000 213766 1280428583\n\ntable2\n\n# A tibble: 12 × 4\n   country      year type            count\n   <chr>       <dbl> <chr>           <dbl>\n 1 Afghanistan  1999 cases             745\n 2 Afghanistan  1999 population   19987071\n 3 Afghanistan  2000 cases            2666\n 4 Afghanistan  2000 population   20595360\n 5 Brazil       1999 cases           37737\n 6 Brazil       1999 population  172006362\n 7 Brazil       2000 cases           80488\n 8 Brazil       2000 population  174504898\n 9 China        1999 cases          212258\n10 China        1999 population 1272915272\n11 China        2000 cases          213766\n12 China        2000 population 1280428583\n\ntable3\n\n# A tibble: 6 × 3\n  country      year rate             \n  <chr>       <dbl> <chr>            \n1 Afghanistan  1999 745/19987071     \n2 Afghanistan  2000 2666/20595360    \n3 Brazil       1999 37737/172006362  \n4 Brazil       2000 80488/174504898  \n5 China        1999 212258/1272915272\n6 China        2000 213766/1280428583\n\n\nA tidy data set is:\n\nEach variable is a column; each column is a variable.\nEach observation is a row; each row is an observation.\nEach value is a cell; each cell is a single value.\n\n\n\n\nSource: r4ds\n\n\nLet’s work with the table1 here.\n\n# Compute rate per 10,000\ntb_table <- table1 %>%\n  mutate(rate = cases / population * 10000)\n\n\ntb_year <- table1 %>% \n  group_by(year) %>% \n  summarize(total_cases = sum(cases))\n\n\n\nExercise\nUsing table1 to calculate the TB cases per year in each country. (Hint: use mean())\n\n\nShow the code\ntable1 %>% \n  group_by(country) %>% \n  summarize(mean_cases = mean(cases))"
  },
  {
    "objectID": "week3.html#data-transformation",
    "href": "week3.html#data-transformation",
    "title": "Week 3 Lab",
    "section": "Data transformation",
    "text": "Data transformation\n\nColumns and rows\nflights is a dataset on flights that departed from New York City in 2013.\n\n#install.packages(\"nycflights13\")\nlibrary(nycflights13)\nflights\nstr(flights)\n\n\n# Subseting certain columns\nflights_sub <- flights %>%\n  select(c(month, day, flight, carrier, origin, dest, distance, air_time))\n\n# Creating new columns that are derived from the existing columns\nflights_sub <- flights_sub %>% \n  mutate(speed = distance / air_time * 60)\n\n\n# Filtering certain rows\nflights_IAH <- flights %>%\n  filter(dest == \"IAH\")\n\nflights_summer <- flights %>%\n   filter(month == 6 | month == 7 | month == 8) #OR\n\nflights_summer <- flights %>%\n   filter(month %in% c(6,7,8))\n\nflights_jan1 <- flights %>% \n  filter(month == 1 & day == 1) #AND\n\n\nflights %>% \n  arrange(year, month, day, dep_time)\n\nflights %>% \n  arrange(desc(dep_delay))\n\n\nflights %>% \n  distinct(origin, dest, .keep_all = TRUE)\n\n\nflights %>% \n  group_by(month) %>% \n  summarize(\n    mean_delay = mean(dep_delay, na.rm = TRUE), \n    n = n()\n  )\n\ndaily_flights <- flights %>%\n  group_by(year, month, day) %>%\n  summarize(\n    mean_delay = mean(dep_delay, na.rm = TRUE), \n    n = n())\n\n\n\nExercise\n\nUsing the same flights dataset to creat a new dataframe flights_houston, and:\n\n\nOnly include flights heading to Houston (IAH and HOU)\nCalcuate the speed (\\(speed = distance / air\\_time * 60\\))\nOnly keep the columns of “year”, “month”, “day”,“dep_time”, “carrier”, “flight”, and “speed”\nArrange the data based on the speed with a desceding order.\n\n\n\nShow the code\nflights_houston <- flights %>% \n  filter(dest == \"IAH\" | dest == \"HOU\") %>% \n  mutate(speed = distance / air_time * 60) %>% \n  select(year:day, dep_time, carrier, flight, speed) %>% \n  arrange(desc(speed))\n\n\n\nUsing the same flights dataset to find out which carrier heading to which airport has the worst average delays?\n\n\n\nShow the code\ndelay_flights <- flights %>%\n  group_by(carrier, dest) %>%\n  summarize(\n    mean_delay = mean(dep_delay, na.rm = TRUE), \n    n = n())\n\n\n\n\nLengthening and widening data\nIn reality, you need long-format data much more commonly than wide-format data (such as visualizing in ggplot2 and modeling).\n\nWide format data: it has a column for each variable and a long format data. The billboard dataset records the billboard rank of songs in the year 2000:\n\n\nbillboard\n\n\nLong format data: it has a column for possible variable types and a column for the values of those variables. cms_patient_experience, is a dataset from the Centers of Medicare and Medicaid services that collects data about patient experiences:\n\n\ncms_patient_experience\n\ntidyr provides two functions for pivoting data:\n\npivot_longer(): it takes wide-format data and turn it into long-format data (melt in reshape2). \npivot_wider(): it takes long-format data and turn it into wide-format data (cast in reshape2).\n\n\nbillboard %>% \n  pivot_longer(\n    cols = starts_with(\"wk\"), \n    names_to = \"week\", \n    values_to = \"rank\"\n  )\n\n\nbilloard_longer <- billboard %>% \n                    pivot_longer(\n                      cols = starts_with(\"wk\"), \n                      names_to = \"week\", \n                      values_to = \"rank\",\n                      values_drop_na = TRUE) %>% \n                    mutate(week = parse_number(week))\n\nBut in reality, we might need to deal with multiple variables at the same time… Now, let’s take a look at the who2 dataset, the source of table1 that you saw above.\n\nwho2\n\nThis dataset is collected by the World Health Organisation, records information about tuberculosis diagnoses. There are two columns that are already variables and are easy to interpret: country and year. They are followed by 56 columns like “sp_m_014”, “ep_m_4554”, and “rel_m_3544”. Each column name is made up of three pieces separated by “_“. The first piece,”sp/rel/ep”, describes the method used for the diagnosis, the second piece, “m/f” is the gender (coded as a binary variable in this dataset), and the third piece, “014/1524/2534/3544/4554/5564/65” is the age range (“014” represents “0-14”, for example).\n\nwho2_long <- who2 %>% \n              pivot_longer(\n                cols = !(country:year),\n                names_to = c(\"diagnosis\", \"gender\", \"age\"), \n                names_sep = \"_\",\n                values_to = \"count\"\n              )\n\n\n\nExercise\nThe following (wide) dataset shows the number of points scored by various NBA basketball players in different years. Please tranform it to a long format.\n\nnba <- data.frame(player=c('Joel Embiid', 'Luka Doncic', 'Jayson Tatum', 'Trae Young'),\n                 year1=c(28.5, 27.7, 26.4,25.3),\n                 year2=c(30.6, 28.4, 26.9, 28.4),\n                 year3=c(33.1, 32.4, 30.1, 26.2))\n\n\n\nShow the code\nnba_long <- nba %>%\n  pivot_longer(cols=c('year1', 'year2', 'year3'),\n               names_to='year',\n               values_to='points')\nnba_long"
  },
  {
    "objectID": "week5.html",
    "href": "week5.html",
    "title": "Week 5 Lab",
    "section": "",
    "text": "You will need to have a Google account/gmail (e.g., RIT email).\nLog in Google’s developers console to create a new project: https://console.developers.google.com/\n\n\n\nClick “Enabled APIs & services”, search for “YouTube Data API V3” and enable it.\n\n  \n\nCreate your API key, choose “Public data”, then restrict the API with only the YouTube Data API V3.   \nCreate the credentials for your API keys. Configure your OAuth consent screen, choose internal user type (if you choose external user, extra steps for authorization might be needed), put in your email contact,then you should be able to see the client ID and secret. Copy and save it, and don’t share with others.\n\n   \nLastly you might need to add a redirect url into your client ID. Under Cresidentials, click on the Client ID you created, then put “http://localhost:1410/” into “Authorized redirect URIs”, and save it.\nAdditionally, if you are making it for external usage, you might need to do an extra step for verification. Under the tab of “OAuth consent screen”, click “Publish APP”.\n\n\n\n\n\n\n# install.packages(\"tuber\")\nlibrary(tuber)\nlibrary(tidyverse)\n\n\n\n\n\nclient_id &lt;- \"YOUR CLIENT ID\"\nclient_secret &lt;- \"YOUR CLIENT SECRET\"\nyt_oauth(app_id = client_id,\n         app_secret = client_secret,\n         token = \" \")\nyt_authorized()\n\nIf you sign in with your gmail account, click allow the authorization, then you should be able to see the following page, which mean you have successfully complete the authorization process and you can now close the page. \n\n\n\n\n\n\nWarning\n\n\n\nYou might run into a error message - “HTTP failure: 400” and “Error: redirect_uri_mismatch”. A quick workaround would be running install.packages(\"httpuv\") in your console, which should fix the issue.\n\n\n\n\n\nBefore collecting the comments, you will need the ID of the video. The ID can be identifid from the url. For instance, we are using a video titled “Exposing the Russian Military Unit Behind a Massacre in Bucha | Visual Investigations” from the New York Times channel as an example. You can find the ID for this specific video is IrGZ66uKcl0 (after v=) in the URL. \n\ncomment &lt;- get_all_comments(video_id = \"IrGZ66uKcl0\")\n\n\n\n\n\n\n\nWarning\n\n\n\nYou might run into a error message - “HTTP failure: 401” once in a while, which is highly likely due to the short expiration time for the access token. It seems that there are no systematic ways to fix it for now. But you can remove the .httr-oauth file in your working directory and re-authorize it manually again (so I would suggest you to set up the working directory before you start any project)\n\n\nYou can take a quick glimpse at the comments you collected. You probabbly notice that the numbers of observations are different than what you saw on the webpage. Some of them could be filtered by anti-spam bots or some other reasons related to the API. Most of the variables are pretty much self-explanatory based on their names. To be noted, parentId specifies the ID of the comment for which replies should be retrieved. In other words, if it shows NA, it is not a reply. Currently, YouTube only supports replies only for top-level comments. For more details, please refer to YouTube data API document.\n\n\n\n\n\n\nNote\n\n\n\nTypically, every API will have rate limits, which can restrict the number of times a user or client can access the server within a specified period of time. By default, YouTube data API v3 token grant rate limit is 10,000 grants per day (not necessary equal to the number of comments). You can either apply to raise the daily token limits; or switch to another API when it reaches the rate limits. \n\n\n\nstr(comment)\n\nAdditionally, you can also get the statistics of the whole videos including the count of view, like, favorite, and comment. To be noted, it will be stored in a list rather than a dataframe. But we can transform it into a dataframe which might be easier to merge or analyze later.\n\nstat &lt;- get_stats(video_id = \"IrGZ66uKcl0\")\nstat_df &lt;- as.data.frame(stat)\n\n\n\n\nFirst you need to identify the channel ID. You can go to the about page, click the share icon and copy channel ID. For instance, the channel ID for NYT’s channel is UCvsAa96EzubF7zNHJEzvG2g.\n\nnytstat &lt;- get_channel_stats(\"UCqnbDFdCpuN8CMEg0VuEBqA\")\nnytvideos = yt_search(term=\"\", type=\"video\", channel_id = \"UCqnbDFdCpuN8CMEg0VuEBqA\")\n\nIn the previous chunk, we would like to collect each video ID in the channel. Then we will extract the IDs and iterate the same function get_comment_threads for each ID.\n\nnytcomments &lt;- lapply(as.character(nytvideos$video_id), function(x){get_comment_threads(c(video_id = x), max_results = 20)})\n\n\nnytcomments_df &lt;- nytcomments %&gt;% map_df(as_tibble)\nnytcomments_df$videoId &lt;- as.factor(nytcomments_df$videoId)\nstr(nytcomments_df$videoId )\n\nSimilarly, you can also get the stats from every video.\n\nnyt_videostat &lt;- lapply(as.character(nytvideos$video_id), function(x){get_stats(c(video_id = x))})\nnyt_videostat_df &lt;- nyt_videostat %&gt;% map_df(as_tibble)\nnyt_videostat_df &lt;- nyt_videostat_df %&gt;% distinct(id, keep_all = T)\n\nAnd you can merge the video stat with the comments (for later analysis if needed)\n\nnyt_all &lt;- merge(nytcomments_df, nyt_videostat_df, by.x = \"videoId\", by.y = \"id\", all.x = T)\n\n\n\n\nSometime, you might not need all the videos in a channel. Then you can collect the videos for a specific list. For instance, we are using Russia-Ukraine War playlist under the New York Times YouTube channel as an example. And you can also identify the playlist ID in the URL, and the playlist ID here is PL4CGYNsoW2iC67ssBNT-h7nTLbckKZByR.\n\n\nukraine_list &lt;- get_playlist_items(\n                c(playlist_id = \"PL4CGYNsoW2iC67ssBNT-h7nTLbckKZByR\"),\n                max_results = 200)\n\n\nukraine_comments &lt;- lapply(as.character(ukraine_list$contentDetails.videoId), function(x){ get_comment_threads(c(video_id = x), max_results = 100)})\nukraine_comments_df &lt;- ukraine_comments %&gt;%\n                       map_df(as_tibble)\n\n\nukraine_videostat &lt;- lapply(as.character(ukraine_stat$contentDetails.videoId), function(x){get_stats(c(video_id = x))})\nukraine_videostat_df &lt;- ukraine_videostat %&gt;% map_df(as_tibble)"
  },
  {
    "objectID": "week5.html#get-your-youtube-client-id-and-sercret",
    "href": "week5.html#get-your-youtube-client-id-and-sercret",
    "title": "Week 5 Lab",
    "section": "",
    "text": "You will need to have a Google account/gmail (e.g., RIT email).\nLog in Google’s developers console to create a new project: https://console.developers.google.com/\n\n\n\nClick “Enabled APIs & services”, search for “YouTube Data API V3” and enable it.\n\n  \n\nCreate your API key, choose “Public data”, then restrict the API with only the YouTube Data API V3.   \nCreate the credentials for your API keys. Configure your OAuth consent screen, choose internal user type (if you choose external user, extra steps for authorization might be needed), put in your email contact,then you should be able to see the client ID and secret. Copy and save it, and don’t share with others.\n\n   \nLastly you might need to add a redirect url into your client ID. Under Cresidentials, click on the Client ID you created, then put “http://localhost:1410/” into “Authorized redirect URIs”, and save it.\nAdditionally, if you are making it for external usage, you might need to do an extra step for verification. Under the tab of “OAuth consent screen”, click “Publish APP”."
  },
  {
    "objectID": "week5.html#download-youtube-data-through-tuber-package.",
    "href": "week5.html#download-youtube-data-through-tuber-package.",
    "title": "Week 5 Lab",
    "section": "",
    "text": "# install.packages(\"tuber\")\nlibrary(tuber)\nlibrary(tidyverse)\n\n\n\n\n\nclient_id &lt;- \"YOUR CLIENT ID\"\nclient_secret &lt;- \"YOUR CLIENT SECRET\"\nyt_oauth(app_id = client_id,\n         app_secret = client_secret,\n         token = \" \")\nyt_authorized()\n\nIf you sign in with your gmail account, click allow the authorization, then you should be able to see the following page, which mean you have successfully complete the authorization process and you can now close the page. \n\n\n\n\n\n\nWarning\n\n\n\nYou might run into a error message - “HTTP failure: 400” and “Error: redirect_uri_mismatch”. A quick workaround would be running install.packages(\"httpuv\") in your console, which should fix the issue.\n\n\n\n\n\nBefore collecting the comments, you will need the ID of the video. The ID can be identifid from the url. For instance, we are using a video titled “Exposing the Russian Military Unit Behind a Massacre in Bucha | Visual Investigations” from the New York Times channel as an example. You can find the ID for this specific video is IrGZ66uKcl0 (after v=) in the URL. \n\ncomment &lt;- get_all_comments(video_id = \"IrGZ66uKcl0\")\n\n\n\n\n\n\n\nWarning\n\n\n\nYou might run into a error message - “HTTP failure: 401” once in a while, which is highly likely due to the short expiration time for the access token. It seems that there are no systematic ways to fix it for now. But you can remove the .httr-oauth file in your working directory and re-authorize it manually again (so I would suggest you to set up the working directory before you start any project)\n\n\nYou can take a quick glimpse at the comments you collected. You probabbly notice that the numbers of observations are different than what you saw on the webpage. Some of them could be filtered by anti-spam bots or some other reasons related to the API. Most of the variables are pretty much self-explanatory based on their names. To be noted, parentId specifies the ID of the comment for which replies should be retrieved. In other words, if it shows NA, it is not a reply. Currently, YouTube only supports replies only for top-level comments. For more details, please refer to YouTube data API document.\n\n\n\n\n\n\nNote\n\n\n\nTypically, every API will have rate limits, which can restrict the number of times a user or client can access the server within a specified period of time. By default, YouTube data API v3 token grant rate limit is 10,000 grants per day (not necessary equal to the number of comments). You can either apply to raise the daily token limits; or switch to another API when it reaches the rate limits. \n\n\n\nstr(comment)\n\nAdditionally, you can also get the statistics of the whole videos including the count of view, like, favorite, and comment. To be noted, it will be stored in a list rather than a dataframe. But we can transform it into a dataframe which might be easier to merge or analyze later.\n\nstat &lt;- get_stats(video_id = \"IrGZ66uKcl0\")\nstat_df &lt;- as.data.frame(stat)\n\n\n\n\nFirst you need to identify the channel ID. You can go to the about page, click the share icon and copy channel ID. For instance, the channel ID for NYT’s channel is UCvsAa96EzubF7zNHJEzvG2g.\n\nnytstat &lt;- get_channel_stats(\"UCqnbDFdCpuN8CMEg0VuEBqA\")\nnytvideos = yt_search(term=\"\", type=\"video\", channel_id = \"UCqnbDFdCpuN8CMEg0VuEBqA\")\n\nIn the previous chunk, we would like to collect each video ID in the channel. Then we will extract the IDs and iterate the same function get_comment_threads for each ID.\n\nnytcomments &lt;- lapply(as.character(nytvideos$video_id), function(x){get_comment_threads(c(video_id = x), max_results = 20)})\n\n\nnytcomments_df &lt;- nytcomments %&gt;% map_df(as_tibble)\nnytcomments_df$videoId &lt;- as.factor(nytcomments_df$videoId)\nstr(nytcomments_df$videoId )\n\nSimilarly, you can also get the stats from every video.\n\nnyt_videostat &lt;- lapply(as.character(nytvideos$video_id), function(x){get_stats(c(video_id = x))})\nnyt_videostat_df &lt;- nyt_videostat %&gt;% map_df(as_tibble)\nnyt_videostat_df &lt;- nyt_videostat_df %&gt;% distinct(id, keep_all = T)\n\nAnd you can merge the video stat with the comments (for later analysis if needed)\n\nnyt_all &lt;- merge(nytcomments_df, nyt_videostat_df, by.x = \"videoId\", by.y = \"id\", all.x = T)\n\n\n\n\nSometime, you might not need all the videos in a channel. Then you can collect the videos for a specific list. For instance, we are using Russia-Ukraine War playlist under the New York Times YouTube channel as an example. And you can also identify the playlist ID in the URL, and the playlist ID here is PL4CGYNsoW2iC67ssBNT-h7nTLbckKZByR.\n\n\nukraine_list &lt;- get_playlist_items(\n                c(playlist_id = \"PL4CGYNsoW2iC67ssBNT-h7nTLbckKZByR\"),\n                max_results = 200)\n\n\nukraine_comments &lt;- lapply(as.character(ukraine_list$contentDetails.videoId), function(x){ get_comment_threads(c(video_id = x), max_results = 100)})\nukraine_comments_df &lt;- ukraine_comments %&gt;%\n                       map_df(as_tibble)\n\n\nukraine_videostat &lt;- lapply(as.character(ukraine_stat$contentDetails.videoId), function(x){get_stats(c(video_id = x))})\nukraine_videostat_df &lt;- ukraine_videostat %&gt;% map_df(as_tibble)"
  },
  {
    "objectID": "week3.html#rstudio-orientation",
    "href": "week3.html#rstudio-orientation",
    "title": "Week 3 Lab",
    "section": "",
    "text": "There are mainly four panels once you open the IDE (you can modify the theme and the layout under File - Preference - Appearance/Pane layout).\n\nConsole\nEnvironment/History/…\nFiles/Plots/Viewer/…\nSources"
  },
  {
    "objectID": "week3.html#r-basic-operators",
    "href": "week3.html#r-basic-operators",
    "title": "Week 3 Lab",
    "section": "R basic operators",
    "text": "R basic operators\n\nArithmetic operators\n\n1 + 19 # addition\n19 - 1 # subtraction\n5 * 4 # multiplication\n10 / 2 # division\n11 %/% 2 # integer division\n41 %% 21 # modulus\n20 ^ 2 # exponents\n20 ** 2\n\n\ndata &lt;- data.frame(x1 = 1:3,  \n                      x2 = 2:4,\n                      x3 = 2)\ndata \n\n\ndata^2\n\n\n\nThe &lt;- operator\nAssignment is a binary operator: the left side is a symbol/variable/object, the right is a value/expression being assigned to the left.\n\nx &lt;- 1\nx &lt;- c(1, 2, 3, 4, 5)\nx &lt;- data.frame(x1 = 1:3,  \n                x2 = 2:4,\n                x3 = 2)\n\n\n\nThe [] operator\nIndexing is a binary operator (two operands: the object being indexed (e.g., a vector, list, or data frame) and the index or indices used to select specific elements from that object. )\n\nx &lt;- c(5, 4, 3, 2, 1)\nx[1] # Extracts the first element\n\n\nx &lt;- data.frame(x1 = 1:3,  \n                x2 = 2:4,\n                x3 = 2)\nx[3] \nx[,3]\nx[3,]\nx[3,2]\nx[\"x3\"]\n\n\n\nThe $ operator\nThe $ operator is used to extract or subset a specific part of a data object in R.\n\nExtract the values in a data frame columns\n\n\ndata &lt;- data.frame(x1 = 1:5,  # Create example data\n                   x2 = letters[1:5],\n                   x3 = 9)\ndata  \n\n\ndata$x2\n\n\nReturn specific list elements\n\n\nmy_list &lt;- list(A = 1:5,  # Create example list\n                B = letters[1:5],\n                C = 9)\nmy_list # Print example list\n\n\nmy_list$B # Extract element of list\n\n\n\nThe () operator\nA function call is also a binary operator as the left side is a symbol pointing to the function argument and the right side are the arguments\n\nmax(1,2)\nx &lt;- max(1,2)\n\n\n\nThe ? operator\n\n?: Search R documentation for a specific term.\n??: Search R help files for a word or phrase.\n\n\n\nThe %&gt;% or |&gt; opertor\n%&gt;% is a longstanding feature of the magrittr package for R. It takes the output of one function and passes it into another function as an argument. This allows us to link a sequence of analysis steps (think about a a conveyor belt in a factory)\n\nReadability and clarity\nEast of modification\nAvoid intermediate variables\n\n\nlibrary(tidyverse)\n?group_by\n?mtcars\nmtcars\n\nx &lt;- filter(mtcars, cyl == 6)\ny &lt;- select(x, c(\"mpg\", \"hp\"))\n\nmtcars %&gt;%\n  filter(cyl == 6) %&gt;%\n  select(mpg, hp)\n\n\nresult &lt;- mtcars %&gt;%\n  group_by(cyl) %&gt;%\n  summarise(meanMPG = mean(mpg))\n\n\n\nThe %in% opertor\n%in% is a matching feature to check if the values of the first argument are present in the second argument and returns a logical vector indicating if there is a match or not for its left operand. Here, the first and second arguments can be a value, vector, list, or sequence.\n\n# Check value in a Vector\n67 %in% c(2,5,8,23,67,34)\n45 %in% c(2,5,8,23,67,34)\n\n# Check values from one vector present in another vector\nvec1 &lt;- c(2,5,8,23,67,34)\nvec2 &lt;- c(1,2,8,34) \nvec2 %in% vec1\n\n# Check values in a dataframe\ndf=data.frame(\n  emp_id=c(1,2,3,5),\n  name=c(\"John\",\"Rose\",\"Williams\", \"Ray\"),\n  dept_id=c(10,20,10,50)\n)\n\ndf$dept_state &lt;- if_else(df$dept_id %in% c(10,50),'NY','CA')\ndf\n\n\ndf2 &lt;- df[df$name %in% c('Ray','Rose'), ]\ndf2"
  },
  {
    "objectID": "week3.html#data-type-in-r",
    "href": "week3.html#data-type-in-r",
    "title": "Week 3 Lab",
    "section": "Data type in R",
    "text": "Data type in R\n\n# numeric (double if with more than two decimal numbers)\nx &lt;- 10.5\nclass(x)\n\n# integer\nx &lt;- 1000L\nclass(x)\n\n\n# complex\nx &lt;- 9i + 3\nclass(x)\n\n# character/string\nx &lt;- \"R is exciting\"\nclass(x)\n\n# logical/boolean\nx &lt;- TRUE\nclass(x)\n\n# date\nx = \"01-06-2021\"\nx = as.Date(x, \"%d-%m-%Y\")\nclass(x)\n\n# Factors\n## Factors are the data objects which are used to categorize the data and store it as levels. \n## They can store both strings and integers. They are useful in the columns which have a limited number of unique values. Like Male/Female and True/False, etc. They are useful in data analysis for statistical modeling.\nx &lt;- c(\"East\",\"South\",\"East\",\"North\",\"North\",\"East\",\"West\",\"West\",\"West\",\"South\",\"North\")\nx_factor &lt;- factor(x) ### as.factor\nx_factor2 &lt;- factor(x, levels = c(\"East\", \"West\", \"South\", \"North\"))\nsummary(x_factor)\nsummary(x_factor2)\n\n# Missing values\nx &lt;- c(1, 2, NA, 4)\nis.na(x)\nwhich(is.na(x))\nx_omit &lt;- na.omit(x)\n\n\nNotes on NA\n\nA missing value in a factor variable is displayed as &lt;NA&gt; rather than just NA.\nR has a special value NaN for “not a number.” 0/0 is an example of a calculation that will produce a NaN. NaNs print as NaN, but generally act like NAs.\nAnother special case is Inf, such as log(0)\n\n\n\nExercise\n\nCreate a new R script\nInstall quanteda package (for textual analysis later in the semester) and load it\nCreate a variable called first_num and assign it the value of 605\nCreate a variable called first_char and assign it the value of my first character\nCreate a vector called gender, including: “male”, “female”, “other”, “female”, “male”, “female”, “female”, “other”, “male”. Make gender as a factor vector, following the order of “female”, “other”, and “male”."
  },
  {
    "objectID": "week3.html#footnotes",
    "href": "week3.html#footnotes",
    "title": "Week 3 Lab",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nDouglas, A. et al. (2023) An Introduction to R↩︎"
  },
  {
    "objectID": "week4.html",
    "href": "week4.html",
    "title": "Week 4 Lab",
    "section": "",
    "text": "Source: r4ds\nData wrangling/manipulation is the most important part when we deal with any types of data - before visualization and modeling. We will be mainly following R for Data Science, which includes importing data, transforming data, and “tidying” data.\nlibrary(tidyverse)\nlibrary(janitor)\ngetwd()\n#setwd(\"~/COMM 605/Tutorial/\") # no need to set up if you are opening a Rproject"
  },
  {
    "objectID": "week4.html#understanding-data-frame",
    "href": "week4.html#understanding-data-frame",
    "title": "Week 4 Lab",
    "section": "Understanding data frame",
    "text": "Understanding data frame\n\nCreate a data frame\n\nheight &lt;- c(180, 155, 160, 167, 181)\nweight &lt;- c(65, 50, 52, 58, 70)\nnames &lt;- c(\"Joanna\", \"Charlotte\", \"Helen\", \"Karen\", \"Amy\")\n\ndata &lt;- data.frame(height = height, weight = weight, names = names) #stringsAsFactors = TRUE\ndata\ndim(data)\nstr(data)\n\n\n\nPositional index\n\ndata[1,3] # the first value (1st row ) of the names variable (3th column)\ndata$names[1]\ndata[1:2, 2:3] #  the first 2 rows and the last 2 columns\ndata[1:2, ]\ndata[, 2:3]\n\n\n\nOrdering data frames\n\nheight_order &lt;- data[order(data$height), ]\nrownames(height_order) &lt;- 1:nrow(height_order)\nheight_order\n\nheight_order &lt;- data[order(data$height, decreasing = T),]\n\n\n\nAdding/Removing columns and rows\n\ndata2 &lt;- data.frame(state = c(\"NY\", \"PA\", \"MD\", \"VA\", \"MA\"))\ndata_newcolumn &lt;- cbind(data, data2)\n\ndata_removecolumn &lt;- data_newcolumn[, c(1:2, 4)]\ndata_newcolumn$state &lt;- NULL\n\ndata3 &lt;- data.frame(height = c(120, 150, 132, 122),\n                    weight = c(44, 56, 49, 45),\n                    names = c(\"Ryan\", \"Chris\", \"Ben\", \"John\"))\ndata_newrow &lt;- rbind(data, data3)\ndata_removerow &lt;- data_newrow[c(1,6:9),]\n\n\n\nMerging data frames\nHere are two fictitious datasets of a clinical trial. One table contains demographic information of the patients and the other one adverse events recorded throughout the course of the trial.\n\ndemographics &lt;- data.frame(\n  id = c(\"P1\", \"P2\", \"P3\"),\n  age = c(40, 54, 47),\n  state = c(\"NY\", \"MA\", \"PA\"),\n  stringsAsFactors = FALSE\n)\n\nadverse_events &lt;- data.frame(\n  id = c(\"P1\", \"P1\", \"P3\", \"P4\"),\n  term = c(\"Headache\", \"Neutropenia\", \"Constipation\", \"Tachycardia\"),\n  onset_date = c(\"2020-12-03\", \"2021-01-03\", \"2020-11-29\", \"2021-01-27\"),\n  stringsAsFactors = FALSE\n)\n\n\nmerge(demographics, adverse_events, by = \"id\")\nmerge(demographics, adverse_events, by = \"id\", all.x = T)\nmerge(demographics, adverse_events, by = \"id\", all.y = T)\nmerge(demographics, adverse_events, by = \"id\", all = T)\n\n\nadverse_events2 &lt;- adverse_events\ncolnames(adverse_events2)[1] &lt;- \"pat_id\"\nmerge(demographics, adverse_events2, by.x = \"id\", by.y = \"pat_id\", all = T)\n\n\n\nExercise\nPlease merge the following two datasets emp_df (employee information)and dept_df (department information) using two ID columns dept_id and dept_branch_id.\n\nemp_df=data.frame(\n  emp_id=c(1,2,3,4,5,6),\n  name=c(\"Chris\",\"Rose\",\"Williams\",\"Jones\",\"Jayden\",\"Xavior\"),\n  superior_emp_id=c(1,1,1,2,2,2),\n  dept_id=c(10,20,10,10,40,30),\n  dept_branch_id= c(101,102,101,101,104,103)\n)\n\ndept_df=data.frame(\n  dept_id=c(10,20,30,40),\n  dept_name=c(\"Finance\",\"Marketing\",\"Sales\",\"IT\"),\n  dept_branch_id= c(101,102,103,104)\n)\n\n\n\nShow the code\nmerge(emp_df, dept_df, by = c(\"dept_id\", \"dept_branch_id\"), all.x = T)"
  },
  {
    "objectID": "week4.html#data-importing-and-exporting",
    "href": "week4.html#data-importing-and-exporting",
    "title": "Week 4 Lab",
    "section": "Data importing (and exporting)",
    "text": "Data importing (and exporting)\nCommon data formats:\n\nCSV (comma-seperated values) / TSV (tab-seperated values)\nxlsx\ntxt\nother softwares/packages: .sav(SPSS), .dta(STATA)\n\nCommon data types in R:\n\ndata frame\ntibble (tbl_df): it does much less than a data frame (a neater data frame), as it never changes the type of the inputs (e.g. it keeps list columns unchanged, and never converts strings to factors), it never changes the names of variables, it only recycles inputs of length 1, and it never creates row.names().\n\n\ndata &lt;- data.frame(a = 1:26, b = letters[1:26], c = Sys.Date() - 1:26)\ndata\nas_tibble(data)\n\n\nstudents &lt;- read.csv(\"https://pos.it/r4ds-students-csv\") # from URL\nstudents &lt;- read.csv(\"data/students.csv\") # from local\nstudents\nstr(students)\nsummary(students)\n\n\nstudents &lt;- read.csv(\"data/students.csv\", na.strings=c(\"N/A\", \"\"))\n# students &lt;- read_csv(\"data/students.csv\", na = c(\"N/A\", \"\"))\n\n\n❓ Question:: What is are the differences between read.csv and read_csv?\n\n\nBasic data cleaning\n\nstr(students)\nstudents %&gt;%\n  rename(student_id = Student.ID,\n         full_name = Full.Name,\n         fav_food = favourite.food)\n\nrename(students,\n       student_id = Student.ID,\n       full_name = Full.Name,\n       fav_food = favourite.food)\n\nstudents_rename &lt;- students %&gt;%\n                   rename(student_id = Student.ID,\n                          full_name = Full.Name,\n                          fav_food = favourite.food)\n\n\n# a faster way\nstudents_rename &lt;- clean_names(students)\nstudents_rename &lt;- mutate(students_rename, meal_plan = factor(meal_plan))\nstr(students_rename)\n\n\nstudents_clean &lt;- students_rename %&gt;%\n                  mutate(age = if_else(age == \"five\", \"5\", age))\n\n#students_rename2 &lt;- students_rename\n#students_rename2$age &lt;- ifelse(students_rename2$age == \"five\", 5, students_rename2$age)\n#students_rename2$age[students_rename2$age == \"five\"] &lt;- 5\n\nNote: if_else() has three arguments. The first argument test should be a logical vector. The result will contain the value of the second argument, yes, when test is TRUE, and the value of the third argument, no, when it is FALSE. Here we’re saying if age is the character string “five”, make it “5”, and if not leave it as age.\n\nstudents_clean_final &lt;- students_clean %&gt;%\n                        mutate(group = case_when(\n                          age &lt;= 5 ~ \"young\",\n                          age &gt; 5 ~ \"old\",\n                          .default = \"other\"\n                        ))\n\n\n❓ Question: how to use pipe %&gt;% to save some time here?\n\n\nShow the code\nstudents_clean_final &lt;- students %&gt;%\n                        clean_names() %&gt;%\n                        mutate(meal_plan = factor(meal_plan),\n                               age = parse_number(if_else(age == \"five\", \"5\", age)),\n                               group = case_when(\n                                       age &lt;= 5 ~ \"young\",\n                                       age &gt; 5 ~ \"old\",\n                                       .default = \"other\"))\n\n\n\nwrite.csv(students_clean_final, \"data/students_final.csv\", row.names = F)"
  },
  {
    "objectID": "week4.html#data-tidying",
    "href": "week4.html#data-tidying",
    "title": "Week 4 Lab",
    "section": "Data tidying",
    "text": "Data tidying\nIn real life, the social media data you collected is “messy” and “dirty”.\n\nData Scientists spend up to 80% of the time on data cleaning and 20 percent of their time on actual data analysis. 1\n\nThe process of “tidying” data would thus create what’s known as tidy data, as populated by Hadley Wickham (one of the authors of R for Data Science).\n\nTidy datasets provide a standardized way to link the structure of a dataset (its physical layout) with its semantics (its meaning). 2\n\nEach dataset shows the same values of four variables: country, year, population, and number of documented cases of TB (tuberculosis), but each dataset organizes the values in a different way.\n\n❓ Question: are they the same datasets? Which one is easier to work with and why?\n\ntable1\n\n# A tibble: 6 × 4\n  country      year  cases population\n  &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;\n1 Afghanistan  1999    745   19987071\n2 Afghanistan  2000   2666   20595360\n3 Brazil       1999  37737  172006362\n4 Brazil       2000  80488  174504898\n5 China        1999 212258 1272915272\n6 China        2000 213766 1280428583\n\ntable2\n\n# A tibble: 12 × 4\n   country      year type            count\n   &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;           &lt;dbl&gt;\n 1 Afghanistan  1999 cases             745\n 2 Afghanistan  1999 population   19987071\n 3 Afghanistan  2000 cases            2666\n 4 Afghanistan  2000 population   20595360\n 5 Brazil       1999 cases           37737\n 6 Brazil       1999 population  172006362\n 7 Brazil       2000 cases           80488\n 8 Brazil       2000 population  174504898\n 9 China        1999 cases          212258\n10 China        1999 population 1272915272\n11 China        2000 cases          213766\n12 China        2000 population 1280428583\n\ntable3\n\n# A tibble: 6 × 3\n  country      year rate             \n  &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;            \n1 Afghanistan  1999 745/19987071     \n2 Afghanistan  2000 2666/20595360    \n3 Brazil       1999 37737/172006362  \n4 Brazil       2000 80488/174504898  \n5 China        1999 212258/1272915272\n6 China        2000 213766/1280428583\n\n\nA tidy data set is:\n\nEach variable is a column; each column is a variable.\nEach observation is a row; each row is an observation.\nEach value is a cell; each cell is a single value.\n\n\n\n\nSource: r4ds\n\n\nLet’s work with the table1 here.\n\n# Compute rate per 10,000\ntb_table &lt;- table1 %&gt;%\n  mutate(rate = cases / population * 10000)\n\n\ntb_year &lt;- table1 %&gt;% \n  group_by(year) %&gt;% \n  summarize(total_cases = sum(cases))\n\n\n\nExercise\nUsing table1 to calculate the TB cases per year in each country. (Hint: use mean())\n\n\nShow the code\ntable1 %&gt;% \n  group_by(country) %&gt;% \n  summarize(mean_cases = mean(cases))"
  },
  {
    "objectID": "week4.html#data-transformation",
    "href": "week4.html#data-transformation",
    "title": "Week 4 Lab",
    "section": "Data transformation",
    "text": "Data transformation\n\nColumns and rows\nflights is a dataset on flights that departed from New York City in 2013.\n\n#install.packages(\"nycflights13\")\nlibrary(nycflights13)\nflights\nstr(flights)\n\n\n# Subseting certain columns\nflights_sub &lt;- flights %&gt;%\n  select(c(month, day, flight, carrier, origin, dest, distance, air_time))\n\n# Creating new columns that are derived from the existing columns\nflights_sub &lt;- flights_sub %&gt;% \n  mutate(speed = distance / air_time * 60)\n\n\n# Filtering certain rows\nflights_IAH &lt;- flights %&gt;%\n  filter(dest == \"IAH\")\n\nflights_summer &lt;- flights %&gt;%\n   filter(month == 6 | month == 7 | month == 8) #OR\n\nflights_summer &lt;- flights %&gt;%\n   filter(month %in% c(6,7,8))\n\nflights_jan1 &lt;- flights %&gt;% \n  filter(month == 1 & day == 1) #AND\n\n\nflights %&gt;% \n  arrange(year, month, day, dep_time)\n\nflights %&gt;% \n  arrange(desc(dep_delay))\n\n\nflights %&gt;% \n  distinct(origin, dest, .keep_all = TRUE)\n\n\nflights %&gt;% \n  group_by(month) %&gt;% \n  summarize(\n    mean_delay = mean(dep_delay, na.rm = TRUE), \n    n = n()\n  )\n\ndaily_flights &lt;- flights %&gt;%\n  group_by(year, month, day) %&gt;%\n  summarize(\n    mean_delay = mean(dep_delay, na.rm = TRUE), \n    n = n())\n\n\n\nExercise\n\nUsing the same flights dataset to creat a new dataframe flights_houston, and:\n\n\nOnly include flights heading to Houston (IAH and HOU)\nCalcuate the speed (\\(speed = distance / air\\_time * 60\\))\nOnly keep the columns of “year”, “month”, “day”,“dep_time”, “carrier”, “flight”, and “speed”\nArrange the data based on the speed with a desceding order.\n\n\n\nShow the code\nflights_houston &lt;- flights %&gt;% \n  filter(dest == \"IAH\" | dest == \"HOU\") %&gt;% \n  mutate(speed = distance / air_time * 60) %&gt;% \n  select(year:day, dep_time, carrier, flight, speed) %&gt;% \n  arrange(desc(speed))\n\n\n\nUsing the same flights dataset to find out which carrier heading to which airport has the worst average delays?\n\n\n\nShow the code\ndelay_flights &lt;- flights %&gt;%\n  group_by(carrier, dest) %&gt;%\n  summarize(\n    mean_delay = mean(dep_delay, na.rm = TRUE), \n    n = n())\n\n\n\n\nLengthening and widening data\nIn reality, you need long-format data much more commonly than wide-format data (such as visualizing in ggplot2 and modeling).\n\nWide format data: it has a column for each variable and a long format data. The billboard dataset records the billboard rank of songs in the year 2000:\n\n\nbillboard\n\n\nLong format data: it has a column for possible variable types and a column for the values of those variables. cms_patient_experience, is a dataset from the Centers of Medicare and Medicaid services that collects data about patient experiences:\n\n\ncms_patient_experience\n\ntidyr provides two functions for pivoting data:\n\npivot_longer(): it takes wide-format data and turn it into long-format data (melt in reshape2). \npivot_wider(): it takes long-format data and turn it into wide-format data (cast in reshape2).\n\n\nbillboard %&gt;% \n  pivot_longer(\n    cols = starts_with(\"wk\"), \n    names_to = \"week\", \n    values_to = \"rank\"\n  )\n\n\nbilloard_longer &lt;- billboard %&gt;% \n                    pivot_longer(\n                      cols = starts_with(\"wk\"), \n                      names_to = \"week\", \n                      values_to = \"rank\",\n                      values_drop_na = TRUE) %&gt;% \n                    mutate(week = parse_number(week))\n\nBut in reality, we might need to deal with multiple variables at the same time… Now, let’s take a look at the who2 dataset, the source of table1 that you saw above.\n\nwho2\n\nThis dataset is collected by the World Health Organisation, records information about tuberculosis diagnoses. There are two columns that are already variables and are easy to interpret: country and year. They are followed by 56 columns like “sp_m_014”, “ep_m_4554”, and “rel_m_3544”. Each column name is made up of three pieces separated by “_”. The first piece,“sp/rel/ep”, describes the method used for the diagnosis, the second piece, “m/f” is the gender (coded as a binary variable in this dataset), and the third piece, “014/1524/2534/3544/4554/5564/65” is the age range (“014” represents “0-14”, for example).\n\nwho2_long &lt;- who2 %&gt;% \n              pivot_longer(\n                cols = !(country:year),\n                names_to = c(\"diagnosis\", \"gender\", \"age\"), \n                names_sep = \"_\",\n                values_to = \"count\"\n              )\n\n\n\nExercise\nThe following (wide) dataset shows the number of points scored by various NBA basketball players in different years. Please tranform it to a long format.\n\nnba &lt;- data.frame(player=c('Joel Embiid', 'Luka Doncic', 'Jayson Tatum', 'Trae Young'),\n                 year1=c(28.5, 27.7, 26.4,25.3),\n                 year2=c(30.6, 28.4, 26.9, 28.4),\n                 year3=c(33.1, 32.4, 30.1, 26.2))\n\n\n\nShow the code\nnba_long &lt;- nba %&gt;%\n  pivot_longer(cols=c('year1', 'year2', 'year3'),\n               names_to='year',\n               values_to='points')\nnba_long"
  },
  {
    "objectID": "week4.html#footnotes",
    "href": "week4.html#footnotes",
    "title": "Week 4 Lab",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nDasu, T. & Johnson, T. (2003). Exploratory Data Mining and Data Cleaning.↩︎\nWickham, H. (2014). Journal of Statistical Software, 59(10), 1–23. https://doi.org/10.18637/jss.v059.i10↩︎"
  },
  {
    "objectID": "week5.html#collect-reddit-data-through-redditextractor-package.",
    "href": "week5.html#collect-reddit-data-through-redditextractor-package.",
    "title": "Week 5 Lab",
    "section": "Collect Reddit data through RedditExtractoR package.",
    "text": "Collect Reddit data through RedditExtractoR package.\n\nLoading package\n\n# install.packages(\"RedditExtractoR\")\nlibrary(RedditExtractoR)\n\n\n\nObtaining the urls from r/Rochester subreddit\n\nrocthreads &lt;- find_thread_urls(subreddit = \"Rochester\", sort_by = \"top\", period = \"all\") # you can use keywords to limit the search as well\n\n\nroccomments &lt;- get_thread_content(rocthreads$url)\n\nYou can find there are two dataframes the large list, the threads is for the posts, and the comments is for the comments. Similarily, you can also merge the metadata of posts with the comments based on the common column url.\n\nreddit_posts &lt;- roccomments$threads\nreddit_comments &lt;- roccomments$comments\nreddit_all &lt;- merge(reddit_comments, reddit_posts, by = \"url\", all.x = T)"
  },
  {
    "objectID": "week6.html",
    "href": "week6.html",
    "title": "Week 6 Lab",
    "section": "",
    "text": "We will be mainly using ggplot2 package (and maybe other extended packages) for data visualization in this class. ggplot2 is a system for declaratively creating graphics, based on The Grammar of Graphics. This lab is just a quick glimpse on ggplot2. And if you would like to learn more, check out Hadley Wickham’s ggplot2: Elegant Graphics for Data Analysis. Here is a cheatsheet by posit."
  },
  {
    "objectID": "week6.html#a-breakdown-of-the-common-layers",
    "href": "week6.html#a-breakdown-of-the-common-layers",
    "title": "Week 6 Lab",
    "section": "A breakdown of the common layers",
    "text": "A breakdown of the common layers\n\nData:\n\nyour data, in tidy format, will provide ingredients for your plot\nuse dplyr techniques to prepare data for optimal plotting format\nusually, this means you should have one row for every observation that you want to plot\n\nAesthetics (aes), to make data visible\n\nx, y: variable along the x and y axis\ncolour: color of geoms according to data\nfill: the inside color of the geom\ngroup: what group a geom belongs to\nshape: the figure used to plot a point\nlinetype: the type of line used (solid, dashed, etc)\nsize: size scaling for an extra dimension\nalpha: the transparency of the geom\n\nGeometric objects (geoms - determines the type of plot)\n\ngeom_point(): scatterplot\ngeom_line(): lines connecting points by increasing value of x\ngeom_path(): lines connecting points in sequence of appearance\ngeom_boxplot(): box and whiskers plot for categorical variables\ngeom_bar(): bar charts for categorical x axis\ngeom_histogram(): histogram for continuous x axis\ngeom_violin(): distribution kernel of data dispersion\ngeom_smooth(): function line based on data\n\nFacets\n\nfacet_wrap() or facet_grid() for small multiples\n\nStatistics\n\nsimilar to geoms, but computed\nshow means, counts, and other statistical summaries of data\n\nCoordinates - fitting data onto a page\n\ncoord_cartesian to set limits\ncoord_polar for circular plots\ncoord_map for different map projections\n\nThemes\n\noverall visual defaults\nfonts, colors, shapes, outlines\n\n\nOur goal: putting all the layers together!"
  },
  {
    "objectID": "slides/week5_slides.html",
    "href": "slides/week5_slides.html",
    "title": "COMM 605",
    "section": "",
    "text": "Platform-centric\n\nAPI\nWeb scrapping/crawling\n\nUser-centeric\n\nData donation\nTracking\n\nBrowser plug-in\nThe Human Screenome Project"
  },
  {
    "objectID": "slides/week5_slides.html#data-collection-on-social-media",
    "href": "slides/week5_slides.html#data-collection-on-social-media",
    "title": "COMM 605",
    "section": "",
    "text": "Platform-centric\n\nAPI\nWeb scrapping/crawling\n\nUser-centeric\n\nData donation\nTracking\n\nBrowser plug-in\nThe Human Screenome Project"
  },
  {
    "objectID": "slides/week5_slides.html#what-is-api",
    "href": "slides/week5_slides.html#what-is-api",
    "title": "COMM 605",
    "section": "What is API?",
    "text": "What is API?\n\n\n\nAn illustration of API"
  },
  {
    "objectID": "slides/week5_slides.html#what-is-api-1",
    "href": "slides/week5_slides.html#what-is-api-1",
    "title": "COMM 605",
    "section": "What is API?",
    "text": "What is API?\nApplication Programming Interfaces: API (Application Programming Interface) is a small script file (i.e., program) written by users, following the rules specified by the web owner, to download data from its database (rather than webpages)\nAn API script usually contains:\n\nLogin information (if required by the owner) ◦ Name of the data source requested\nName of the fields (i.e., variables) requested ◦ Range of date/time\nOther information requested\nFormat of the output data\netc."
  },
  {
    "objectID": "slides/week5_slides.html#what-is-api-2",
    "href": "slides/week5_slides.html#what-is-api-2",
    "title": "COMM 605",
    "section": "What is API?",
    "text": "What is API?\n\n\n\nAn illustration of Twitter API"
  },
  {
    "objectID": "slides/week5_slides.html#what-is-api-3",
    "href": "slides/week5_slides.html#what-is-api-3",
    "title": "COMM 605",
    "section": "What is API?",
    "text": "What is API?\nThe package(s) has made the life easier for you - It is a wrapper for everything: easy request"
  },
  {
    "objectID": "slides/week5_slides.html#the-apicalypse-and-its-aftermath",
    "href": "slides/week5_slides.html#the-apicalypse-and-its-aftermath",
    "title": "COMM 605",
    "section": "The APIcalypse and its aftermath",
    "text": "The APIcalypse and its aftermath\n\nCambridge Analytica\nAftermath: shutdown, suspension and updated TOS\n\nMeta: the CrowdTangle model, Social Science One, Ad Library\nTwitter\n\nbefore: the easy-access API and related packages (rtweet and academictwitteR)\nnow: restriction on free access\n\nYouTube: still available for now!!!\nReddit: restriction on free access\nTikTok: Research API\n\n\n\nCambridge analytica for micro-targeting"
  },
  {
    "objectID": "slides/week5_slides.html#data-collection-through-other-channels",
    "href": "slides/week5_slides.html#data-collection-through-other-channels",
    "title": "COMM 605",
    "section": "Data collection through other channels?",
    "text": "Data collection through other channels?\n\nCurated dataset:\n\nDocNow\nDataverse\nKaggle\nOther publications\n\nResearch institutes\n\nDigital methods initiatives\nSocial media lab\n\nCompanies/Organizations\n\nSocial Media Analysis Toolkit (SMAT)\nJunkipedia\n4CAT"
  },
  {
    "objectID": "slides/week3.html",
    "href": "slides/week3.html",
    "title": "Week 3: Overview of social media and computational communication research",
    "section": "",
    "text": "{css, echo=FALSE} .title {   font-size: 100px;   color: blue;   font-family: Arial;   font-variant: small-caps; }\n\nFirst slide"
  },
  {
    "objectID": "slides/week3_slides.html",
    "href": "slides/week3_slides.html",
    "title": "COMM 605",
    "section": "",
    "text": "Opportunities: individual, group, and societal level collected by new technologies and analyzed by new methods\nChallenges:\n\nnew paradigms needed for the massive dataset?\ninstitutional obstacle?\ndata access and privacy issue\n\n\n\n\n\n\nParadigm change?\n\nObservational\nTheoretical research\nExperimental\n\nQuestions:\n\nMore data is good data?\nNew tools but new theories?\nEthical issues?\nResources\n\n\n\n\n\n\nWhat is social media – compared to mass media\n\nOnline platform: blogs, business networks, collaborative projects, enterprise social networks, forums, microblogs, photo sharing, products review, social bookmarking, social gaming, SN, video sharing, and virtual worlds.\n\nWhat do social media do?\n\nSocializing/Romance\nInteracting with government/politicians/companies/brands\nJob seeking and professional networking\nBusiness\n\nHow would you define it?\n\n\n\n\n\n\n\n\n\n\n\n\n\nCommunication as a field has put less emphasis on methodologies\n\n\n“…lack of methods for the study of [communication] process and adoption of approaches from other fields”1\n\n\nDefinition of Computational Communication Science (if we ever need one)?\n\n\nAn application of computational science to questions of human and social communication”2\n\n\n\n\n\nA subfield of Computational Social Science 3\n\nlarge and complex data sets;\nconsisting of digital traces and other “naturally occurring” data;\nrequiring algorithmic solutions to analyze;\nallowing the study of human communication by applying and testing communication theory.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTax records\nSchool records\nPhone call records\n\n\n\n\n\n\n\n\nTwitter: text & network\nInstagram: image & text\nYoutube: image/video & text\n\n\n\n\n\n\n\nSome applications of machine learning\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Unsupervised\n\n\n\n\n\n\n\n(b) Supervised\n\n\n\n\nFigure 1: An analogy of unsupervised and supervised machine learning\n\n\n\n\n\n\n\n\nAutomatic content analysis\n\n\n\nExplanatory modeling\nPredictive modeling\n\n\n\n\n\n\n\n\n\n\nBIGNESS\nALWAYS-ON\nNONREACTIVE\n\n\n\n\n\n\nINCOMPLETE\nINACCESSIBLE\nNONREPRESENTATIVE\nDRIFTING\nALGORITHMICALLY CONFOUNDED\nDIRTY AND NOISY\nSENSITIVE (potential harm?)\n\n\n\n\n\n\n\n\n\n\nBig data hubris? “Big” data does not mean “Good” data\nValid and reliable measurement?\n\nValidation matters\n\nInterpretable machine learning?\n\nThe good-for-prediction vs. The good-for-explanation\n\nEthics: the fairness of machine learning\n\nAlgorithmic bias – annotation matters!\nCase: large language models, toxicity detection"
  },
  {
    "objectID": "slides/week3_slides.html#lazer-et-al.-2009",
    "href": "slides/week3_slides.html#lazer-et-al.-2009",
    "title": "COMM 605",
    "section": "",
    "text": "Opportunities: individual, group, and societal level collected by new technologies and analyzed by new methods\nChallenges:\n\nnew paradigms needed for the massive dataset?\ninstitutional obstacle?\ndata access and privacy issue"
  },
  {
    "objectID": "slides/week3_slides.html#hilbert-et-al.-2019",
    "href": "slides/week3_slides.html#hilbert-et-al.-2019",
    "title": "COMM 605",
    "section": "",
    "text": "Paradigm change?\n\nObservational\nTheoretical research\nExperimental\n\nQuestions:\n\nMore data is good data?\nNew tools but new theories?\nEthical issues?\nResources"
  },
  {
    "objectID": "slides/week3_slides.html#aichner-et-al.-2021",
    "href": "slides/week3_slides.html#aichner-et-al.-2021",
    "title": "COMM 605",
    "section": "",
    "text": "What is social media – compared to mass media\n\nOnline platform: blogs, business networks, collaborative projects, enterprise social networks, forums, microblogs, photo sharing, products review, social bookmarking, social gaming, SN, video sharing, and virtual worlds.\n\nWhat do social media do?\n\nSocializing/Romance\nInteracting with government/politicians/companies/brands\nJob seeking and professional networking\nBusiness\n\nHow would you define it?"
  },
  {
    "objectID": "slides/week3_slides.html#what-is-ccs",
    "href": "slides/week3_slides.html#what-is-ccs",
    "title": "COMM 605",
    "section": "",
    "text": "Communication as a field has put less emphasis on methodologies\n\n\n“…lack of methods for the study of [communication] process and adoption of approaches from other fields”1\n\n\nDefinition of Computational Communication Science (if we ever need one)?\n\n\nAn application of computational science to questions of human and social communication”2"
  },
  {
    "objectID": "slides/week3_slides.html#what-is-ccs-1",
    "href": "slides/week3_slides.html#what-is-ccs-1",
    "title": "COMM 605",
    "section": "",
    "text": "A subfield of Computational Social Science 3\n\nlarge and complex data sets;\nconsisting of digital traces and other “naturally occurring” data;\nrequiring algorithmic solutions to analyze;\nallowing the study of human communication by applying and testing communication theory."
  },
  {
    "objectID": "slides/week3_slides.html#what-data-is-out-there-1",
    "href": "slides/week3_slides.html#what-data-is-out-there-1",
    "title": "COMM 605",
    "section": "",
    "text": "Tax records\nSchool records\nPhone call records"
  },
  {
    "objectID": "slides/week3_slides.html#what-kinds-of-social-media-data",
    "href": "slides/week3_slides.html#what-kinds-of-social-media-data",
    "title": "COMM 605",
    "section": "",
    "text": "Twitter: text & network\nInstagram: image & text\nYoutube: image/video & text"
  },
  {
    "objectID": "slides/week3_slides.html#how-can-we-utilize-social-media-data",
    "href": "slides/week3_slides.html#how-can-we-utilize-social-media-data",
    "title": "COMM 605",
    "section": "",
    "text": "Some applications of machine learning"
  },
  {
    "objectID": "slides/week3_slides.html#how-can-we-utilize-social-media-data-1",
    "href": "slides/week3_slides.html#how-can-we-utilize-social-media-data-1",
    "title": "COMM 605",
    "section": "",
    "text": "(a) Unsupervised\n\n\n\n\n\n\n\n(b) Supervised\n\n\n\n\nFigure 1: An analogy of unsupervised and supervised machine learning"
  },
  {
    "objectID": "slides/week3_slides.html#how-can-we-utilize-social-media-data-2",
    "href": "slides/week3_slides.html#how-can-we-utilize-social-media-data-2",
    "title": "COMM 605",
    "section": "",
    "text": "Automatic content analysis\n\n\n\nExplanatory modeling\nPredictive modeling"
  },
  {
    "objectID": "slides/week3_slides.html#characteristics-of-big-data",
    "href": "slides/week3_slides.html#characteristics-of-big-data",
    "title": "COMM 605",
    "section": "",
    "text": "BIGNESS\nALWAYS-ON\nNONREACTIVE\n\n\n\n\n\n\nINCOMPLETE\nINACCESSIBLE\nNONREPRESENTATIVE\nDRIFTING\nALGORITHMICALLY CONFOUNDED\nDIRTY AND NOISY\nSENSITIVE (potential harm?)"
  },
  {
    "objectID": "slides/week3_slides.html#is-computational-approach-a-one-for-all-solution",
    "href": "slides/week3_slides.html#is-computational-approach-a-one-for-all-solution",
    "title": "COMM 605",
    "section": "",
    "text": "Big data hubris? “Big” data does not mean “Good” data\nValid and reliable measurement?\n\nValidation matters\n\nInterpretable machine learning?\n\nThe good-for-prediction vs. The good-for-explanation\n\nEthics: the fairness of machine learning\n\nAlgorithmic bias – annotation matters!\nCase: large language models, toxicity detection"
  },
  {
    "objectID": "slides/week3_slides.html#footnotes",
    "href": "slides/week3_slides.html#footnotes",
    "title": "COMM 605",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nPoole, M. S. (2007). Generalization in process theories of communication. Communication Methods and Measures, 1(3), 181-190.↩︎\nHilbert, M., Barnett, G., Blumenstock, J., Contractor, N., Diesner, J., Frey, S., … & Zhu, J. J. (2019). Computational communication science: A methodological catalyzer for a maturing discipline. International Journal of Communication, 13, 3912-3934.↩︎\nvan Atteveldt, W., & Peng, T. Q. (2018). When communication meets computation: Opportunities, challenges, and pitfalls in computational communication science. Communication Methods and Measures, 12(2-3), 81-92.↩︎"
  },
  {
    "objectID": "slides/week4_slides.html",
    "href": "slides/week4_slides.html",
    "title": "COMM 605",
    "section": "",
    "text": "Definition of digital media data: data that are collected, extracted, gathered, or scraped from a web-based platform such as a website, social networking site, mobile application, or another online space.\n\nDigital life data (e.g., text, image, and videos)\nDigital trace data (e.g., timestamp, author information, social media metrics)\nDigitized data (e.g., books, radio recordings, and broadcasts shared online)\n\n\n\n\nMain goal for social scientists:\n\nfeature extraction (variable)\nfeed the features into statistical modeling\n\nMethods:\n\nSupervised machine learning (classification of text): BERT model\n\nhate speech, conspiracy theory, framing, sentiment, toxicity, personality\n\nUnsupervised machine learning: topic modeling\n\nDiscourse/theme…\n\nDictionary-based approach: custom-made dictionaries, LIWC, VADER\n\nemotion, sentiment\n\n\n\nThe polarized discourses on Twitter about Gillette’s campaign on toxic masculinity (Xu & Xiong, 2020) The global LGBTQ CSR discourse in Fortune Global 500 companies’ annual report\n\n\n\n\n\n\n\nPeng, Y., Lu, Y., & Shen, C. (2023). An Agenda for Studying Credibility Perceptions of Visual Misinformation. Political Communication, 40(2), 225-237.\n\n\n\n\n\n\nZero-shot model (i.e., Microsoft’s Azure Face API):\n\nFacial detection/recognition (gender, age)\n\nThe links between personalities and number of faces in the respondents’ Instagram accounts (Kim & Kim, 2018)\n\nEmotion detection\n\nFacial expression emotions in credibility perception of a crisis management (Stephens et al., 2019)\n\nAesthetic features: brightness, contrast, composition, color, texture, blur, and complexity (image entropy)\n\nHow do the environmental characteristics (colors, luminance, and saturation, style)inspire consumers to engage in creating and posting environment-cued indirect advertising (Campell et al., 2022)\n\n\nSupervised machine learning:\n\nViolence in political protest\n\nUnsupervised machine learning:\n\nImage clustering\n\n\n\nMulti-modal data\n\n\n\n\nVocal pitch as measurement of emotional intensity in legislators’ speech when mentioning about women (Dietrich et al. 2019)\n\n\n\n\nDescriptive analysis\n\nCommunity detection\nNetwork centrality (i.e., opinion leader)\n\nInferential analysis\n\nHow do nodal and dyadic features influence the emergence of edge/link\n\nExponential Random Graph Model (ERGM): cross-sectional network\nStochastic actor-oriented model (SAOM) or SIENA model: longitudinal network\n\n\n\n\n\n\nResearch methods:\n\nObservational research\n\nLinking survey data and digital trace data\n\nExperiment\n\nVirtual lab experiment (e.g., apps/web as labs)\nVirtual field experiment (Bail et al., 2018)\n\nAlgorithm auditing\nSimulation\n\nHow does theory fit in?\n\nFraming (Chen et al., 2023)\nAgenda-setting (Vargo et al., 2018)\nTwo-step flow model/networked-step flow model (Hilbert et al., 2017)\nSelective exposure (Song & Boomgaarden, 2017)"
  },
  {
    "objectID": "slides/week4_slides.html#lukito-et-al.-2023",
    "href": "slides/week4_slides.html#lukito-et-al.-2023",
    "title": "COMM 605",
    "section": "",
    "text": "Definition of digital media data: data that are collected, extracted, gathered, or scraped from a web-based platform such as a website, social networking site, mobile application, or another online space.\n\nDigital life data (e.g., text, image, and videos)\nDigital trace data (e.g., timestamp, author information, social media metrics)\nDigitized data (e.g., books, radio recordings, and broadcasts shared online)"
  },
  {
    "objectID": "slides/week4_slides.html#text-as-data-natural-language-processing",
    "href": "slides/week4_slides.html#text-as-data-natural-language-processing",
    "title": "COMM 605",
    "section": "",
    "text": "Main goal for social scientists:\n\nfeature extraction (variable)\nfeed the features into statistical modeling\n\nMethods:\n\nSupervised machine learning (classification of text): BERT model\n\nhate speech, conspiracy theory, framing, sentiment, toxicity, personality\n\nUnsupervised machine learning: topic modeling\n\nDiscourse/theme…\n\nDictionary-based approach: custom-made dictionaries, LIWC, VADER\n\nemotion, sentiment\n\n\n\nThe polarized discourses on Twitter about Gillette’s campaign on toxic masculinity (Xu & Xiong, 2020) The global LGBTQ CSR discourse in Fortune Global 500 companies’ annual report"
  },
  {
    "objectID": "slides/week4_slides.html#imagevideo-as-data",
    "href": "slides/week4_slides.html#imagevideo-as-data",
    "title": "COMM 605",
    "section": "",
    "text": "Peng, Y., Lu, Y., & Shen, C. (2023). An Agenda for Studying Credibility Perceptions of Visual Misinformation. Political Communication, 40(2), 225-237."
  },
  {
    "objectID": "slides/week4_slides.html#imagevideo-as-data-1",
    "href": "slides/week4_slides.html#imagevideo-as-data-1",
    "title": "COMM 605",
    "section": "",
    "text": "Zero-shot model (i.e., Microsoft’s Azure Face API):\n\nFacial detection/recognition (gender, age)\n\nThe links between personalities and number of faces in the respondents’ Instagram accounts (Kim & Kim, 2018)\n\nEmotion detection\n\nFacial expression emotions in credibility perception of a crisis management (Stephens et al., 2019)\n\nAesthetic features: brightness, contrast, composition, color, texture, blur, and complexity (image entropy)\n\nHow do the environmental characteristics (colors, luminance, and saturation, style)inspire consumers to engage in creating and posting environment-cued indirect advertising (Campell et al., 2022)\n\n\nSupervised machine learning:\n\nViolence in political protest\n\nUnsupervised machine learning:\n\nImage clustering\n\n\n\nMulti-modal data"
  },
  {
    "objectID": "slides/week4_slides.html#audio-as-data",
    "href": "slides/week4_slides.html#audio-as-data",
    "title": "COMM 605",
    "section": "",
    "text": "Vocal pitch as measurement of emotional intensity in legislators’ speech when mentioning about women (Dietrich et al. 2019)"
  },
  {
    "objectID": "slides/week4_slides.html#network-as-data",
    "href": "slides/week4_slides.html#network-as-data",
    "title": "COMM 605",
    "section": "",
    "text": "Descriptive analysis\n\nCommunity detection\nNetwork centrality (i.e., opinion leader)\n\nInferential analysis\n\nHow do nodal and dyadic features influence the emergence of edge/link\n\nExponential Random Graph Model (ERGM): cross-sectional network\nStochastic actor-oriented model (SAOM) or SIENA model: longitudinal network"
  },
  {
    "objectID": "slides/week4_slides.html#new-methods-old-methods",
    "href": "slides/week4_slides.html#new-methods-old-methods",
    "title": "COMM 605",
    "section": "",
    "text": "Research methods:\n\nObservational research\n\nLinking survey data and digital trace data\n\nExperiment\n\nVirtual lab experiment (e.g., apps/web as labs)\nVirtual field experiment (Bail et al., 2018)\n\nAlgorithm auditing\nSimulation\n\nHow does theory fit in?\n\nFraming (Chen et al., 2023)\nAgenda-setting (Vargo et al., 2018)\nTwo-step flow model/networked-step flow model (Hilbert et al., 2017)\nSelective exposure (Song & Boomgaarden, 2017)"
  },
  {
    "objectID": "slides/week4_slides.html#serrano-et-al.-2020",
    "href": "slides/week4_slides.html#serrano-et-al.-2020",
    "title": "COMM 605",
    "section": "Serrano et al. (2020)",
    "text": "Serrano et al. (2020)\n\nData collection:\n\nhashtags: #republican and #democrat on February 1, 2020\noriginal videos of above\nvideos using the same audio clip\n\n\n\nData types:\n\nVideo\n\nAudio\n\nCaption\nInteraction: duet\n\n\n\nMeasurement and data analysis:\n\nManual labeling: Pro-democrat/Pro-republican/Non-partisan\nFace detection (Microsoft’s Azure Face API): emotion, gender, age\nAudio (speech-to-text): topic modeling\nCaption: topic modeling\nDuet: Intra-partisan or cross-partisan interaction"
  },
  {
    "objectID": "slides/week4_slides.html#kim-et-al.-2023",
    "href": "slides/week4_slides.html#kim-et-al.-2023",
    "title": "COMM 605",
    "section": "Kim et al. (2023)",
    "text": "Kim et al. (2023)\n\nData collection (through Junkipedia):\n\nanti-vaccination keywords (e.g., #abolishbigpharma, #arrestbillgates)\n\n\n\nData types:\n\nVideo: Audio/Transcript\nCaption\n\n\n\nMeasurement:\n\nEmotion\n\nFace emotion detection (thumbnails): fear, anger, and happiness\nEmotion detection (caption and transcript): Dictionary based analysis (NRC)\n\nIdentity\n\nA list of vocabularies signifying partisanships (e.g., Republicans, Democrats), socio-cultural identities (e.g., Asian, child, doctor), different age groups (e.g., old, young, GenZ), and ideological identities (e.g.,liberal, conservative)\nCount the words associated with each identity in TikTok captions and transcripts\n\nTikTok engagement\n\nnumber of shares, number of comments, and number of likes"
  },
  {
    "objectID": "slides/week4_slides.html#kim-et-al.-2023-1",
    "href": "slides/week4_slides.html#kim-et-al.-2023-1",
    "title": "COMM 605",
    "section": "Kim et al. (2023)",
    "text": "Kim et al. (2023)\n\nData analysis:\n\n(negative binominal) Regression\n\nEngagement as the dependent variables\nEmotion and identity cues as the independent variables\n\nMessage multimodalities\n\nEmotions in video captions, transcripts, and thumbnails\nIdentities in video captions and transcripts\nInteraction effect (affective polarization)\n\n\n\n\n\n\n\nTable 3 (Kim et al., 2023)"
  },
  {
    "objectID": "week8.html",
    "href": "week8.html",
    "title": "Week 8 Lab",
    "section": "",
    "text": "On social media platforms, text is often one of the most prevalent forms of data you will come across, in the shape of posts or comments. Similarly, textual data plays a significant role in social science, spanning from political discussions and newspaper archives to open-ended survey questions and reviews. We will be mainly using stringr package and glue package in the tidyverse to wrangle textual data in this tutorial. And we will mainly use quanteda for textual analysis (an alternative choice could be tidytext.\nlibrary(glue)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.0     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.3     ✔ tibble    3.1.8\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors"
  },
  {
    "objectID": "week8.html#understanding-text-as-a-string-of-characters",
    "href": "week8.html#understanding-text-as-a-string-of-characters",
    "title": "Week 8 Lab",
    "section": "Understanding text as a string of characters",
    "text": "Understanding text as a string of characters\nIn the previous tutorials, we have talked about various data types such as integer, numeric, factor, etc. Today we will delve into string (or character), which could be stored in multiple formats, such as txt, csv or json. When we think about text, we might think of sentences or words, but the computer only “thinks” about letters: text is represented internally as a string of characters.\n\n\n\n\n\n\nNote\n\n\n\nTechnically speaking, text is represented as bytes (numbers) rather than characters. The Unicode standard determines how these bytes should be interpreted or “decoded.” Here we assume that the bytes in a file are already “decoded” into characters (or Unicode code points), and we can just work with the characters. Especially if you are working with non-English text, it is very important to make sure you understand Unicode and encodings and check that the texts you work with are decoded properly.\n\n\n\ntext = \"This is text.\"\nglue(\"class(text): {class(text)}\")\n\nclass(text): character\n\nglue(\"length(text): {length(text)}\")\n\nlength(text): 1\n\nglue(\"text[1]: {text[1]}\")\n\ntext[1]: This is text.\n\nglue(\"str_length(text): {str_length(text)}\")\n\nstr_length(text): 13\n\nglue(\"str_sub(text, 6,7): {str_sub(text, 6,7)}\")\n\nstr_sub(text, 6,7): is\n\n\n\nwords = c(\"These\", \"are\", \"words\")\nglue(\"class(words): {class(words)}\")\n\nclass(words): character\n\nglue(\"length(words): {length(words)}\")\n\nlength(words): 3\n\nglue(\"words[1]: {words[1]}\")\n\nwords[1]: These\n\nwords_2_3 = str_c(words[2:3], collapse=\", \")\nglue(\"words[2:3]: {words_2_3}\")\n\nwords[2:3]: are, words"
  },
  {
    "objectID": "week8.html#deal-with-text-in-r",
    "href": "week8.html#deal-with-text-in-r",
    "title": "Week 8 Lab",
    "section": "Deal with text in R",
    "text": "Deal with text in R\n\nCommon strings operations in R (stringr)\n\n\nString operation\nstringr function\n\n\n\n\nCount characters in s\nstr_length(s)\n\n\nExtract a substring\nstr_sub(s, n1, n2)\n\n\nTest if s contains s2\nstr_detect(s, s2)\n\n\nCount number of matches of s2\nstr_count(s, s2)\n\n\nStrip spaces (at start and end)\ntrimws(s)\n\n\nConvert to lowercase/uppercase\ntolower(s) / toupper(s)\n\n\nFind s1 and replace by s2\nstr_replace(s, s1, s2)\n\n\n\n\ntext &lt;- \"    &lt;b&gt;Communication&lt;/b&gt;    (from Latin 'communicare', meaning to share)  \"\ncleaned &lt;- text %&gt;% \n  # remove HTML tags:\n  str_replace_all(\"&lt;b&gt;\", \" \")  %&gt;% \n  str_replace_all(\"&lt;/b&gt;\", \" \")  %&gt;%\n  # remove quotation marks:\n  str_replace_all(\"'\", \"\") %&gt;%\n  # normalize white space \n  str_squish() %&gt;%\n  # lower case\n  tolower()  %&gt;% \n  # trim spaces at start and end\n  trimws()\n\nglue(cleaned)\n\ncommunication (from latin communicare, meaning to share)\n\n\n\nstr_length(cleaned)\n\n[1] 56\n\nstr_sub(cleaned, 1, 13)\n\n[1] \"communication\"\n\nstr_detect(cleaned, \"communication\")\n\n[1] TRUE\n\nstr_count(cleaned, \"communication\")\n\n[1] 1"
  },
  {
    "objectID": "week8.html#regular-expression",
    "href": "week8.html#regular-expression",
    "title": "Week 8 Lab",
    "section": "Regular expression",
    "text": "Regular expression\nA regular expression or regex is a powerful language to locate strings that conform to a given pattern. For instance, we can extract usernames or email-addresses from text, or normalize spelling variations and improve the cleaning methods covered in the previous section. Specifically, regular expressions are a sequence of characters that we can use to design a pattern and then use this pattern to find strings (identify or extract) and also replace those strings by new ones.\nRegular expressions look complicated, and in fact they take time to get used to initially. For example, a relatively simple (and not totally correct) expression to match an email address is [\\w\\.-]+@[\\w\\.-]+\\.\\w\\w+, which doesn’t look like anything at all unless you know what you are looking for. The good news is that regular expression syntax is the same in R, Python as well as many other languages, so once you learn regular expressions you will have acquired a powerful and versatile tool for text processing.\nWhy is it important to learn regex for handling social media data? Social media data is typically quite messy and noisy, requiring extensive cleaning and extraction. However, there is no need to memorize all the expressions. You can refer to the [cheatsheet](https://rstudio.github.io/cheatsheets/strings.pdf for quick reference. Additionally, you can always copy and paste the codes from the tutorials and apply them to other social media platforms, in most cases.\n\ntweets &lt;- data.frame(id = 1:4,\n                     text = c(\"RT: @john_doe https://example.com/news VERY interesting! 😁\",\n                            \"tweet with just text 😉\",\n                            \"http://example.com/pandas #breaking #mustread \",\n                            \"@me and @myself #selfietime\"))\n\ntweets &lt;- tweets %&gt;% mutate(\n    # identify tweets with hashtags\n    hash_tag=str_extract_all(text, \"#\\\\w+\"),\n    # How many at-mentions are there?\n    n_at = str_count(text, \"(^|\\\\s)@\\\\w+\"),\n    mention = str_extract_all(text, \"(^|\\\\s)@\\\\w+\"),\n    # Extract first url\n    url = str_extract(text, \"(https?://\\\\S+)\"),\n    # Remove at-mentions, tags, and urls\n    clean_text = str_replace_all(text, \"(|^|\\\\s)(RT: |@|#|https?://)\\\\S+\", \" \") %&gt;%\n                 str_replace_all(\"\\\\W+\", \" \") %&gt;%\n                 tolower() %&gt;%\n                 trimws()\n          )\ntweets\n\n  id                                                        text\n1  1 RT: @john_doe https://example.com/news VERY interesting! 😁\n2  2                                     tweet with just text 😉\n3  3              http://example.com/pandas #breaking #mustread \n4  4                                 @me and @myself #selfietime\n              hash_tag n_at       mention                       url\n1                         1     @john_doe  https://example.com/news\n2                         0                                    &lt;NA&gt;\n3 #breaking, #mustread    0               http://example.com/pandas\n4          #selfietime    2 @me,  @myself                      &lt;NA&gt;\n            clean_text\n1     very interesting\n2 tweet with just text\n3                     \n4                  and\n\n\nAdditionally, you might want to split and joining strings.\n\ntext = \"apples, pears, oranges\"\nitems=strsplit(text,\", \", fixed=T)[[1]]\nitems=str_split(text,\"\\\\p{PUNCTUATION}\\\\s*\")[[1]]\nitems=str_extract_all(text,\"\\\\p{LETTER}+\")[[1]]\nprint(items)\n\n[1] \"apples\"  \"pears\"   \"oranges\"\n\njoined = str_c(items, collapse=\" & \")\nprint(joined)\n\n[1] \"apples & pears & oranges\"\n\n\n\nretweet &lt;- tweets %&gt;% \n  unnest_wider(mention, names_sep = \"_\") %&gt;%\n  select(id, starts_with(\"mention\")) %&gt;%\n  pivot_longer(cols = starts_with(\"mention\"),\n  values_to = \"mention\") %&gt;%\n  filter(!is.na(mention))"
  },
  {
    "objectID": "week8.html#simple-frequency",
    "href": "week8.html#simple-frequency",
    "title": "Week 8 Lab",
    "section": "Simple frequency",
    "text": "Simple frequency\n\ndfm_inaug &lt;- corpus(data_corpus_inaugural) %&gt;%\n             tokens(remove_punct = T) %&gt;%\n             dfm(tolower = T) %&gt;%\n             dfm_remove(stopwords(\"english\")) %&gt;%\n             dfm_trim(min_termfreq = 10, verbose = FALSE)\n\nfeatures_dfm_inaug &lt;- textstat_frequency(dfm_inaug, n = 100)\n\n# Sort by reverse frequency order\nfeatures_dfm_inaug$feature &lt;- with(features_dfm_inaug, reorder(feature, -frequency))\n\nggplot(features_dfm_inaug, aes(x = feature, y = frequency)) +\n    geom_point() + \n    theme(axis.text.x = element_text(angle = 90, hjust = 1))\n\n\n\n\n\n# Get frequency grouped by president\nfreq_grouped &lt;- textstat_frequency(dfm_inaug, \n                                   groups = President)\n\n# Filter the term \"american\"\nfreq_american &lt;- subset(freq_grouped, freq_grouped$feature %in% \"american\")  \n\nggplot(freq_american, aes(x = group, y = frequency)) +\n    geom_point() + \n    scale_y_continuous(limits = c(0, 14), breaks = c(seq(0, 14, 2))) +\n    xlab(NULL) + \n    ylab(\"Frequency\") +\n    theme(axis.text.x = element_text(angle = 90, hjust = 1))\n\nWarning: Removed 1 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\ndfm_weight_pres &lt;- data_corpus_inaugural %&gt;%\n  corpus_subset(Year &gt; 2000) %&gt;%\n  tokens(remove_punct = TRUE) %&gt;%\n  tokens_remove(stopwords(\"english\")) %&gt;%\n  dfm() %&gt;%\n  dfm_weight(scheme = \"prop\")\n\n# Calculate relative frequency by president\nfreq_weight &lt;- textstat_frequency(dfm_weight_pres, n = 10, \n                                  groups = dfm_weight_pres$President)\n\nggplot(data = freq_weight, aes(x = nrow(freq_weight):1, y = frequency)) +\n     geom_point() +\n     facet_wrap(~ group, scales = \"free\") +\n     coord_flip() +\n     scale_x_continuous(breaks = nrow(freq_weight):1,\n                        labels = freq_weight$feature) +\n     labs(x = NULL, y = \"Relative frequency\")"
  },
  {
    "objectID": "week8.html#collocation-analysis",
    "href": "week8.html#collocation-analysis",
    "title": "Week 8 Lab",
    "section": "Collocation analysis",
    "text": "Collocation analysis\n\ntoks &lt;- corpus(data_corpus_inaugural) %&gt;%\n             tokens(remove_punct = T) \ncol &lt;- toks %&gt;% \n       tokens_remove(stopwords(\"en\")) %&gt;% \n       tokens_select(pattern = \"^[A-Z]\", valuetype = \"regex\", \n                     case_insensitive = FALSE, padding = TRUE) %&gt;% \n       textstat_collocations(min_count = 5, tolower = FALSE)\ncol\n\n                collocation count count_nested length    lambda         z\n1             United States   158            0      2  8.477149 27.929888\n2        Federal Government    32            0      2  5.401997 21.196633\n3             Chief Justice    14            0      2  8.740786 18.100926\n4              Almighty God    15            0      2  6.879013 17.672882\n5                  Mr Chief     9            0      2  7.426440 16.193146\n6               North South    10            0      2  8.551340 15.667999\n7       Constitution United    19            0      2  3.835956 15.233607\n8                 World War     6            0      2  7.697471 14.091753\n9                   Mr Vice     5            0      2  7.168344 12.776044\n10               Justice Mr     5            0      2  6.669358 12.702532\n11      National Government    11            0      2  5.524524 12.497626\n12               Mr Speaker     5            0      2  8.015733 12.059646\n13           United Nations     9            0      2  6.752797 11.689890\n14                Old World    10            0      2 10.898295 11.620480\n15         Chief Magistrate    10            0      2 10.120438 11.203691\n16           Father Country     5            0      2 11.139123 10.459320\n17       Western Hemisphere     5            0      2 11.139123 10.459320\n18            South America     6            0      2  4.481760 10.027368\n19         President United     8            0      2  3.678079  9.996520\n20                Thank God     5            0      2  7.408127  9.618008\n21          Southern States     6            0      2  5.673082  9.213078\n22            Federal Union     5            0      2  3.806929  8.386138\n23 Declaration Independence     8            0      2 13.520351  8.102662\n24        District Columbia     5            0      2 13.085079  7.752972\n25           Vice President    18            0      2 10.378109  7.217968\n26        George Washington     5            0      2 10.469830  7.010152\n27       General Government    24            0      2  9.267504  6.481981\n28           President Bush     6            0      2  9.173255  6.234166\n29          America America     6            0      2  2.444162  6.030211\n30       Constitution Union     5            0      2  2.615266  5.931796\n31         Government Union     6            0      2  2.293547  5.655374\n32           States America     7            0      2  2.044860  5.435018\n33        Government United     5            0      2  1.908017  4.365975"
  },
  {
    "objectID": "week8.html#documentfeature-similarity-scores",
    "href": "week8.html#documentfeature-similarity-scores",
    "title": "Week 8 Lab",
    "section": "Document/feature similarity scores",
    "text": "Document/feature similarity scores"
  },
  {
    "objectID": "week5.html#get-your-youtube-client-id-and-secret",
    "href": "week5.html#get-your-youtube-client-id-and-secret",
    "title": "Week 5 Lab",
    "section": "",
    "text": "You will need to have a Google account/gmail (e.g., RIT email).\nLog in Google’s developers console to create a new project: https://console.developers.google.com/\n\n\n\nClick “Enabled APIs & services”, search for “YouTube Data API V3” and enable it.\n\n  \n\nCreate your API key, choose “Public data”, then restrict the API with only the YouTube Data API V3.   \nCreate the credentials for your API keys. Configure your OAuth consent screen, choose internal user type (if you choose external user, extra steps for authorization might be needed), put in your email contact,then you should be able to see the client ID and secret. Copy and save it, and don’t share with others.\n\n   \nLastly you might need to add a redirect url into your client ID. Under Cresidentials, click on the Client ID you created, then put “http://localhost:1410/” into “Authorized redirect URIs”, and save it.\nAdditionally, if you are making it for external usage, you might need to do an extra step for verification. Under the tab of “OAuth consent screen”, click “Publish APP”."
  },
  {
    "objectID": "slides/week6_slides.html#defining-social-data",
    "href": "slides/week6_slides.html#defining-social-data",
    "title": "COMM 605",
    "section": "",
    "text": "Social data in the digital form: digital traces produced by or about users, with an emphrasis on content explictly when written with the intent of communicating or interacting with others.\n\nSocial media and networking\nQ&A platforms\nCollaboration"
  },
  {
    "objectID": "slides/week6_slides.html#the-case-of-google-flu-trends",
    "href": "slides/week6_slides.html#the-case-of-google-flu-trends",
    "title": "COMM 605",
    "section": "The case of Google Flu Trends",
    "text": "The case of Google Flu Trends\n\n\n\nLazer et al. (2014)\n\n\n\nThe example of Google Flu Trend which was discontinued in 2015. A team at Google and CDC using the data from search engine about influenzas, because people might search for flu remedies or flu symptom, and they used that do prediction. It was quite successful at the beginning then it turns out to be embarrassment. Google Flu Trends persistently overestimating flu prevalence for a much longer time. GFT also missed by a very large margin in the 2011–2012 flu season and has missed high for 100 out of 108 weeks starting with August 2011. And this error is not random errors. One of the potential causes are algorithmic confounding because Google began suggesting related search terms when people search for flu symptoms, which increase the so-called flu prevalence."
  },
  {
    "objectID": "slides/week6_slides.html#validity-of-social-data-research",
    "href": "slides/week6_slides.html#validity-of-social-data-research",
    "title": "COMM 605",
    "section": "Validity of social data research",
    "text": "Validity of social data research\n\n\nConstruct validity: “Do our measurements over our data measure what we think they measure?”\n\nTheoretical construct\n\nInternal validity: “Does our analysis correctly lead from the measurements to the conclusions of the study?”\n\nAnalysis and assumption: data cleaning, distribution\n\nExternal validity: “To what extent can research findings be generalized to other sitations?”\n\nEcological validity: to what extent a social media platform propoely refects a broader real-world phenomenon\nTemporal validity: to what extent the construct change over time"
  },
  {
    "objectID": "slides/week6_slides.html#potential-biases",
    "href": "slides/week6_slides.html#potential-biases",
    "title": "COMM 605",
    "section": "Potential biases",
    "text": "Potential biases\n\nData bias: sparsity and noise; availability and representativeness\nPopulation bias: demographic attributes\nBehavioral bias: interaction bias; content consumption bias; self-selection and response bias\nContent production bias: the use of language(s), the contextual factors, power users, and different populations.\nLinking bias: network attributes; behavior-base and cpnnection-based social links;\nTemporal bias:\n\nPopulations, behaviors and systems drifting over time\nSeasonal and periodic phenonmena\nSudden-onset phenomena\nTime granularity can be too fine-grained to observe long-term phenomena: An example of Trump and hate speech on Twitter\nTime granularity can be too coarse-grained to observe short-lived phenonmena\nDataset decay and lose utility over time\n\nRedundancy: lexical and semantic redundancy"
  },
  {
    "objectID": "slides/week6_slides.html#other-biases",
    "href": "slides/week6_slides.html#other-biases",
    "title": "COMM 605",
    "section": "Other biases",
    "text": "Other biases\n\nData source or origin\nData collection: data acquistion, querying data, and (post-)filtering\nData processing: data cleaning, enrichment (annotation), aggregation\nData analysis: qualitative, descritive, inference and/or prediction"
  },
  {
    "objectID": "slides/week6_slides.html#three-ethical-challenges-examples",
    "href": "slides/week6_slides.html#three-ethical-challenges-examples",
    "title": "COMM 605",
    "section": "Three ethical challenges examples",
    "text": "Three ethical challenges examples\n\nEmotion contagion project\nFour experimental groups:\n\na “negativity-reduced” group, for whom posts with negative words (e.g., sad) were randomly blocked from appearing in the News Feed\na “positivity-reduced” group, for whom posts with positive words (e.g., happy) were randomly blocked\nand two control groups, one for the positivity-reduced group and one for the negativity-reduced group.\n\nResults:\n\nPeople in the positivity-reduced group used slightly fewer positive words and slightly more negative words relative to the control group. Likewise, people in the negativity-reduced group used slightly more positive words and slightly fewer negative words. Thus, the researchers found evidence of emotional contagion\n\nIssues (PNAS and editorial expression of concern):\n\nParticipants did not provide any consent beyond the standard Facebook terms of service and\nThe study had not undergone meaningful third-party ethical review"
  },
  {
    "objectID": "slides/week6_slides.html#three-ethical-challenges-examples-1",
    "href": "slides/week6_slides.html#three-ethical-challenges-examples-1",
    "title": "COMM 605",
    "section": "Three ethical challenges examples",
    "text": "Three ethical challenges examples\n\nTastes, Ties, and Time project\nBeginning in 2006, each year, a team of professors and research assistants scraped the Facebook profiles of members of the Class of 2009 at a “diverse private college in the Northeastern U.S.” The researchers then merged these data from Facebook, which included information about friendships and cultural tastes, with data from the college, which included information about academic majors and where the students lived on campus.\nIssue (oepn-source and privacy): Unfortunately, just days after the data were made available, other re- searchers deduced that the school in question was Harvard\n\nThese merged data were a valuable resource, and they were used to create new knowledge about topics such as how social networks form and how social networks and behavior co-evolve."
  },
  {
    "objectID": "slides/week6_slides.html#three-ethical-challenges-examples-2",
    "href": "slides/week6_slides.html#three-ethical-challenges-examples-2",
    "title": "COMM 605",
    "section": "Three ethical challenges examples",
    "text": "Three ethical challenges examples\n\nEncore project\nIn March 2014, Sam Burnett and Nick Feamster launched Encore, a system to provide real-time and global measurements of Internet censorship. To do this, the researchers, who were at Georgia Tech, encouraged website owners to install this small code snippet into the source files of their web page.\nIssue: Ben Zevenbergen concerned that people in certain countries could be exposed to risk if their computer attempted to visit certain sensitive websites, and these people did not con- sent to participate in the study.\n\nIf you happen to visit a web page with this code snippet in it, your web browser will try to contact a website that the researchers were monitoring for possible censorship. Before launching the project, the researchers conferred with their IRB, which declined to review the project because it was not “human subjects research” under the Common Rule."
  },
  {
    "objectID": "slides/week6_slides.html#ethics-in-digital-research",
    "href": "slides/week6_slides.html#ethics-in-digital-research",
    "title": "COMM 605",
    "section": "Ethics in digital research",
    "text": "Ethics in digital research\n\nFour principles (Salganik, 2017):\n\nRespect for persons: treating people as autoonomous and honoring their wishes\nBeneficence: understanding and improving the risk/benefit profile of your study, and then deciding if it strikes the right balance.\n\nDo not harm and maximinze possible benefit and minimize possible harms\n\nJustice: ensuring the risks and benefits of reesarch are distributed fairly\nRespect for law and public interest: including all relevant stakeholders"
  },
  {
    "objectID": "slides/week6_slides.html#but-how",
    "href": "slides/week6_slides.html#but-how",
    "title": "COMM 605",
    "section": "But how?",
    "text": "But how?\n\nPractical tips (Salganik, 2017):\n\nThe IRB is a floor, not a ceiling\nPut yourself in everyone else shoes\nThink of research ethics as continuous, not discrete"
  },
  {
    "objectID": "slides/week8_slides.html#defining-computational-text-analysis",
    "href": "slides/week8_slides.html#defining-computational-text-analysis",
    "title": "COMM 605",
    "section": "",
    "text": "A few different jargons:\n\nNatural language processing\n\nNOT Natural language understanding or generation\n\nAutomatic content analysis\n\nCompare with traditional content analysis (human coders)\n\nText-as-data\n\nPolitical science: political speech, newspaper\nWhat are the differences?\n\n\n\nLong articles, which might less messy (comparatively speaking)"
  },
  {
    "objectID": "slides/week8_slides.html#defining-computational-text-analysis-1",
    "href": "slides/week8_slides.html#defining-computational-text-analysis-1",
    "title": "COMM 605",
    "section": "Defining computational text analysis",
    "text": "Defining computational text analysis\nAn approach in which the analysis of text is, to some extent, automatically conducted by machines.\n\nUnlike qualitative way: text is not read and understood as one unit, but automatically broken down to its features, or token. The complexity of texts is then reduced further by converting text to numbers.\nAutomated approaches do not replace human abilities to understand text. Rather, they amplify them: human decisions lie at the core of “automated” content analyses and thus necessarily introduce certain degrees of freedom to these approaches.\n\nHow to prepare text\nHow to clean text\nWhat method to infer latent concepts of interest\nOngoing debate: which variables/latent constructs can and should be measured automatically instead of relying on human coding\n\nSentiment? Frames? Media bias?"
  },
  {
    "objectID": "slides/week8_slides.html#a-broad-typology",
    "href": "slides/week8_slides.html#a-broad-typology",
    "title": "COMM 605",
    "section": "A broad typology",
    "text": "A broad typology\n\nDeductive: assignment known categories to text\n\nDictionaries and rule-based approach\n\ni.e., keywords, hashtags, link, name entitie\n\nSupervised machine learning\n\ni.e., classification (categories), regression (score)\n\n\nInductive: exploring unknown categories in text\n\nUnsupervised machine learning\n\ni.e., topic modeling to identify topics based on word co-occurences.\n\n\nLarge language model: GPT, LLaMA, PaLM, etc."
  },
  {
    "objectID": "slides/week8_slides.html#what-questions-we-can-ask-in-communication",
    "href": "slides/week8_slides.html#what-questions-we-can-ask-in-communication",
    "title": "COMM 605",
    "section": "What questions we can ask (in communication)?",
    "text": "What questions we can ask (in communication)?\n, - Actors: Name entities recognition (NER) for persons, organizations, or location - simple count (mention) - measure how different entities relate to each other (who talks about whom) - sentiment concerning specific actors (how an entity is evaluated) - Sentiment or Tone (not stance) - The general sentiment or tone of news in economics or political coverage - Machine learning approaches in general might be better suited to analyze sentiment than dictionaries, and they might still fall short of human coding (van Atteveldt et al. 2021). - Topics: what is being talked about in texts - Frames: How issues are being talked about, in particular framing as the selection and salience of specific aspects - OpenFraming"
  },
  {
    "objectID": "slides/week8_slides.html#what-is-missing",
    "href": "slides/week8_slides.html#what-is-missing",
    "title": "COMM 605",
    "section": "What is missing?",
    "text": "What is missing?\nData validiation (for supervised approach)\n\nOne should not blindly trust the results of any automated method!\nValidity is reassured by comparing automated results,\n\ni.e., which texts were assigned which sentiment, to a benchmark.\nOftentimes, this benchmark is manually annotated data as a gold standard, here describing which sentiment humans would assign. While this gold standard not necessarily implies the “true” value as human coding is quite erroneous\n\nPrecision indicates how many articles predicted to contain negative sentiment according to the automated analysis actually contain negative sentiment according to the manual benchmark\n\nHow good is the model at not creating too many false positives?\n\nRecall indicates how many articles that actually contain negative sentiment were found\n\nHow good is our model at not creating too many false negatives?\n\n\n\n\n\nPrecision: For example, a value of .8 implies that 80 % of all articles that do contain negative sentiment according to the automated classification actually contain negative sentiment according to the manual benchmark. However, 20 % were misclassified as containing negative sentiment and do, in fact, not.\nRecall:For example, a value of .8 implies that 80 % of all articles with negative sentiment were found by the automated approach. However, 20 % were not because they had been misclassified as not containing negative sentiment when they in fact did\nThe validation of unsupervised models is less direct. While studies argue that topic models, for example, can be validated by manually checking whether topics are coherent (Quinn et al. 2010) and can be differentiated from other topics (Chang et al. 2009, see Grimmer and Stewart 2013 for other approaches), there are no clear thresholds for what constitutes a valid mode."
  },
  {
    "objectID": "slides/week8_slides.html#next-steps",
    "href": "slides/week8_slides.html#next-steps",
    "title": "COMM 605",
    "section": "Next steps?",
    "text": "Next steps?\n\nWhat we will cover in this class?\n\nDictionaries and rule-based approach in sentiment analysis\nUnsupervised machine learning in topic modeling\n(maybe) Zero-shot machine learning in toxcicity detection\n\nGoogle’s Perspective API (no training dataset is needed)\n\n\n\n\nWhat we will NOT cover in this class?\n\nSupervised machine learning: train your own model for classification/regression\nDeep learning (neural network)"
  },
  {
    "objectID": "practice1.html",
    "href": "practice1.html",
    "title": "Practice 1: Data wrangling and visualization",
    "section": "",
    "text": "The dataset youtubers.csv comprises information about top 1000 YouTube influencers as of Sep 2022 (original dataset could be accessed from Kaggle). This dataset includes the following columns:\n\nRanking\nChannel name\nYouTuber name\nCategory\nCountry\nNumber of subscribers\nAverage number of views (on the channel)\nAverage number of likes (on the channel)\nAverage number of comments (on the channel)\n\nAdditionally, there is a supplementary dataset named channel.csv, which I created for this (yep, I made it up!). It contains the number of videos uploaded on each channel and and the year of channel creation. Please use the youtuber.csv and channel.csv to complete the tasks outlined below. You are feel free to submit a R script or a markdown file. You are free to submit your responses in the form of an R script or a markdown file. If you choose to use an R script, you can include your answers as comments (begining with a #). As long as your code can render the same results as mine, I will give it full credits. In other words, the elegance of code will not be considered part of the rubrics. Additionally, there are no restrictions on the packages you can use. If you encounter challenges during the assignment, please submit your best effort, and your work will be evaluated based on your attempts.\n\n\n\n\n\n\nNote\n\n\n\nDisclaimer: Please be aware that the data obtained from Kaggle may not be entirely accurate. It’s important to note that we are utilizing this data solely for the purpose of practice and learning, rather than relying on it for real-world applications.”\n\n\n\nQuestion 1: Data Cleaning\n\nUpon importing the youtubers.csv, you may observe that certain column names and rows require some basic data cleaning:\n\n\nRename the following column names (you are freee choose alternative names if you prefer, but please keep them clear and straightforward):\n\n“S.no” to “ranking”\n“X.views..Avg….” to “views”\n“X.likes..Avg….” to “likes”\n“X.comments..Avg….” to “comments”\n\nRemove the column “Category_3”, as it will not be used in subsequent analyses.\nEnsure that there are 1000 unique YouTubers in the list. If you encounter duplicate rows, remove them based on the column of “ranking” (or the previous “S.no”).\nSave your result in a new dataframe.\n\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.0     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.3     ✔ tibble    3.1.8\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nyoutubers &lt;- read.csv(\"youtubers.csv\")\n\n\nyoutubers &lt;- youtubers %&gt;%\n  rename(ranking = \"S.no\",\n         views = \"X.views..Avg....\",\n         likes = \"X.likes..Avg....\",\n         comments = \"X.comments..Avg....\")\nyoutubers$Category_3 &lt;- NULL\nyoutubers &lt;- youtubers %&gt;%\n  arrange(ranking) %&gt;%\n  distinct(ranking, .keep_all = T)\n\n\n\nQuestion 2: Exploratory Data Analysis\n\nLet’s take a quick glimpse of the data. We are particularly interested in two variables: “country” and “category” (tip: do not forget to convert them into factor variables):\n\n\nHow many unique countries and categories (of these YouTube channels) exist in the dataset (omiting the NA value)?\nWhich are the top 3 countries and top 3 categories to which these YouTubers belong (omiting the NA value)?\nUtilize a bar chart to visualize the distribution of the two variables separately: “country” and “category” (note: There are no specific requirements for aesthetics; you can choose the visualization style that you find most suitable.)\n\n\nyoutubers$country &lt;- as.factor(youtubers$country)\nyoutubers$category &lt;- as.factor(youtubers$category)\n\nsummary(youtubers$country)\n\n       Algeria      Argentina     Bangladesh         Brazil       Colombia \n             5              6              3             79             10 \n       Ecuador          Egypt         France          India      Indonesia \n             1              7              5            208             38 \n          Iraq          Japan         Jordan       Malaysia         Mexico \n             6              4              1              1             66 \n       Morocco       Pakistan           Peru    Philippines         Russia \n             3              7              2             13             32 \n  Saudi Arabia        Somalia          Spain       Thailand         Turkey \n             4              1              5             15              4 \nUnited Kingdom  United States           NA's \n             9            293            172 \n\nsummary(youtubers$category)\n\n      Animals & Pets            Animation                 ASMR \n                   2                   94                    2 \n    Autos & Vehicles               Beauty          Daily vlogs \n                   2                    2                   37 \n          Design/art     DIY & Life Hacks            Education \n                   3                   10                   32 \n             Fashion              Fitness        Food & Drinks \n                   3                    3                   16 \n  Health & Self Help                Humor               Movies \n                   3                   15                  101 \n       Music & Dance              Mystery      News & Politics \n                 226                    1                   49 \nScience & Technology               Sports                 Toys \n                  17                    9                   21 \n              Travel          Video games                 NA's \n                   1                   63                  288 \n\nggplot(data = youtubers, aes(x = country)) +\n  geom_bar() +\n  coord_flip()\n\n\n\nggplot(data = youtubers, aes(x = category)) +\n  geom_bar() +\n  coord_flip()\n\n\n\n\n\n\nQuestion 3: Classifying Influencers\n\nThese 1000 YouTubers are curated based on the count of subscribers. Now let’s further classify these influencers by the following steps.\n\n\nCreate a new column named “type” and recode the values according to the following criteria:\n\nIf the number of subscribers exceeds 100 million, code it as “super influencer”;\nIf the number of subscribers falls within the range of 10 million to 100 million, code it as “mega influencer”;\nIf the number of subscribers falls within the range of 1 million to 10 million, code it as “power influencer”.\n\nHow many YouTubers are classified under each of these three types?\nUse a boxplot to show distribution of the views for these three influencer type. (tip: Given the potentially large range of view counts, consider applying data transformations to enhance visualization if necessary, such as log transformation [log()])\n\n\n\n\n\n\n\nNote\n\n\n\nDisclaimer: Please note that the classification applied here may not necessarily align with established categories found in social media influencer research. This classification is for the purpose of this assignment and may not reflect the nuances present in the broader literature.\n\n\n\nyoutubers &lt;- youtubers %&gt;%\n  mutate(type = case_when(\n    subscribers &gt; 100000000 ~ \"super influencer\",\n    subscribers &gt; 10000000 ~ \"mega influencer\",\n    subscribers &gt; 1000000 ~ \"power influencer\"\n  ))\n\nyoutubers$type &lt;- as.factor(youtubers$type) \nsummary(youtubers$type)\n\n mega influencer power influencer super influencer \n             958               36                6 \n\nyoutubers %&gt;%\nggplot(aes(x = type, y = log(views))) +\n  geom_boxplot() +\n  coord_flip()\n\nWarning: Removed 22 rows containing non-finite values (`stat_boxplot()`).\n\n\n\n\n\n\n\nQuestion 4: Analyzing YouTube Channels\n\nincorporate the channel.csv dataset and conduct further analysis. Follow these steps:\n\n\nImport the channel.csv, and merge it with the modified youtubers.csv dataset (note: not the original one, but the one that has been modified as per previous steps), based on channel_id (from channel.csv) and channel ( from youtubers.csv) columns.\nCalculate the average number of videos uploaded to YouTube for each of the three influencer types (tip: summarise() function)?\nInvestigate the relationship between the number of videos uploaded to the channel and the number of comments specificall among all the mega influencer (tip: remember to exclude the other two types). Visualize this relationship using a scatterplot with a linear fit line. If required, apply appropriate data transformations to enhance the clarity of the relationship.\n\n\n\n\n\n\n\nNote\n\n\n\nDisclaimer: Again, please be aware that channel.csv iis an arbitrary dataset and does not reflect the actual figures for the respective YouTube channels. We emphasize that we are employing this data exclusively for the purpose of practice and learning, and it should not be considered suitable for real-world applications.\n\n\n\nchannel &lt;- read.csv(\"channel.csv\")\nyoutubers_merge &lt;- merge(youtubers, channel, by.x = \"channel\", by.y = \"channel_id\", all.x = TRUE)\n\nyoutubers_merge %&gt;%\n  group_by(type) %&gt;%\n  summarise(mean_video = mean(video))\n\n# A tibble: 3 × 2\n  type             mean_video\n  &lt;fct&gt;                 &lt;dbl&gt;\n1 mega influencer       2351.\n2 power influencer      2092.\n3 super influencer      2696 \n\nyoutubers_merge %&gt;%\n  subset(type == \"mega influencer\") %&gt;%\n  ggplot(aes(x = video, y = log(comments))) +\n  geom_point() +\n  geom_smooth(method = lm) +\n  ylim(c(5,6))\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 853 rows containing non-finite values (`stat_smooth()`).\n\n\nWarning: Removed 649 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\n\nQuestion 5: Comparing YouTube Channels\n\nIn this final analysis, we aim to compare YouTube channels from the United States and India, excluding all other countries and NA values.\n\n\nGenerate summary statistics that include the means of views, likes, and comments for channels in both the United States and India.\nVisualize the distribution of the number of videos uploaded to YouTube by utilizing both a histogram and a density plot on a single graph. Please use different colors to represent the data from these two countries. (tip: geom_histogram() and geom_density()).\nCreate a line plot to visualize the count of YouTube channels created each year, distinguishing between the counts for the United States and India. Please use different colors to represent the data for these two countries in the line plot (tip: geom_line())\n\n\nyoutubers_compare &lt;- youtubers_merge %&gt;%\n  subset(country == \"United States\" | country == \"India\")\n\nyoutubers_compare %&gt;%\n  group_by(country) %&gt;%\n  summarise(mean_views = mean(views),\n            mean_likes = mean(likes),\n            mean_comments = mean(comments))\n\n# A tibble: 2 × 4\n  country       mean_views mean_likes mean_comments\n  &lt;fct&gt;              &lt;dbl&gt;      &lt;dbl&gt;         &lt;dbl&gt;\n1 India           1316478.     86862.         2650.\n2 United States   1485326.     60694.         2558.\n\nyoutubers_compare %&gt;%\n  ggplot(aes(x = video, color = country, fill = country)) +\n  geom_histogram(aes(y = ..density..)) +\n  geom_density(color = \"black\")\n\nWarning: The dot-dot notation (`..density..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(density)` instead.\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\nyoutubers_compare %&gt;%\n  ggplot(aes(x = year, color = country)) +\n  geom_line(stat = \"count\")"
  },
  {
    "objectID": "slides/week8_slides.html",
    "href": "slides/week8_slides.html",
    "title": "COMM 605",
    "section": "",
    "text": "A few different jargons:\n\nNatural language processing\n\nNOT Natural language understanding or generation\n\nAutomatic content analysis\n\nCompare with traditional content analysis (human coders)\n\nText-as-data\n\nPolitical science: political speech, newspaper\nWhat are the differences?\n\n\n\nLong articles, which might less messy (comparatively speaking)"
  },
  {
    "objectID": "slides/week6_slides.html",
    "href": "slides/week6_slides.html",
    "title": "COMM 605",
    "section": "",
    "text": "Social data in the digital form: digital traces produced by or about users, with an emphrasis on content explictly when written with the intent of communicating or interacting with others.\n\nSocial media and networking\nQ&A platforms\nCollaboration"
  },
  {
    "objectID": "week8.html#document-term-matrix-dtm",
    "href": "week8.html#document-term-matrix-dtm",
    "title": "Week 8 Lab",
    "section": "Document-term matrix (DTM)",
    "text": "Document-term matrix (DTM)\nFor the analysis of textual data, we will employ computational textual analysis, also known as natural language processing, or text-as-data, depending on the field. In the previous tutorial, computations are usually done on numerical data. Hence, you must find a way to represent the text by numbers. The document-term matrix (DTM, also called the term-document matrix or TDM) is one common numerical representation of text. It represents a corpus (or set of documents) as a matrix or table, where each row represents a document, each column represents a term (word), and the numbers in each cell show how often that word occurs in that document. Here is an example:\n\ntexts &lt;- c(\n    \"The caged bird sings with a fearful trill\", \n    \"for the caged bird sings of freedom\")\ndtm &lt;- tokens(texts) %&gt;% dfm()\n# Inspect by converting to a (dense) matrix\nconvert(dtm, \"matrix\") \n\n       features\ndocs    the caged bird sings with a fearful trill for of freedom\n  text1   1     1    1     1    1 1       1     1   0  0       0\n  text2   1     1    1     1    0 0       0     0   1  1       1\n\n\nAs you can observe from above, it shows a DTM made from two lines from the famous poem by Maya Angelou. The resulting matrix has two rows, one for each line; and 11 columns, one for each unique term (word). In the columns you see the document frequencies of each term: the word “bird” occurs once in each line, but the word “with” occurs only in the first line (text1) and not in the second (text2). The dfm function here (from the quanteda package) can take a vector or column of texts and transforms it directly into a DTM (which quanteda actually calls a document-feature matrix."
  },
  {
    "objectID": "week8.html#tokenization",
    "href": "week8.html#tokenization",
    "title": "Week 8 Lab",
    "section": "Tokenization",
    "text": "Tokenization\nIn order to turn a corpus into a matrix, each text needs to be tokenized, meaning that it must be split into a list (vector) of words. This seems trivial, as English (and most western) text generally uses spaces to demarcate words. However, even for English there are a number of edge cases. For example, should “haven’t” be seen as a single word, or two? In some other tokenizers, “haven’t” could be splitted into “have” and “n’t” (TreebankWordTokenizer included in the nltk package pf python). Other might silently drops all single-letter words, including the ’t, ’s, and I, which could be problematic in this case, “have’t” to “haven”.\n\ntext &lt;- \"I haven't seen John's derring-do\"\ntokens(text)\n\nTokens consisting of 1 document.\ntext1 :\n[1] \"I\"          \"haven't\"    \"seen\"       \"John's\"     \"derring-do\"\n\n\nIn languages such as Chinese, Japanese, and Korean, which do not use spaces to delimit words, the story is more difficult. For example, below is a case of the famous haiku “the sound of water” by Bashō. Although quanteda’s tokenizer did pretty good in this case, you might want to check out the specific tokenizer when dealing with different languages.\n\nhaiku &lt;- \"\\u53e4\\u6c60\\u86d9\\u98db\\u3073\\u8fbc\\u3080\\u6c34\\u306e\\u97f3\"\ntokens(haiku)\n\nTokens consisting of 1 document.\ntext1 :\n[1] \"古池\"     \"蛙\"       \"飛び込む\" \"水\"       \"の\"       \"音\""
  },
  {
    "objectID": "week8.html#dtm-as-a-sparse-matrix",
    "href": "week8.html#dtm-as-a-sparse-matrix",
    "title": "Week 8 Lab",
    "section": "DTM as a Sparse Matrix",
    "text": "DTM as a Sparse Matrix\n\nd &lt;- corpus(data_corpus_inaugural) %&gt;% tokens() %&gt;% dfm()\nd\n\nDocument-feature matrix of: 59 documents, 9,441 features (91.84% sparse) and 4 docvars.\n                 features\ndocs              fellow-citizens  of the senate and house representatives :\n  1789-Washington               1  71 116      1  48     2               2 1\n  1793-Washington               0  11  13      0   2     0               0 1\n  1797-Adams                    3 140 163      1 130     0               2 0\n  1801-Jefferson                2 104 130      0  81     0               0 1\n  1805-Jefferson                0 101 143      0  93     0               0 0\n  1809-Madison                  1  69 104      0  43     0               0 0\n                 features\ndocs              among vicissitudes\n  1789-Washington     1            1\n  1793-Washington     0            0\n  1797-Adams          4            0\n  1801-Jefferson      1            0\n  1805-Jefferson      7            0\n  1809-Madison        0            0\n[ reached max_ndoc ... 53 more documents, reached max_nfeat ... 9,431 more features ]\n\n\ndata_corpus_inaugural is a collection all US presidential inaugural address texts, and metadata (for the corpus), from 1789 to present, and we just created a document-term matrix from them. You can tell from the meta-data, which lists 59 documents (rows) and 9,439 features (columns).\n\ntextstat_frequency(d)[c(1, 10, 100, 1000, 9000), ]\n\n        feature frequency rank docfreq group\n1           the     10183    1      59   all\n10           we      1827   10      58   all\n100    national       158  100      46   all\n1000     suffer        15  973      13   all\n9000 physicists         1 5509       1   all\n\n\n\nas.matrix(d[\n  c(1, 10, 56, 57, 58, 59),\n  c(\"the\",\"first\",\"justice\", \"america\", \"people\", \"us\", \"we\")])\n\n                 features\ndocs              the first justice america people us we\n  1789-Washington 116     2       0       0      4  1  1\n  1825-Adams      304     8       4       0      7  4  8\n  2009-Obama      135     0       0       8      7 23 62\n  2013-Obama      104     0       2       6     11 21 68\n  2017-Trump       71     4       1      18     10  2 46\n  2021-Biden      101     3       5      18      9 27 88\n\n\nFirst, we show the overall term and document frequencies of each word, where we showcase words at different frequencies. Unsurprisingly, the word the tops both charts, but further down there are minor differences. In all cases, the highly frequent words are mostly functional words like the or first. More informative words such as investments are by their nature used much less often. Such term statistics are very useful to check for noise in the data and get a feeling of the kind of language that is used.\nHowever, the words that ranked around 1000 in the top frequency are still used in less than half of the documents. Since there are about 8,000 even less frequent words in the corpus, you can imagine that most of the document-term matrix consists of zeros. The output also noted this sparsity in the first output above. In fact, R reports that the DTM is sparse, meaning 91.84% percent of all entries are zero.\n\n\n\n\n\n\nNote\n\n\n\nMatrices that contain mostly zero values are called sparse, distinct from matrices where most of the values are non-zero, called dense."
  },
  {
    "objectID": "week8.html#dtm-as-a-bag-of-words",
    "href": "week8.html#dtm-as-a-bag-of-words",
    "title": "Week 8 Lab",
    "section": "DTM as a “Bag of Words”",
    "text": "DTM as a “Bag of Words”\nAs you can see already in these simple examples, the document-term matrix discards quite a lot of information from text. Specifically, it disregards the order or words in a text: “John fired Mary” and “Mary fired John” both result in the same DTM, even though the meaning of the sentences is quite different. For this reason, a DTM is often called a bag of words, in the sense that all words in the document are simply put in a big bag without looking at the sentences or context of these words.\nThus, the DTM can be said to be a specific and “lossy” representation of the text, that turns out to be quite useful for certain tasks: the frequent occurrence of words like “employment”, “great”, or “I” might well be good indicators that a text is about the economy, is positive, or contains personal expressions respectively. If a president uses the word “terrorism” more often than the word “economy”, that could be an indication of their policy priorities. The DTM representation can be used for many different text analyses, from dictionaries to supervised and unsupervised machine learning. Sometimes, however, you need information that is encoded in the order of words. For example, in analyzing conflict coverage it might be quite important to know who attacks whom, not just that an attack took place. Then we need to get into more advance representation of text, such as word embedding."
  },
  {
    "objectID": "week8.html#pre-processingcleaning-stop-words-punctuation-numbers-lowercasing-stemming-or-lemmatization-etc",
    "href": "week8.html#pre-processingcleaning-stop-words-punctuation-numbers-lowercasing-stemming-or-lemmatization-etc",
    "title": "Week 8 Lab",
    "section": "Pre-processing/Cleaning (stop words, punctuation, numbers, lowercasing, stemming or lemmatization, etc)",
    "text": "Pre-processing/Cleaning (stop words, punctuation, numbers, lowercasing, stemming or lemmatization, etc)\n\nd &lt;- corpus(data_corpus_inaugural) %&gt;% \n  tokens() %&gt;%\n  dfm()\ntextplot_wordcloud(d, max_words=200)\n\n\n\n\nA first step in cleaning a DTM is often stop word removal. Words such as “a” and “the” are often called stop words, i.e. words that do not tell us much about the content. However, we need to be more cautious about the stop wordsd list. As an example of the substantive choices inherent in using a stop word lists, consider the word “will”. As an auxiliary verb, this is probably indeed a stop word: for most substantive questions, there is no difference whether you will do something or simply do it. However, “will” can also be a noun (a testament) and a name (e.g. Will Smith). Simply dropping such words from the corpus can be problematic. In some cases, some research questions might actually be interested in certain stop words. If you are interested in references to the future or specific modalities, the word might actually be a key indicator. Similarly, if you are studying self-expression on Internet forums, social identity theory, or populist rhetoric, words like “I”, “us” and “them” can actually be very informative. Additionally, you can also customize your own stop word list.\nNext to stop words, text often contains punctuation, numbers, symbol, hyphens, and other things that can be considered “noise” for most research questions. For example, it could contain emoticons or emoji, Twitter hashtags or at-mentions, or HTML tags or other annotations. Meanwhile, it is a common practice to lowercase the tokens.\nLast, we need to consider stemming and lemmatization in the pre-processing. Stemming is the process of reducing infected words to their stem. For instance, stemming with replace words “history” and “historical” with “histori”. Similarly, for the words finally and final, the base word is “fina”. In a lot of cases (i.e., sentiment analysis), getting base word is important to know whether the word is positive or negative. The purpose of lemmatization is same as that of stemming but overcomes the drawbacks of stemming. In stemming, for some words, it may not give may not give meaningful representation such as “Histori”. Here, lemmatization comes into picture as it gives meaningful word.\nLemmatization takes more time as compared to stemming because it finds meaningful word/ representation. Stemming just needs to get a base word and therefore takes less time.\n\n\n\n\n\n\n\nStemming\nLemmatization\n\n\n\n\nStemming is a process that stems or removes last few characters from a word, often leading to incorrect meanings and spelling.\nLemmatization considers the context and converts the word to its meaningful base form, which is called Lemma.\n\n\nExample: stemming the word ‘Caring’ would return ‘Car’.\nExample: lemmatizing the word ‘Caring’ would return ‘Care’.\n\n\nStemming is used in case of large dataset where performance is an issue.\nLemmatization is computationally expensive since it involves look-up tables and what not.\n\n\n\n\ntxt &lt;- c(one = \"eating eater eaters eats ate\",\n         two = \"taxing taxes taxed my tax return\")\ntxt_dfm &lt;- txt %&gt;% \n           tokens() %&gt;%\n           tokens_wordstem()%&gt;%\n           dfm()\n           \ntxt_dfm\n\nDocument-feature matrix of: 2 documents, 6 features (50.00% sparse) and 0 docvars.\n     features\ndocs  eat eater ate tax my return\n  one   2     2   1   0  0      0\n  two   0     0   0   4  1      1\n\n\n\ntxt &lt;- c(\"I am going to lemmatize makes into make, but not maker\")\n# stemming\ntokens_wordstem(tokens(txt))\n\nTokens consisting of 1 document.\ntext1 :\n [1] \"I\"      \"am\"     \"go\"     \"to\"     \"lemmat\" \"make\"   \"into\"   \"make\"  \n [9] \",\"      \"but\"    \"not\"    \"maker\" \n\n# lemmatizing using lemma table\ntokens_replace(tokens(txt), pattern = lexicon::hash_lemmas$token, replacement = lexicon::hash_lemmas$lemma)\n\nTokens consisting of 1 document.\ntext1 :\n [1] \"I\"         \"be\"        \"go\"        \"to\"        \"lemmatize\" \"make\"     \n [7] \"into\"      \"make\"      \",\"         \"but\"       \"not\"       \"maker\"    \n\n\n\nd_clean &lt;- corpus(data_corpus_inaugural) %&gt;% \n  tokens(remove_punct = T) %&gt;%\n  dfm() %&gt;%\n  dfm_remove(stopwords(\"english\"))\ntextplot_wordcloud(d_clean, max_words=200)\n\n\n\n\n\nmystopwords = stopwords(\"english\", source=\"snowball\")\nmystopwords = c(\"can\", \"one\", \"let\", \"upon\", mystopwords)\nglue(\"Now {length(mystopwords)} stopwords:\")\n\nNow 179 stopwords:\n\nd_stopword &lt;- corpus(data_corpus_inaugural) %&gt;% \n  tokens(remove_punct = T) %&gt;% \n  dfm() %&gt;%\n  dfm_remove(mystopwords)\ntextplot_wordcloud(d_stopword, max_words=200)\n\n\n\n\n\nd_clean_all &lt;- corpus(data_corpus_inaugural) %&gt;% \n  tokens(remove_punct = T, remove_numbers = T, remove_symbols = T, remove_hyphens = T) %&gt;%\n  tokens_wordstem()%&gt;%\n  dfm(tolower = T) %&gt;%\n  dfm_remove(stopwords(\"english\"))\n\nWarning: remove_hyphens argument is not used.\n\ntextplot_wordcloud(d_clean_all, max_words=200)"
  },
  {
    "objectID": "week8.html#wordcloud",
    "href": "week8.html#wordcloud",
    "title": "Week 8 Lab",
    "section": "Wordcloud",
    "text": "Wordcloud\n\ncorpus_subset(data_corpus_inaugural, \n              President %in% c(\"Washington\", \"Obama\", \"Trump\")) %&gt;%\n    tokens(remove_punct = TRUE) %&gt;%\n    tokens_remove(stopwords(\"english\")) %&gt;%\n    dfm() %&gt;%\n    dfm_group(groups = President) %&gt;%\n    dfm_trim(min_termfreq = 5, verbose = FALSE) %&gt;%\n    textplot_wordcloud(comparison = TRUE)"
  },
  {
    "objectID": "week9.html",
    "href": "week9.html",
    "title": "Week 9 Lab",
    "section": "",
    "text": "library(tidyverse)\nlibrary(quanteda)\nlibrary(tidytext)\nlibrary(quanteda.textstats)\nlibrary(quanteda.textplots)\nlibrary(vader) #install.packages(\"vader\")\nlibrary(wordcloud) #install.packages(\"wordcloud\")"
  },
  {
    "objectID": "week9.html#pre-processing",
    "href": "week9.html#pre-processing",
    "title": "Week 9 Lab",
    "section": "Pre-processing",
    "text": "Pre-processing\n\ntrump_clean &lt;- trump\ntrump_clean &lt;- trump %&gt;% mutate(\n  ### Extract mentions, hashtags, and url\n  mention = str_extract_all(text, \"(^|\\\\s)@\\\\w+\"),\n  hashtag = str_extract_all(text, \"#\\\\w+\"),\n  url = str_extract_all(text, \"https?://\\\\S+\"), \n  ### Remove all the mentions, hashtags, and url for text cleaning\n  clean_text = str_replace_all(text, \"RT\\\\s*@|@\\\\w+\", \" \") %&gt;% ### remove mentions\n               str_replace_all(., \"#\\\\w+\", \" \") %&gt;% ### remove hashtags\n               str_replace_all(., \"https?://\\\\S+\", \" \") %&gt;% ### remove URL\n               str_replace_all(\"\\\\W+\", \" \") %&gt;% ### remove all the non-words\n               tolower() %&gt;% ### lowercase\n               trimws() ### trim the white space\n)\n\nclean_dfm &lt;- corpus(trump_clean$clean_text) %&gt;% \n  tokens(remove_punct = T, remove_numbers = T, remove_symbols = T) %&gt;%\n  dfm(tolower = T) %&gt;%\n  dfm_remove(stopwords(\"english\"))\ntextstat_frequency(clean_dfm, n = 100)\n\n            feature frequency rank docfreq group\n1                 s      9925    1    8704   all\n2             great      7678    2    7139   all\n3             trump      6856    3    6460   all\n4                 t      5904    4    5399   all\n5               amp      5682    5    4707   all\n6         president      4768    6    4520   all\n7             thank      3622    7    3582   all\n8            people      3534    8    3330   all\n9              just      3412    9    3310   all\n10              can      2876   10    2708   all\n11              now      2716   11    2662   all\n12              new      2517   12    2398   all\n13          america      2453   13    2322   all\n14          country      2433   14    2361   all\n15              get      2425   15    2276   all\n16           thanks      2309   16    2296   all\n17              big      2154   17    2006   all\n18              one      2062   18    1963   all\n19             like      2059   19    1982   all\n20             time      2035   20    1960   all\n21            obama      2004   21    1921   all\n22           donald      1931   22    1894   all\n23            never      1893   23    1776   all\n24             good      1880   24    1807   all\n25             make      1871   25    1811   all\n26             news      1862   26    1759   all\n27              don      1859   27    1766   all\n28            today      1852   28    1815   all\n29        democrats      1831   29    1775   all\n30                u      1790   30    1646   all\n31             many      1755   31    1691   all\n32             vote      1732   32    1550   all\n33               us      1562   33    1477   all\n34            going      1554   34    1476   all\n35             back      1517   35    1441   all\n36         american      1480   36    1373   all\n37             want      1443   37    1321   all\n38                m      1441   38    1368   all\n39             much      1434   39    1360   all\n40              see      1364   40    1336   all\n41             even      1309   41    1269   all\n42            years      1302   42    1251   all\n43              job      1300   43    1273   all\n44             fake      1294   44    1214   all\n45              run      1284   45    1200   all\n46             best      1276   46    1234   all\n47             must      1275   47    1225   all\n48            china      1247   48    1021   all\n49             need      1225   49    1164   all\n50         election      1219   50    1149   all\n51            media      1205   51    1142   all\n52             love      1190   52    1139   all\n53               re      1183   53    1119   all\n54              win      1177   54    1097   all\n55               go      1172   55    1127   all\n56            think      1152   56    1093   all\n57            biden      1144   57    1007   all\n58              way      1140   58    1107   all\n59           really      1128   59    1091   all\n60              day      1127   60    1083   all\n61             last      1116   61    1100   all\n62             know      1114   62    1057   all\n63            house      1087   63    1050   all\n64             done      1082   64    1035   all\n65           states      1070   65    1027   all\n66            world      1068   66    1035   all\n67            state      1067   67    1012   all\n68             ever      1065   68    1033   all\n69            first      1064   69    1037   all\n70             jobs      1058   70     902   all\n71             work      1048   71    1003   all\n72             true      1041   72    1028   all\n73              bad      1039   73     997   all\n74  realdonaldtrump      1033   74    1033   all\n75             said      1024   75     969   all\n76          tonight      1010   76     997   all\n77          nothing       985   77     930   all\n78            right       984   78     952   all\n79           border       980   79     852   all\n80             deal       979   80     884   all\n81             show       974   81     938   all\n82          hillary       970   82     935   all\n83              via       966   83     963   all\n84            watch       951   84     938   all\n85             made       937   85     913   all\n86           united       892   86     866   all\n87            night       886   87     869   all\n88           better       879   88     850   all\n89              got       867   89     840   all\n90              joe       862   90     785   all\n91              let       845   91     827   all\n92             look       843   92     819   all\n93            total       837   93     832   all\n94             well       826   94     805   all\n95           always       818   95     793   all\n96       republican       817   96     792   all\n97             year       815   97     768   all\n98              won       809   98     784   all\n99         national       808   99     773   all\n100            keep       801  100     774   all\n\n\n\nmystopwords = stopwords(\"english\", source=\"snowball\")\nmystopwords = c(\"s\", \"t\", \"amp\", \"u\", \"m\", mystopwords)\nclean_dfm2 &lt;- corpus(trump_clean$clean_text) %&gt;% \n  tokens(remove_punct = T, remove_numbers = T, remove_symbols = T) %&gt;%\n  dfm(tolower = T) %&gt;%\n  dfm_remove(mystopwords)\n\ntextplot_wordcloud(clean_dfm2, max_words=200)"
  },
  {
    "objectID": "week9.html#simple-frequency",
    "href": "week9.html#simple-frequency",
    "title": "Week 9 Lab",
    "section": "Simple frequency",
    "text": "Simple frequency\n\n# Sort by reverse frequency order\nfeatures_trump &lt;- textstat_frequency(clean_dfm2, n = 100)\nfeatures_trump$feature &lt;- with(features_trump, reorder(feature, -frequency))\n\nfeatures_trump %&gt;%\n  top_n(30, wt = frequency) %&gt;%\n  ggplot(aes(x = feature, y = frequency)) +\n  geom_bar(stat = \"identity\", fill = \"steelblue\") + \n  labs(y = \"Frequency\", x = NULL) +\n  theme_classic() +\n  theme(axis.text.x = element_text(angle = 60, hjust = 1),\n        legend.position = \"none\")"
  },
  {
    "objectID": "week9.html#hashtags",
    "href": "week9.html#hashtags",
    "title": "Week 9 Lab",
    "section": "Hashtags",
    "text": "Hashtags\n\ntag_dfm &lt;- corpus(trump_clean$text) %&gt;% \n  tokens(remove_punct = T) %&gt;%\n  dfm(tolower = T) %&gt;%\n  dfm_select(pattern = \"#*\")\ntoptag &lt;- names(topfeatures(tag_dfm, 50))\ntoptag\n\n [1] \"#trump2016\"                 \"#makeamericagreatagain\"    \n [3] \"#maga\"                      \"#celebapprentice\"          \n [5] \"#celebrityapprentice\"       \"#1\"                        \n [7] \"#trump\"                     \"#americafirst\"             \n [9] \"#timetogettough\"            \"#trumpvlog\"                \n[11] \"#draintheswamp\"             \"#trumpforpresident\"        \n[13] \"#kag2020\"                   \"#votetrump\"                \n[15] \"#covid19\"                   \"#trump2016https\"           \n[17] \"#usa\"                       \"#bigleaguetruth\"           \n[19] \"#2a\"                        \"#imwithyou\"                \n[21] \"#debate\"                    \"#2016\"                     \n[23] \"#trumptrain\"                \"#demdebate\"                \n[25] \"#usmca\"                     \"#crookedhillary\"           \n[27] \"#gopdebate\"                 \"#fitn\"                     \n[29] \"#coronavirus\"               \"#tbt\"                      \n[31] \"#dorian\"                    \"#icymi\"                    \n[33] \"#gop\"                       \"#oscars\"                   \n[35] \"#obamacare\"                 \"#trumppence16\"             \n[37] \"#teamtrump\"                 \"#fakenews\"                 \n[39] \"#vpdebate\"                  \"#donaldtrump\"              \n[41] \"#paycheckprotectionprogram\" \"#kag\"                      \n[43] \"#debates2016\"               \"#iacaucus\"                 \n[45] \"#theapprentice\"             \"#lesm\"                     \n[47] \"#sweepstweet\"               \"#impeachment\"              \n[49] \"#tcot\"                      \"#trump16\"                  \n\n\n\ntag_fcm &lt;- fcm(tag_dfm)\nhead(tag_fcm)\n\nFeature co-occurrence matrix of: 6 by 2,901 features.\n                        features\nfeatures                 #kag2020 #maga #vote #millionmagamarch #developing\n  #kag2020                      0     8     0                 0           0\n  #maga                         0     0     2                 0           0\n  #vote                         0     0     0                 0           0\n  #millionmagamarch             0     0     0                 0           0\n  #developing                   0     0     0                 0           0\n  #dominionvotingsystems        0     0     0                 0           0\n                        features\nfeatures                 #dominionvotingsystems #marchfortrump #proamericarally\n  #kag2020                                    0              0                0\n  #maga                                       0              1                0\n  #vote                                       0              0                0\n  #millionmagamarch                           0              0                0\n  #developing                                 1              0                0\n  #dominionvotingsystems                      0              0                0\n                        features\nfeatures                 #affidavits #stopthesteal\n  #kag2020                         0             0\n  #maga                            0             0\n  #vote                            0             0\n  #millionmagamarch                0             0\n  #developing                      0             0\n  #dominionvotingsystems           0             0\n[ reached max_nfeat ... 2,891 more features ]\n\n\n\ntopgat_fcm &lt;- fcm_select(tag_fcm, pattern = toptag)\nset.seed(123)\ntextplot_network(topgat_fcm, min_freq = 0.5, edge_alpha = 0.8, edge_size = 1, vertex_labelsize = 3.5)"
  },
  {
    "objectID": "week9.html#users",
    "href": "week9.html#users",
    "title": "Week 9 Lab",
    "section": "Users",
    "text": "Users\n\nuser_dfm &lt;- corpus(trump_clean$text) %&gt;% \n  tokens(remove_punct = T) %&gt;%\n  dfm(tolower = T) %&gt;%\n  dfm_select(pattern = \"@*\")\ntopuser &lt;- names(topfeatures(user_dfm, 50))\nhead(topuser)\n\n[1] \"@realdonaldtrump\" \"@foxnews\"         \"@whitehouse\"      \"@foxandfriends\"  \n[5] \"@barackobama\"     \"@cnn\"            \n\n\n\nuser_fcm &lt;- fcm(user_dfm)\nhead(user_fcm)\n\nFeature co-occurrence matrix of: 6 by 16,348 features.\n                  features\nfeatures           @cbs_herridge @mzhemingway @whitehouse @realdonaldtrump\n  @cbs_herridge                0            0           0                0\n  @mzhemingway                 0            0           0                1\n  @whitehouse                  0            0           0              292\n  @realdonaldtrump             0            0           0               78\n  @erictrump                   0            0           0                0\n  @gopchairwoman               0            0           0                0\n                  features\nfeatures           @erictrump @gopchairwoman @tomfitton @marklevinshow\n  @cbs_herridge             0              0          0              0\n  @mzhemingway              0              0          0              0\n  @whitehouse               0              0          0              0\n  @realdonaldtrump         42            121         50              5\n  @erictrump                0              1          0              0\n  @gopchairwoman            0              0          0              0\n                  features\nfeatures           @realrlimbaugh @therightmelissa\n  @cbs_herridge                 0                0\n  @mzhemingway                  0                0\n  @whitehouse                   0                0\n  @realdonaldtrump              1                7\n  @erictrump                    0                0\n  @gopchairwoman                0                0\n[ reached max_nfeat ... 16,338 more features ]\n\n\n\nuser_fcm &lt;- fcm_select(user_fcm, pattern = topuser)\nset.seed(123)\ntextplot_network(user_fcm, min_freq = 0.1, edge_color = \"orange\", edge_alpha = 0.8, edge_size = 1, vertex_labelsize = 3.5)"
  },
  {
    "objectID": "week9.html#dictionaries-based-sentiment-analysis",
    "href": "week9.html#dictionaries-based-sentiment-analysis",
    "title": "Week 9 Lab",
    "section": "Dictionaries-based sentiment analysis",
    "text": "Dictionaries-based sentiment analysis"
  },
  {
    "objectID": "week9.html#cutomized-dictionaries",
    "href": "week9.html#cutomized-dictionaries",
    "title": "Week 9 Lab",
    "section": "Cutomized dictionaries",
    "text": "Cutomized dictionaries"
  },
  {
    "objectID": "week10.html",
    "href": "week10.html",
    "title": "Week 10 Lab",
    "section": "",
    "text": "library(tidyverse)\nlibrary(quanteda)\n\n\nWeighting: “Bag-of-words” and tf-idf\nAs you may recall, we previously discussed the “bag-of-words” approach, which involves converting a corpus into a document-term matrix (DTM) where each value represents a count. In many cases, it is beneficial to convert count data into logged count data. This transformation can be accomplished using the dfm_weight command here. The most commonly used weighting scheme is term frequency-inverse document frequency (tf-idf). The general idea is to take a term frequency or logged term frequency and downweight that according to (logged) document frequency. The intuition is that the most important words are those that are used a lot in a given document but relatively rare in the corpus overall. Mathematically, the tf-idf is:\n\n\n\nThe mathematical expression of tf-idf\n\n\nIf we would like to translate into layman language:\n\\[\nterm\\; frequency = \\frac{number\\; of\\; times\\; the\\; term\\; appears\\; in\\; the\\; document}{total\\; number\\; of\\; terms\\; in\\; the\\; document}\n\\] \\[\ninverse\\; document\\; frequency = \\log(\\frac{number\\; of\\; the\\; documents\\; in\\; the\\; corpus}{number\\; of\\; documents\\; in\\; the\\; corpus\\; contain\\; the\\; term})\n\\] \\[\nTF-IDF = TF * IDF\n\\] With tf-idf, the importance of a term is high when it occurs a lot in a given document and rarely in others. In short, commonality within a document measured by TF is balanced by rarity between documents measured by IDF. The resulting TF-IDF score reflects the importance of a term for a document in the corpus. Let’s use Maya Angelou’s Caged Birdas an illustration again:\n\ntexts &lt;- c(\n    \"The caged bird sings with a fearful trill\", \n    \"for the caged bird sings of freedom\",\n    \"The free bird thinks of another breeze\")\ndtm &lt;- tokens(texts) %&gt;% dfm()\ndtm_tfidf &lt;- dfm_tfidf(dtm)\nconvert(dtm, \"matrix\")\n\n       features\ndocs    the caged bird sings with a fearful trill for of freedom free thinks\n  text1   1     1    1     1    1 1       1     1   0  0       0    0      0\n  text2   1     1    1     1    0 0       0     0   1  1       1    0      0\n  text3   1     0    1     0    0 0       0     0   0  1       0    1      1\n       features\ndocs    another breeze\n  text1       0      0\n  text2       0      0\n  text3       1      1\n\nconvert(dtm_tfidf, \"matrix\")\n\n       features\ndocs    the     caged bird     sings      with         a   fearful     trill\n  text1   0 0.1760913    0 0.1760913 0.4771213 0.4771213 0.4771213 0.4771213\n  text2   0 0.1760913    0 0.1760913 0.0000000 0.0000000 0.0000000 0.0000000\n  text3   0 0.0000000    0 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000\n       features\ndocs          for        of   freedom      free    thinks   another    breeze\n  text1 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000\n  text2 0.4771213 0.1760913 0.4771213 0.0000000 0.0000000 0.0000000 0.0000000\n  text3 0.0000000 0.1760913 0.0000000 0.4771213 0.4771213 0.4771213 0.4771213"
  }
]